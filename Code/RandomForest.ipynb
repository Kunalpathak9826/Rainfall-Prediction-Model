{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b69982bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "730/730 [==============================] - 1s 606us/step - loss: nan - mse: nan\n",
      "Epoch 2/10\n",
      "730/730 [==============================] - 0s 598us/step - loss: nan - mse: nan\n",
      "Epoch 3/10\n",
      "730/730 [==============================] - 0s 663us/step - loss: nan - mse: nan\n",
      "Epoch 4/10\n",
      "730/730 [==============================] - 0s 641us/step - loss: nan - mse: nan\n",
      "Epoch 5/10\n",
      "730/730 [==============================] - 0s 646us/step - loss: nan - mse: nan\n",
      "Epoch 6/10\n",
      "730/730 [==============================] - 1s 690us/step - loss: nan - mse: nan\n",
      "Epoch 7/10\n",
      "730/730 [==============================] - 1s 699us/step - loss: nan - mse: nan\n",
      "Epoch 8/10\n",
      "730/730 [==============================] - 1s 699us/step - loss: nan - mse: nan\n",
      "Epoch 9/10\n",
      "730/730 [==============================] - 0s 682us/step - loss: nan - mse: nan\n",
      "Epoch 10/10\n",
      "730/730 [==============================] - 0s 635us/step - loss: nan - mse: nan\n",
      "Epoch 1/10\n",
      "730/730 [==============================] - 5s 5ms/step - loss: nan\n",
      "Epoch 2/10\n",
      "730/730 [==============================] - 3s 5ms/step - loss: nan\n",
      "Epoch 3/10\n",
      "730/730 [==============================] - 3s 4ms/step - loss: nan\n",
      "Epoch 4/10\n",
      "730/730 [==============================] - 3s 5ms/step - loss: nan\n",
      "Epoch 5/10\n",
      "730/730 [==============================] - 3s 5ms/step - loss: nan\n",
      "Epoch 6/10\n",
      "730/730 [==============================] - 3s 4ms/step - loss: nan\n",
      "Epoch 7/10\n",
      "730/730 [==============================] - 3s 4ms/step - loss: nan\n",
      "Epoch 8/10\n",
      "730/730 [==============================] - 3s 4ms/step - loss: nan\n",
      "Epoch 9/10\n",
      "730/730 [==============================] - 3s 4ms/step - loss: nan\n",
      "Epoch 10/10\n",
      "730/730 [==============================] - 3s 4ms/step - loss: nan\n",
      "313/313 [==============================] - 0s 285us/step\n",
      "313/313 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 119\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m#print(\"Random Forest RMSE:\", rf_rmse)\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m#print(\"Random Forest MAE:\", rf_mae)\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 119\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[2], line 103\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m rnn_pred \u001b[38;5;241m=\u001b[39m rnn_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m#rf_pred = rf_model.predict(X_test)\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Calculate RMSE and MAE\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m cnn_rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(mean_squared_error(y_test, cnn_pred))\n\u001b[1;32m    104\u001b[0m rnn_rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(mean_squared_error(y_test, rnn_pred))\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m#rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:442\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean_squared_error\u001b[39m(\n\u001b[1;32m    383\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, multioutput\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m\"\u001b[39m, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    384\u001b[0m ):\n\u001b[1;32m    385\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \n\u001b[1;32m    387\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;124;03m    0.825...\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m _check_reg_targets(\n\u001b[1;32m    443\u001b[0m         y_true, y_pred, multioutput\n\u001b[1;32m    444\u001b[0m     )\n\u001b[1;32m    445\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    446\u001b[0m     output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:101\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    correct keyword.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    100\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[0;32m--> 101\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    102\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    916\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    918\u001b[0m         )\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 921\u001b[0m         _assert_all_finite(\n\u001b[1;32m    922\u001b[0m             array,\n\u001b[1;32m    923\u001b[0m             input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    924\u001b[0m             estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    925\u001b[0m             allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    926\u001b[0m         )\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    929\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m     )\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, LSTM\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_and_preprocess_data(directory):\n",
    "    data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            df = pd.read_csv(filepath)\n",
    "            # Drop unnecessary columns if any\n",
    "            df.drop(columns=['longitude', 'latitude'], inplace=True)\n",
    "            # Combine the columns related to each timestamp\n",
    "            df = df.groupby(np.arange(len(df))//3).mean()\n",
    "            data.append(df)\n",
    "    data = pd.concat(data, ignore_index=True)\n",
    "    return data\n",
    "\n",
    "# Function to perform PCA with handling missing values\n",
    "def perform_pca(data):\n",
    "    # Impute missing values\n",
    "    imputer = SimpleImputer(strategy='mean')  # You can change the strategy as per your requirement\n",
    "    data_imputed = imputer.fit_transform(data)\n",
    "    \n",
    "    # Perform PCA\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(data_imputed)\n",
    "    pca = PCA(n_components=10)  # You can change the number of components as per your requirement\n",
    "    pca_data = pca.fit_transform(scaled_data)\n",
    "    return pca_data\n",
    "\n",
    "# Function to build CNN model\n",
    "def build_cnn_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "# Function to build RNN model\n",
    "def build_rnn_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(LSTM(50, return_sequences=False))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Function to build Random Forest Regressor model\n",
    "#def build_rf_model():\n",
    "    #return RandomForestRegressor()\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    directory = \"/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated_CSV\"\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    data = load_and_preprocess_data(directory)\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca_data = perform_pca(X)\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(pca_data, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Reshape data for CNN and RNN\n",
    "    X_train_cnn = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test_cnn = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "    input_shape = (X_train_cnn.shape[1], 1)\n",
    "    \n",
    "    # Build and train CNN model\n",
    "    cnn_model = build_cnn_model(input_shape)\n",
    "    cnn_model.fit(X_train_cnn, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "    \n",
    "    # Build and train RNN model\n",
    "    rnn_model = build_rnn_model(input_shape)\n",
    "    rnn_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "    \n",
    "    # Build and train Random Forest Regressor model\n",
    "    #rf_model = build_rf_model()\n",
    "    #rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate models\n",
    "    cnn_pred = cnn_model.predict(X_test_cnn)\n",
    "    rnn_pred = rnn_model.predict(X_test)\n",
    "    #rf_pred = rf_model.predict(X_test)\n",
    "    \n",
    "    # Calculate RMSE and MAE\n",
    "    cnn_rmse = np.sqrt(mean_squared_error(y_test, cnn_pred))\n",
    "    rnn_rmse = np.sqrt(mean_squared_error(y_test, rnn_pred))\n",
    "    #rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "    \n",
    "    cnn_mae = mean_absolute_error(y_test, cnn_pred)\n",
    "    rnn_mae = mean_absolute_error(y_test, rnn_pred)\n",
    "    #rf_mae = mean_absolute_error(y_test, rf_pred)\n",
    "    \n",
    "    print(\"CNN RMSE:\", cnn_rmse)\n",
    "    print(\"CNN MAE:\", cnn_mae)\n",
    "    print(\"RNN RMSE:\", rnn_rmse)\n",
    "    print(\"RNN MAE:\", rnn_mae)\n",
    "    #print(\"Random Forest RMSE:\", rf_rmse)\n",
    "    #print(\"Random Forest MAE:\", rf_mae)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2a580ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1641/1641 [==============================] - 1s 539us/step - loss: nan - mse: nan\n",
      "Epoch 2/10\n",
      "1641/1641 [==============================] - 1s 510us/step - loss: nan - mse: nan\n",
      "Epoch 3/10\n",
      "1641/1641 [==============================] - 1s 512us/step - loss: nan - mse: nan\n",
      "Epoch 4/10\n",
      "1641/1641 [==============================] - 1s 506us/step - loss: nan - mse: nan\n",
      "Epoch 5/10\n",
      "1641/1641 [==============================] - 1s 521us/step - loss: nan - mse: nan\n",
      "Epoch 6/10\n",
      "1641/1641 [==============================] - 1s 530us/step - loss: nan - mse: nan\n",
      "Epoch 7/10\n",
      "1641/1641 [==============================] - 1s 504us/step - loss: nan - mse: nan\n",
      "Epoch 8/10\n",
      "1641/1641 [==============================] - 1s 503us/step - loss: nan - mse: nan\n",
      "Epoch 9/10\n",
      "1641/1641 [==============================] - 1s 505us/step - loss: nan - mse: nan\n",
      "Epoch 10/10\n",
      "1641/1641 [==============================] - 1s 505us/step - loss: nan - mse: nan\n",
      "Epoch 1/10\n",
      "1641/1641 [==============================] - 3s 2ms/step - loss: nan - mse: nan\n",
      "Epoch 2/10\n",
      "1641/1641 [==============================] - 3s 2ms/step - loss: nan - mse: nan\n",
      "Epoch 3/10\n",
      "1641/1641 [==============================] - 3s 2ms/step - loss: nan - mse: nan\n",
      "Epoch 4/10\n",
      "1641/1641 [==============================] - 3s 2ms/step - loss: nan - mse: nan\n",
      "Epoch 5/10\n",
      "1641/1641 [==============================] - 3s 2ms/step - loss: nan - mse: nan\n",
      "Epoch 6/10\n",
      "1641/1641 [==============================] - 3s 2ms/step - loss: nan - mse: nan\n",
      "Epoch 7/10\n",
      "1641/1641 [==============================] - 3s 2ms/step - loss: nan - mse: nan\n",
      "Epoch 8/10\n",
      "1641/1641 [==============================] - 3s 2ms/step - loss: nan - mse: nan\n",
      "Epoch 9/10\n",
      "1641/1641 [==============================] - 3s 2ms/step - loss: nan - mse: nan\n",
      "Epoch 10/10\n",
      "1641/1641 [==============================] - 3s 2ms/step - loss: nan - mse: nan\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input y contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 112\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom Forest MAE:\u001b[39m\u001b[38;5;124m\"\u001b[39m, rf_mae)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 112\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[9], line 88\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Build and train Random Forest Regressor model\u001b[39;00m\n\u001b[1;32m     87\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m build_rf_model()\n\u001b[0;32m---> 88\u001b[0m rf_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Evaluate models\u001b[39;00m\n\u001b[1;32m     91\u001b[0m cnn_pred \u001b[38;5;241m=\u001b[39m cnn_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:345\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 345\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    346\u001b[0m     X, y, multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mDTYPE\n\u001b[1;32m    347\u001b[0m )\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1122\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1104\u001b[0m     )\n\u001b[1;32m   1106\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1107\u001b[0m     X,\n\u001b[1;32m   1108\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1120\u001b[0m )\n\u001b[0;32m-> 1122\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1124\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1132\u001b[0m, in \u001b[0;36m_check_y\u001b[0;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Isolated part of check_X_y dedicated to y validation\"\"\"\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multi_output:\n\u001b[0;32m-> 1132\u001b[0m     y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1133\u001b[0m         y,\n\u001b[1;32m   1134\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1135\u001b[0m         force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1136\u001b[0m         ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1137\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1138\u001b[0m         input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1139\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m   1140\u001b[0m     )\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1142\u001b[0m     estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    916\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    918\u001b[0m         )\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 921\u001b[0m         _assert_all_finite(\n\u001b[1;32m    922\u001b[0m             array,\n\u001b[1;32m    923\u001b[0m             input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    924\u001b[0m             estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    925\u001b[0m             allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    926\u001b[0m         )\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    929\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m     )\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input y contains NaN."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, LSTM\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_and_preprocess_data(directory):\n",
    "    dfs = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            df = pd.read_csv(filepath)\n",
    "            dfs.append(df)\n",
    "    data = pd.concat(dfs, ignore_index=True)\n",
    "    return data\n",
    "\n",
    "# Function to perform PCA\n",
    "def perform_pca(data):\n",
    "    scaler = StandardScaler()\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    data_imputed = imputer.fit_transform(data)\n",
    "    scaled_data = scaler.fit_transform(data_imputed)\n",
    "    pca = PCA(n_components=10)  # You can change the number of components as per your requirement\n",
    "    pca_data = pca.fit_transform(scaled_data)\n",
    "    return pca_data\n",
    "\n",
    "# Function to build CNN model\n",
    "def build_cnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dense(50, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "# Function to build RNN model\n",
    "def build_rnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(50, activation='relu', input_shape=input_shape),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "# Function to build Random Forest Regressor model\n",
    "def build_rf_model():\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    return model\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    directory = \"/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated_CSV\"\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    data = load_and_preprocess_data(directory)\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, :-1].values\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca_data = perform_pca(X)\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(pca_data, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Build and train CNN model\n",
    "    input_shape = (X_train.shape[1], 1)\n",
    "    cnn_model = build_cnn_model(input_shape)\n",
    "    cnn_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "    \n",
    "    # Build and train RNN model\n",
    "    rnn_model = build_rnn_model(input_shape)\n",
    "    rnn_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "    \n",
    "    # Build and train Random Forest Regressor model\n",
    "    rf_model = build_rf_model()\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate models\n",
    "    cnn_pred = cnn_model.predict(X_test)\n",
    "    rnn_pred = rnn_model.predict(X_test)\n",
    "    rf_pred = rf_model.predict(X_test)\n",
    "    \n",
    "    # Calculate RMSE and MAE\n",
    "    cnn_rmse = np.sqrt(mean_squared_error(y_test, cnn_pred))\n",
    "    rnn_rmse = np.sqrt(mean_squared_error(y_test, rnn_pred))\n",
    "    rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "    \n",
    "    cnn_mae = mean_absolute_error(y_test, cnn_pred)\n",
    "    rnn_mae = mean_absolute_error(y_test, rnn_pred)\n",
    "    rf_mae = mean_absolute_error(y_test, rf_pred)\n",
    "    \n",
    "    print(\"CNN RMSE:\", cnn_rmse)\n",
    "    print(\"CNN MAE:\", cnn_mae)\n",
    "    print(\"RNN RMSE:\", rnn_rmse)\n",
    "    print(\"RNN MAE:\", rnn_mae)\n",
    "    print(\"Random Forest RMSE:\", rf_rmse)\n",
    "    print(\"Random Forest MAE:\", rf_mae)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2e7afdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file: interpolated_insat_on_imerg_20180101.csv\n",
      "Columns in the loaded DataFrame: Index(['longitude', 'latitude', '20180101 0015 IMG_TIR1',\n",
      "       '20180101 0015 IMG_TIR2', '20180101 0015 IMG_WV',\n",
      "       '20180101 0045 IMG_TIR1', '20180101 0045 IMG_TIR2',\n",
      "       '20180101 0045 IMG_WV', '20180101 0115 IMG_TIR1',\n",
      "       '20180101 0115 IMG_TIR2', '20180101 0115 IMG_WV',\n",
      "       '20180101 0215 IMG_TIR1', '20180101 0215 IMG_TIR2',\n",
      "       '20180101 0215 IMG_WV', 'precipitationCal'],\n",
      "      dtype='object')\n",
      "Loading data from file: interpolated_insat_on_imerg_20181222.csv\n",
      "Columns in the loaded DataFrame: Index(['longitude', 'latitude', '20181222 1615 IMG_TIR1',\n",
      "       '20181222 1615 IMG_TIR2', '20181222 1615 IMG_WV',\n",
      "       '20181222 1645 IMG_TIR1', '20181222 1645 IMG_TIR2',\n",
      "       '20181222 1645 IMG_WV', 'precipitationCal'],\n",
      "      dtype='object')\n",
      "Loading data from file: interpolated_insat_on_imerg_20180408.csv\n",
      "Columns in the loaded DataFrame: Index(['longitude', 'latitude', '20180408 2215 IMG_TIR1',\n",
      "       '20180408 2215 IMG_TIR2', '20180408 2215 IMG_WV',\n",
      "       '20180408 2245 IMG_TIR1', '20180408 2245 IMG_TIR2',\n",
      "       '20180408 2245 IMG_WV', '20180408 2315 IMG_TIR1',\n",
      "       '20180408 2315 IMG_TIR2', '20180408 2315 IMG_WV', 'precipitationCal'],\n",
      "      dtype='object')\n",
      "Concatenated DataFrame shape: (75000, 30)\n",
      "Checking for NaN values in the concatenated data:\n",
      "       longitude   latitude  20180101 0015 IMG_TIR1  20180101 0015 IMG_TIR2  \\\n",
      "0      45.049999 -24.950000              525.254494              537.381092   \n",
      "1      45.150005 -24.950000              526.454234              538.401817   \n",
      "2      45.249996 -24.950000              530.277070              540.638535   \n",
      "3      45.350002 -24.950000              529.115770              542.272883   \n",
      "4      45.450008 -24.950000              529.408662              541.954427   \n",
      "...          ...        ...                     ...                     ...   \n",
      "74995  69.550003 -15.049998                     NaN                     NaN   \n",
      "74996  69.650009 -15.049998                     NaN                     NaN   \n",
      "74997  69.750000 -15.049998                     NaN                     NaN   \n",
      "74998  69.850006 -15.049998                     NaN                     NaN   \n",
      "74999  69.950012 -15.049998                     NaN                     NaN   \n",
      "\n",
      "       20180101 0015 IMG_WV  20180101 0045 IMG_TIR1  20180101 0045 IMG_TIR2  \\\n",
      "0                869.727117              523.219427              533.838335   \n",
      "1                867.287898              524.568153              537.856051   \n",
      "2                867.638535              529.542816              540.538629   \n",
      "3                870.421444              528.545766              540.512975   \n",
      "4                873.204331              527.931448              541.068552   \n",
      "...                     ...                     ...                     ...   \n",
      "74995                   NaN                     NaN                     NaN   \n",
      "74996                   NaN                     NaN                     NaN   \n",
      "74997                   NaN                     NaN                     NaN   \n",
      "74998                   NaN                     NaN                     NaN   \n",
      "74999                   NaN                     NaN                     NaN   \n",
      "\n",
      "       20180101 0045 IMG_WV  20180101 0115 IMG_TIR1  20180101 0115 IMG_TIR2  \\\n",
      "0                866.727117              522.800259              533.181351   \n",
      "1                866.000000              523.272883              531.454234   \n",
      "2                867.911418              526.000000              534.638535   \n",
      "3                868.000000              526.967209              536.842887   \n",
      "4                869.204331              529.795669              538.477214   \n",
      "...                     ...                     ...                     ...   \n",
      "74995                   NaN                     NaN                     NaN   \n",
      "74996                   NaN                     NaN                     NaN   \n",
      "74997                   NaN                     NaN                     NaN   \n",
      "74998                   NaN                     NaN                     NaN   \n",
      "74999                   NaN                     NaN                     NaN   \n",
      "\n",
      "       ...  20181222 1645 IMG_WV  20180408 2215 IMG_TIR1  \\\n",
      "0      ...                   NaN                     NaN   \n",
      "1      ...                   NaN                     NaN   \n",
      "2      ...                   NaN                     NaN   \n",
      "3      ...                   NaN                     NaN   \n",
      "4      ...                   NaN                     NaN   \n",
      "...    ...                   ...                     ...   \n",
      "74995  ...                   NaN              565.558888   \n",
      "74996  ...                   NaN              576.333084   \n",
      "74997  ...                   NaN              606.204437   \n",
      "74998  ...                   NaN              576.640859   \n",
      "74999  ...                   NaN              560.372553   \n",
      "\n",
      "       20180408 2215 IMG_TIR2  20180408 2215 IMG_WV  20180408 2245 IMG_TIR1  \\\n",
      "0                         NaN                   NaN                     NaN   \n",
      "1                         NaN                   NaN                     NaN   \n",
      "2                         NaN                   NaN                     NaN   \n",
      "3                         NaN                   NaN                     NaN   \n",
      "4                         NaN                   NaN                     NaN   \n",
      "...                       ...                   ...                     ...   \n",
      "74995              605.160393            888.709902              564.771685   \n",
      "74996              607.103166            889.112585              580.526151   \n",
      "74997              629.932792            887.000000              569.932059   \n",
      "74998              613.335391            887.379537              577.597653   \n",
      "74999              599.339721            892.841112              568.306717   \n",
      "\n",
      "       20180408 2245 IMG_TIR2  20180408 2245 IMG_WV  20180408 2315 IMG_TIR1  \\\n",
      "0                         NaN                   NaN                     NaN   \n",
      "1                         NaN                   NaN                     NaN   \n",
      "2                         NaN                   NaN                     NaN   \n",
      "3                         NaN                   NaN                     NaN   \n",
      "4                         NaN                   NaN                     NaN   \n",
      "...                       ...                   ...                     ...   \n",
      "74995              602.268790            886.841401              552.688594   \n",
      "74996              609.573399            886.868501              553.732253   \n",
      "74997              608.525265            886.275295              548.660414   \n",
      "74998              607.627575            887.737001              546.014961   \n",
      "74999              605.539666            886.895889              545.868501   \n",
      "\n",
      "       20180408 2315 IMG_TIR2  20180408 2315 IMG_WV  \n",
      "0                         NaN                   NaN  \n",
      "1                         NaN                   NaN  \n",
      "2                         NaN                   NaN  \n",
      "3                         NaN                   NaN  \n",
      "4                         NaN                   NaN  \n",
      "...                       ...                   ...  \n",
      "74995              588.294095            886.868501  \n",
      "74996              592.863753            885.131499  \n",
      "74997              584.767322            884.406794  \n",
      "74998              582.678732            883.868501  \n",
      "74999              585.523336            885.000000  \n",
      "\n",
      "[75000 rows x 30 columns]\n",
      "Unique values in the target variable (y): [-24.95       -24.850002   -24.749996   ... 995.54623267 996.\n",
      "          nan]\n",
      "Epoch 1/10\n",
      "1641/1641 [==============================] - 1s 655us/step - loss: nan - mse: nan\n",
      "Epoch 2/10\n",
      "1641/1641 [==============================] - 1s 658us/step - loss: nan - mse: nan\n",
      "Epoch 3/10\n",
      "1641/1641 [==============================] - 1s 617us/step - loss: nan - mse: nan\n",
      "Epoch 4/10\n",
      "1641/1641 [==============================] - 1s 587us/step - loss: nan - mse: nan\n",
      "Epoch 5/10\n",
      "1641/1641 [==============================] - 1s 569us/step - loss: nan - mse: nan\n",
      "Epoch 6/10\n",
      "1641/1641 [==============================] - 1s 590us/step - loss: nan - mse: nan\n",
      "Epoch 7/10\n",
      "1641/1641 [==============================] - 1s 621us/step - loss: nan - mse: nan\n",
      "Epoch 8/10\n",
      "1641/1641 [==============================] - 1s 613us/step - loss: nan - mse: nan\n",
      "Epoch 9/10\n",
      "1641/1641 [==============================] - 1s 589us/step - loss: nan - mse: nan\n",
      "Epoch 10/10\n",
      "1641/1641 [==============================] - 1s 599us/step - loss: nan - mse: nan\n",
      "Epoch 1/10\n",
      "1641/1641 [==============================] - 3s 2ms/step - loss: nan - mse: nan\n",
      "Epoch 2/10\n",
      "1641/1641 [==============================] - 3s 2ms/step - loss: nan - mse: nan\n",
      "Epoch 3/10\n",
      "1641/1641 [==============================] - 3s 2ms/step - loss: nan - mse: nan\n",
      "Epoch 4/10\n",
      "1641/1641 [==============================] - 3s 2ms/step - loss: nan - mse: nan\n",
      "Epoch 5/10\n",
      "1641/1641 [==============================] - 3s 2ms/step - loss: nan - mse: nan\n",
      "Epoch 6/10\n",
      "1641/1641 [==============================] - 3s 2ms/step - loss: nan - mse: nan\n",
      "Epoch 7/10\n",
      "1641/1641 [==============================] - 3s 2ms/step - loss: nan - mse: nan\n",
      "Epoch 8/10\n",
      "1641/1641 [==============================] - 3s 2ms/step - loss: nan - mse: nan\n",
      "Epoch 9/10\n",
      "1641/1641 [==============================] - 3s 2ms/step - loss: nan - mse: nan\n",
      "Epoch 10/10\n",
      "1641/1641 [==============================] - 3s 2ms/step - loss: nan - mse: nan\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input y contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 118\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom Forest MAE:\u001b[39m\u001b[38;5;124m\"\u001b[39m, rf_mae)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 118\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[14], line 94\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Build and train Random Forest Regressor model\u001b[39;00m\n\u001b[1;32m     93\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m build_rf_model()\n\u001b[0;32m---> 94\u001b[0m rf_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Evaluate models\u001b[39;00m\n\u001b[1;32m     97\u001b[0m cnn_pred \u001b[38;5;241m=\u001b[39m cnn_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:345\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 345\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    346\u001b[0m     X, y, multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mDTYPE\n\u001b[1;32m    347\u001b[0m )\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1122\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1104\u001b[0m     )\n\u001b[1;32m   1106\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1107\u001b[0m     X,\n\u001b[1;32m   1108\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1120\u001b[0m )\n\u001b[0;32m-> 1122\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1124\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1132\u001b[0m, in \u001b[0;36m_check_y\u001b[0;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Isolated part of check_X_y dedicated to y validation\"\"\"\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multi_output:\n\u001b[0;32m-> 1132\u001b[0m     y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1133\u001b[0m         y,\n\u001b[1;32m   1134\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1135\u001b[0m         force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1136\u001b[0m         ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1137\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1138\u001b[0m         input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1139\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m   1140\u001b[0m     )\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1142\u001b[0m     estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    916\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    918\u001b[0m         )\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 921\u001b[0m         _assert_all_finite(\n\u001b[1;32m    922\u001b[0m             array,\n\u001b[1;32m    923\u001b[0m             input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    924\u001b[0m             estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    925\u001b[0m             allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    926\u001b[0m         )\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    929\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m     )\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input y contains NaN."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, LSTM\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_and_preprocess_data(directory):\n",
    "    dfs = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            print(f\"Loading data from file: {filename}\")\n",
    "            df = pd.read_csv(filepath)\n",
    "            print(\"Columns in the loaded DataFrame:\", df.columns)\n",
    "            dfs.append(df)\n",
    "    data = pd.concat(dfs, ignore_index=True)\n",
    "    print(\"Concatenated DataFrame shape:\", data.shape)\n",
    "    print(\"Checking for NaN values in the concatenated data:\")\n",
    "    print(data[data.isnull().any(axis=1)])  # Check for rows with NaN values in any column\n",
    "    return data\n",
    "\n",
    "# Function to perform PCA\n",
    "def perform_pca(data):\n",
    "    scaler = StandardScaler()\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    data_imputed = imputer.fit_transform(data)\n",
    "    scaled_data = scaler.fit_transform(data_imputed)\n",
    "    pca = PCA(n_components=10)  # You can change the number of components as per your requirement\n",
    "    pca_data = pca.fit_transform(scaled_data)\n",
    "    return pca_data\n",
    "\n",
    "# Function to build CNN model\n",
    "def build_cnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dense(50, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "# Function to build RNN model\n",
    "def build_rnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(50, activation='relu', input_shape=input_shape),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "# Function to build Random Forest Regressor model\n",
    "def build_rf_model():\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    return model\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    directory = \"/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated_CSV\"\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    data = load_and_preprocess_data(directory)\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, :-1].values\n",
    "    \n",
    "    # Print unique values in y to understand which values are considered as NaN\n",
    "    print(\"Unique values in the target variable (y):\", np.unique(y))\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca_data = perform_pca(X)\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(pca_data, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Build and train CNN model\n",
    "    input_shape = (X_train.shape[1], 1)\n",
    "    cnn_model = build_cnn_model(input_shape)\n",
    "    cnn_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "    \n",
    "    # Build and train RNN model\n",
    "    rnn_model = build_rnn_model(input_shape)\n",
    "    rnn_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "    \n",
    "    # Build and train Random Forest Regressor model\n",
    "    rf_model = build_rf_model()\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate models\n",
    "    cnn_pred = cnn_model.predict(X_test)\n",
    "    rnn_pred = rnn_model.predict(X_test)\n",
    "    rf_pred = rf_model.predict(X_test)\n",
    "    \n",
    "    # Calculate RMSE and MAE\n",
    "    cnn_rmse = np.sqrt(mean_squared_error(y_test, cnn_pred))\n",
    "    rnn_rmse = np.sqrt(mean_squared_error(y_test, rnn_pred))\n",
    "    rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "    \n",
    "    cnn_mae = mean_absolute_error(y_test, cnn_pred)\n",
    "    rnn_mae = mean_absolute_error(y_test, rnn_pred)\n",
    "    rf_mae = mean_absolute_error(y_test, rf_pred)\n",
    "    \n",
    "    print(\"CNN RMSE:\", cnn_rmse)\n",
    "    print(\"CNN MAE:\", cnn_mae)\n",
    "    print(\"RNN RMSE:\", rnn_rmse)\n",
    "    print(\"RNN MAE:\", rnn_mae)\n",
    "    print(\"Random Forest RMSE:\", rf_rmse)\n",
    "    print(\"Random Forest MAE:\", rf_mae)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68416909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file: interpolated_insat_on_imerg_20180101.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'interpolated_insat_on_imerg_20180101.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 75\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCNN MAE:\u001b[39m\u001b[38;5;124m\"\u001b[39m, cnn_mae)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 75\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[15], line 51\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     file_paths \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterpolated_insat_on_imerg_20180101.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterpolated_insat_on_imerg_20181222.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterpolated_insat_on_imerg_20180408.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m     ]\n\u001b[0;32m---> 51\u001b[0m     df \u001b[38;5;241m=\u001b[39m load_data(file_paths)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# Preprocess data\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     df \u001b[38;5;241m=\u001b[39m preprocess_data(df)\n",
      "Cell \u001b[0;32mIn[15], line 14\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(file_paths)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m file_paths:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading data from file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns in the loaded DataFrame: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m     dfs\u001b[38;5;241m.\u001b[39mappend(df)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'interpolated_insat_on_imerg_20180101.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "def load_data(file_paths):\n",
    "    dfs = []\n",
    "    for file_path in file_paths:\n",
    "        print(f\"Loading data from file: {file_path}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Columns in the loaded DataFrame: {df.columns}\")\n",
    "        dfs.append(df)\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "def preprocess_data(df):\n",
    "    # Drop rows with NaN values in target variable\n",
    "    df.dropna(subset=['precipitationCal'], inplace=True)\n",
    "    # Fill NaN values in predictor variables with mean\n",
    "    df.fillna(df.mean(), inplace=True)\n",
    "    return df\n",
    "\n",
    "def split_data(df):\n",
    "    X = df.drop(columns=['precipitationCal'])\n",
    "    y = df['precipitationCal']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def build_rf_model():\n",
    "    return RandomForestRegressor(random_state=42)\n",
    "\n",
    "def build_cnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(input_shape,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    file_paths = [\n",
    "        \"interpolated_insat_on_imerg_20180101.csv\",\n",
    "        \"interpolated_insat_on_imerg_20181222.csv\",\n",
    "        \"interpolated_insat_on_imerg_20180408.csv\"\n",
    "    ]\n",
    "    df = load_data(file_paths)\n",
    "    # Preprocess data\n",
    "    df = preprocess_data(df)\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = split_data(df)\n",
    "\n",
    "    # Build and train Random Forest Regressor model\n",
    "    rf_model = build_rf_model()\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # Build and train CNN model\n",
    "    cnn_model = build_cnn_model(X_train.shape[1])\n",
    "    cnn_model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "    # Evaluate models\n",
    "    rf_pred = rf_model.predict(X_test)\n",
    "    cnn_pred = cnn_model.predict(X_test)\n",
    "    rf_mae = mean_absolute_error(y_test, rf_pred)\n",
    "    cnn_mae = mean_absolute_error(y_test, cnn_pred)\n",
    "\n",
    "    print(\"Random Forest MAE:\", rf_mae)\n",
    "    print(\"CNN MAE:\", cnn_mae)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22dd398f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file: interpolated_insat_on_imerg_20180101.csv\n",
      "Loading data from file: interpolated_insat_on_imerg_20181222.csv\n",
      "Loading data from file: interpolated_insat_on_imerg_20180408.csv\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 109\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCNN MAE:\u001b[39m\u001b[38;5;124m\"\u001b[39m, cnn_mae)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 109\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[17], line 88\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m X, y \u001b[38;5;241m=\u001b[39m preprocess_data(df)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Split the data into training and testing sets\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Build and train Random Forest Regressor model\u001b[39;00m\n\u001b[1;32m     91\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m build_rf_model()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2562\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2559\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[1;32m   2561\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m-> 2562\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2563\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[1;32m   2564\u001b[0m )\n\u001b[1;32m   2566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m   2567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2236\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2233\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[1;32m   2235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2238\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2239\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[1;32m   2240\u001b[0m     )\n\u001b[1;32m   2242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def load_data_from_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Load and concatenate data from CSV files in a directory.\n",
    "\n",
    "    Args:\n",
    "    - directory_path (str): Path to the directory containing CSV files.\n",
    "\n",
    "    Returns:\n",
    "    - df (DataFrame): Concatenated DataFrame containing data from all CSV files.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            print(\"Loading data from file:\", filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            dfs.append(df)\n",
    "    concatenated_df = pd.concat(dfs, ignore_index=True)\n",
    "    return concatenated_df\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess the concatenated DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - df (DataFrame): Concatenated DataFrame containing data from all CSV files.\n",
    "\n",
    "    Returns:\n",
    "    - X (ndarray): Features.\n",
    "    - y (ndarray): Target variable.\n",
    "    \"\"\"\n",
    "    # Drop rows with NaN values\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Extract features (X) and target variable (y)\n",
    "    X = df.drop(columns=['precipitationCal'])  # Assuming 'precipitationCal' is the target variable\n",
    "    y = df['precipitationCal'].values\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def build_rf_model():\n",
    "    \"\"\"\n",
    "    Build Random Forest Regressor model.\n",
    "\n",
    "    Returns:\n",
    "    - rf_model: Random Forest Regressor model.\n",
    "    \"\"\"\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    return rf_model\n",
    "\n",
    "def build_cnn_model(input_shape):\n",
    "    \"\"\"\n",
    "    Build Convolutional Neural Network (CNN) model.\n",
    "\n",
    "    Args:\n",
    "    - input_shape (tuple): Shape of the input data.\n",
    "\n",
    "    Returns:\n",
    "    - cnn_model: CNN model.\n",
    "    \"\"\"\n",
    "    cnn_model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=input_shape),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    cnn_model.compile(optimizer=Adam(), loss='mse', metrics=['mae'])\n",
    "    return cnn_model\n",
    "\n",
    "def main():\n",
    "    directory_path = \"/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated_CSV\"  # Change this to your directory path\n",
    "    df = load_data_from_directory(directory_path)\n",
    "\n",
    "    X, y = preprocess_data(df)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Build and train Random Forest Regressor model\n",
    "    rf_model = build_rf_model()\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # Build and train CNN model\n",
    "    input_shape = (X_train.shape[1],)\n",
    "    cnn_model = build_cnn_model(input_shape)\n",
    "    cnn_model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "    # Evaluate models\n",
    "    rf_pred = rf_model.predict(X_test)\n",
    "    rf_mae = mean_absolute_error(y_test, rf_pred)\n",
    "    print(\"Random Forest MAE:\", rf_mae)\n",
    "\n",
    "    cnn_pred = cnn_model.predict(X_test)\n",
    "    cnn_mae = mean_absolute_error(y_test, cnn_pred)\n",
    "    print(\"CNN MAE:\", cnn_mae)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f019be51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "547/547 [==============================] - 1s 732us/step - loss: 217619.2500 - mse: 217619.2500\n",
      "Epoch 2/10\n",
      "547/547 [==============================] - 0s 693us/step - loss: 6821.0605 - mse: 6821.0605\n",
      "Epoch 3/10\n",
      "547/547 [==============================] - 0s 681us/step - loss: 3546.2883 - mse: 3546.2883\n",
      "Epoch 4/10\n",
      "547/547 [==============================] - 0s 658us/step - loss: 1787.6882 - mse: 1787.6882\n",
      "Epoch 5/10\n",
      "547/547 [==============================] - 0s 622us/step - loss: 995.5864 - mse: 995.5864\n",
      "Epoch 6/10\n",
      "547/547 [==============================] - 0s 636us/step - loss: 616.7054 - mse: 616.7054\n",
      "Epoch 7/10\n",
      "547/547 [==============================] - 0s 666us/step - loss: 435.9445 - mse: 435.9445\n",
      "Epoch 8/10\n",
      "547/547 [==============================] - 0s 650us/step - loss: 354.7572 - mse: 354.7572\n",
      "Epoch 9/10\n",
      "547/547 [==============================] - 0s 649us/step - loss: 319.2118 - mse: 319.2118\n",
      "Epoch 10/10\n",
      "547/547 [==============================] - 0s 630us/step - loss: 301.0358 - mse: 301.0358\n",
      "Epoch 1/10\n",
      "547/547 [==============================] - 2s 2ms/step - loss: 98783.6953 - mse: 98783.6953\n",
      "Epoch 2/10\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 303.3221 - mse: 303.3221\n",
      "Epoch 3/10\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 226.9205 - mse: 226.9205\n",
      "Epoch 4/10\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 224.9584 - mse: 224.9584\n",
      "Epoch 5/10\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 191.4371 - mse: 191.4371\n",
      "Epoch 6/10\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 177.3436 - mse: 177.3436\n",
      "Epoch 7/10\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 177.0860 - mse: 177.0860\n",
      "Epoch 8/10\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 173.5383 - mse: 173.5383\n",
      "Epoch 9/10\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 164.4057 - mse: 164.4057\n",
      "Epoch 10/10\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 162.8761 - mse: 162.8761\n",
      "235/235 [==============================] - 0s 319us/step\n",
      "235/235 [==============================] - 0s 667us/step\n",
      "CNN RMSE: 16.80658827404362\n",
      "CNN MAE: 12.745022051357113\n",
      "RNN RMSE: 10.592317546901056\n",
      "RNN MAE: 7.942599743213869\n",
      "Random Forest RMSE: 5.42882076631188\n",
      "Random Forest MAE: 3.172783383050234\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, LSTM\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_and_preprocess_data(directory):\n",
    "    dfs = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            df = pd.read_csv(filepath)\n",
    "            dfs.append(df)\n",
    "    data = pd.concat(dfs, ignore_index=True)\n",
    "    return data\n",
    "\n",
    "# Function to perform PCA\n",
    "def perform_pca(data):\n",
    "    scaler = StandardScaler()\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    data_imputed = imputer.fit_transform(data)\n",
    "    scaled_data = scaler.fit_transform(data_imputed)\n",
    "    pca = PCA(n_components=10)  # You can change the number of components as per your requirement\n",
    "    pca_data = pca.fit_transform(scaled_data)\n",
    "    return pca_data\n",
    "\n",
    "# Function to build CNN model\n",
    "def build_cnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dense(50, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "# Function to build RNN model\n",
    "def build_rnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(50, activation='relu', input_shape=input_shape),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "# Function to build Random Forest Regressor model\n",
    "def build_rf_model():\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    return model\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    directory = \"/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated_CSV\"\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    data = load_and_preprocess_data(directory)\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "    \n",
    "    # Remove NaN values from y\n",
    "    not_nan_indices = ~np.isnan(y)\n",
    "    X = X[not_nan_indices]\n",
    "    y = y[not_nan_indices]\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca_data = perform_pca(X)\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(pca_data, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Build and train CNN model\n",
    "    input_shape = (X_train.shape[1], 1)\n",
    "    cnn_model = build_cnn_model(input_shape)\n",
    "    cnn_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "    \n",
    "    # Build and train RNN model\n",
    "    rnn_model = build_rnn_model(input_shape)\n",
    "    rnn_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "    \n",
    "    # Build and train Random Forest Regressor model\n",
    "    rf_model = build_rf_model()\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate models\n",
    "    cnn_pred = cnn_model.predict(X_test)\n",
    "    rnn_pred = rnn_model.predict(X_test)\n",
    "    rf_pred = rf_model.predict(X_test)\n",
    "    \n",
    "    # Calculate RMSE and MAE\n",
    "    cnn_rmse = np.sqrt(mean_squared_error(y_test, cnn_pred))\n",
    "    rnn_rmse = np.sqrt(mean_squared_error(y_test, rnn_pred))\n",
    "    rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "    \n",
    "    cnn_mae = mean_absolute_error(y_test, cnn_pred)\n",
    "    rnn_mae = mean_absolute_error(y_test, rnn_pred)\n",
    "    rf_mae = mean_absolute_error(y_test, rf_pred)\n",
    "    \n",
    "    print(\"CNN RMSE:\", cnn_rmse)\n",
    "    print(\"CNN MAE:\", cnn_mae)\n",
    "    print(\"RNN RMSE:\", rnn_rmse)\n",
    "    print(\"RNN MAE:\", rnn_mae)\n",
    "    print(\"Random Forest RMSE:\", rf_rmse)\n",
    "    print(\"Random Forest MAE:\", rf_mae)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d94d9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated_CSV/interpolated_insat_on_imerg_20180105.csv\n",
      "Loaded data shape: (25000, 12)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated_CSV/interpolated_insat_on_imerg_20180101.csv\n",
      "Loaded data shape: (25000, 15)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated_CSV/interpolated_insat_on_imerg_20181222.csv\n",
      "Loaded data shape: (25000, 9)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated_CSV/interpolated_insat_on_imerg_20180408.csv\n",
      "Loaded data shape: (25000, 12)\n",
      "Concatenated data shape: (100000, 39)\n",
      "Removing NaN values from target variable (y)...\n",
      "Data shape after removing NaN values: (25000, 38), (25000,)\n",
      "Performing PCA...\n",
      "Data shape after PCA: (25000, 10)\n",
      "PCA completed.\n",
      "Splitting data into training and testing sets...\n",
      "Building and training CNN model...\n",
      "Building CNN model...\n",
      "CNN model built.\n",
      "Epoch 1/10\n",
      "547/547 [==============================] - 1s 692us/step - loss: 226736.2500 - mse: 226736.2500\n",
      "Epoch 2/10\n",
      "547/547 [==============================] - 0s 666us/step - loss: 6670.0234 - mse: 6670.0234\n",
      "Epoch 3/10\n",
      "547/547 [==============================] - 0s 692us/step - loss: 3611.0842 - mse: 3611.0842\n",
      "Epoch 4/10\n",
      "547/547 [==============================] - 0s 750us/step - loss: 1856.4877 - mse: 1856.4877\n",
      "Epoch 5/10\n",
      "547/547 [==============================] - 0s 671us/step - loss: 1030.8037 - mse: 1030.8037\n",
      "Epoch 6/10\n",
      "547/547 [==============================] - 0s 652us/step - loss: 644.4373 - mse: 644.4373\n",
      "Epoch 7/10\n",
      "547/547 [==============================] - 0s 676us/step - loss: 459.4130 - mse: 459.4130\n",
      "Epoch 8/10\n",
      "547/547 [==============================] - 0s 651us/step - loss: 374.6193 - mse: 374.6193\n",
      "Epoch 9/10\n",
      "547/547 [==============================] - 0s 654us/step - loss: 337.4656 - mse: 337.4656\n",
      "Epoch 10/10\n",
      "547/547 [==============================] - 0s 655us/step - loss: 317.5347 - mse: 317.5347\n",
      "Building and training RNN model...\n",
      "Building RNN model...\n",
      "RNN model built.\n",
      "Epoch 1/10\n",
      "547/547 [==============================] - 2s 2ms/step - loss: 115429.1016 - mse: 115429.1016\n",
      "Epoch 2/10\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 439.2095 - mse: 439.2095\n",
      "Epoch 3/10\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 298.9689 - mse: 298.9689\n",
      "Epoch 4/10\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 275.7096 - mse: 275.7096\n",
      "Epoch 5/10\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 203.0140 - mse: 203.0140\n",
      "Epoch 6/10\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 176.7985 - mse: 176.7985\n",
      "Epoch 7/10\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 176.1457 - mse: 176.1457\n",
      "Epoch 8/10\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 146.6206 - mse: 146.6206\n",
      "Epoch 9/10\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 154.3633 - mse: 154.3633\n",
      "Epoch 10/10\n",
      "547/547 [==============================] - 1s 2ms/step - loss: 142.3801 - mse: 142.3801\n",
      "Building and training Random Forest Regressor model...\n",
      "Building Random Forest Regressor model...\n",
      "Random Forest Regressor model built.\n",
      "Evaluating models...\n",
      "235/235 [==============================] - 0s 335us/step\n",
      "235/235 [==============================] - 0s 691us/step\n",
      "Evaluation results:\n",
      "CNN RMSE: 17.35707858322409\n",
      "CNN MAE: 13.169759055593877\n",
      "RNN RMSE: 9.489419380064051\n",
      "RNN MAE: 7.036330234376669\n",
      "Random Forest RMSE: 5.42882076631188\n",
      "Random Forest MAE: 3.172783383050234\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, LSTM\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_and_preprocess_data(directory):\n",
    "    dfs = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            print(f\"Loading data from: {filepath}\")\n",
    "            df = pd.read_csv(filepath)\n",
    "            print(f\"Loaded data shape: {df.shape}\")\n",
    "            dfs.append(df)\n",
    "    data = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"Concatenated data shape: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# Function to perform PCA\n",
    "def perform_pca(data):\n",
    "    print(\"Performing PCA...\")\n",
    "    scaler = StandardScaler()\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    data_imputed = imputer.fit_transform(data)\n",
    "    scaled_data = scaler.fit_transform(data_imputed)\n",
    "    pca = PCA(n_components=10)  # You can change the number of components as per your requirement\n",
    "    pca_data = pca.fit_transform(scaled_data)\n",
    "    print(f\"Data shape after PCA: {pca_data.shape}\")\n",
    "    print(\"PCA completed.\")\n",
    "    return pca_data\n",
    "\n",
    "# Function to build CNN model\n",
    "def build_cnn_model(input_shape):\n",
    "    print(\"Building CNN model...\")\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dense(50, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "    print(\"CNN model built.\")\n",
    "    return model\n",
    "\n",
    "# Function to build RNN model\n",
    "def build_rnn_model(input_shape):\n",
    "    print(\"Building RNN model...\")\n",
    "    model = Sequential([\n",
    "        LSTM(50, activation='relu', input_shape=input_shape),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "    print(\"RNN model built.\")\n",
    "    return model\n",
    "\n",
    "# Function to build Random Forest Regressor model\n",
    "def build_rf_model():\n",
    "    print(\"Building Random Forest Regressor model...\")\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    print(\"Random Forest Regressor model built.\")\n",
    "    return model\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    directory = \"/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated_CSV\"\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    data = load_and_preprocess_data(directory)\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "    \n",
    "    # Remove NaN values from y\n",
    "    print(\"Removing NaN values from target variable (y)...\")\n",
    "    not_nan_indices = ~np.isnan(y)\n",
    "    X = X[not_nan_indices]\n",
    "    y = y[not_nan_indices]\n",
    "    \n",
    "    # Print data shape after removing NaN values\n",
    "    print(f\"Data shape after removing NaN values: {X.shape}, {y.shape}\")\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca_data = perform_pca(X)\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    print(\"Splitting data into training and testing sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(pca_data, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Build and train CNN model\n",
    "    print(\"Building and training CNN model...\")\n",
    "    input_shape = (X_train.shape[1], 1)\n",
    "    cnn_model = build_cnn_model(input_shape)\n",
    "    cnn_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "    \n",
    "    # Build and train RNN model\n",
    "    print(\"Building and training RNN model...\")\n",
    "    rnn_model = build_rnn_model(input_shape)\n",
    "    rnn_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "    \n",
    "    # Build and train Random Forest Regressor model\n",
    "    print(\"Building and training Random Forest Regressor model...\")\n",
    "    rf_model = build_rf_model()\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate models\n",
    "    print(\"Evaluating models...\")\n",
    "    cnn_pred = cnn_model.predict(X_test)\n",
    "    rnn_pred = rnn_model.predict(X_test)\n",
    "    rf_pred = rf_model.predict(X_test)\n",
    "    \n",
    "    # Calculate RMSE and MAE\n",
    "    cnn_rmse = np.sqrt(mean_squared_error(y_test, cnn_pred))\n",
    "    rnn_rmse = np.sqrt(mean_squared_error(y_test, rnn_pred))\n",
    "    rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "    \n",
    "    cnn_mae = mean_absolute_error(y_test, cnn_pred)\n",
    "    rnn_mae = mean_absolute_error(y_test, rnn_pred)\n",
    "    rf_mae = mean_absolute_error(y_test, rf_pred)\n",
    "    \n",
    "    # Print evaluation results\n",
    "    print(\"Evaluation results:\")\n",
    "    print(\"CNN RMSE:\", cnn_rmse)\n",
    "    print(\"CNN MAE:\", cnn_mae)\n",
    "    print(\"RNN RMSE:\", rnn_rmse)\n",
    "    print(\"RNN MAE:\", rnn_mae)\n",
    "    print(\"Random Forest RMSE:\", rf_rmse)\n",
    "    print(\"Random Forest MAE:\", rf_mae)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bffeaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20170109.csv\n",
      "Loaded data shape: (25000, 145)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20170108.csv\n",
      "Loaded data shape: (25000, 148)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20190101.csv\n",
      "Loaded data shape: (25000, 142)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20180104.csv\n",
      "Loaded data shape: (25000, 145)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20180110.csv\n",
      "Loaded data shape: (25000, 148)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20180105.csv\n",
      "Loaded data shape: (25000, 142)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20190102.csv\n",
      "Loaded data shape: (25000, 142)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20180107.csv\n",
      "Loaded data shape: (25000, 148)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20180106.csv\n",
      "Loaded data shape: (25000, 148)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20190103.csv\n",
      "Loaded data shape: (25000, 139)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20190107.csv\n",
      "Loaded data shape: (25000, 106)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20180102.csv\n",
      "Loaded data shape: (25000, 145)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20180103.csv\n",
      "Loaded data shape: (25000, 139)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20190106.csv\n",
      "Loaded data shape: (25000, 115)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20190110.csv\n",
      "Loaded data shape: (25000, 148)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20190104.csv\n",
      "Loaded data shape: (25000, 139)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20180101.csv\n",
      "Loaded data shape: (25000, 145)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20190105.csv\n",
      "Loaded data shape: (25000, 127)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20190108.csv\n",
      "Loaded data shape: (25000, 82)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20190109.csv\n",
      "Loaded data shape: (25000, 115)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20180108.csv\n",
      "Loaded data shape: (25000, 145)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20180109.csv\n",
      "Loaded data shape: (25000, 145)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20230204.csv\n",
      "Loaded data shape: (25000, 136)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20170101.csv\n",
      "Loaded data shape: (25000, 148)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20170103.csv\n",
      "Loaded data shape: (25000, 148)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20170102.csv\n",
      "Loaded data shape: (25000, 139)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20230415.csv\n",
      "Loaded data shape: (25000, 121)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20170106.csv\n",
      "Loaded data shape: (25000, 145)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20170107.csv\n",
      "Loaded data shape: (25000, 148)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20170105.csv\n",
      "Loaded data shape: (25000, 148)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20170104.csv\n",
      "Loaded data shape: (25000, 145)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20170110.csv\n",
      "Loaded data shape: (25000, 148)\n",
      "Concatenated data shape: (800000, 148)\n",
      "Removing NaN values from target variable (y)...\n",
      "Data shape after removing NaN values: (725000, 147), (725000,)\n",
      "Performing PCA...\n",
      "Data shape after PCA: (725000, 10)\n",
      "PCA completed.\n",
      "Splitting data into training and testing sets...\n",
      "Building and training CNN model...\n",
      "Building CNN model...\n",
      "CNN model built.\n",
      "Epoch 1/10\n",
      "15860/15860 [==============================] - 9s 550us/step - loss: 8399.1807 - mse: 8399.1807\n",
      "Epoch 2/10\n",
      "15860/15860 [==============================] - 9s 544us/step - loss: 92.1357 - mse: 92.1357\n",
      "Epoch 3/10\n",
      "15860/15860 [==============================] - 9s 548us/step - loss: 91.1380 - mse: 91.1380\n",
      "Epoch 4/10\n",
      "15860/15860 [==============================] - 8s 534us/step - loss: 90.5025 - mse: 90.5025\n",
      "Epoch 5/10\n",
      "15860/15860 [==============================] - 9s 547us/step - loss: 90.2090 - mse: 90.2090\n",
      "Epoch 6/10\n",
      "15860/15860 [==============================] - 9s 540us/step - loss: 89.5284 - mse: 89.5284\n",
      "Epoch 7/10\n",
      "15860/15860 [==============================] - 9s 542us/step - loss: 88.5323 - mse: 88.5323\n",
      "Epoch 8/10\n",
      "15860/15860 [==============================] - 9s 543us/step - loss: 88.1822 - mse: 88.1822\n",
      "Epoch 9/10\n",
      "15860/15860 [==============================] - 9s 538us/step - loss: 87.6348 - mse: 87.6348\n",
      "Epoch 10/10\n",
      "15860/15860 [==============================] - 9s 540us/step - loss: 87.5244 - mse: 87.5244\n",
      "Building and training RNN model...\n",
      "Building RNN model...\n",
      "RNN model built.\n",
      "Epoch 1/10\n",
      "15860/15860 [==============================] - 33s 2ms/step - loss: 3292.3853 - mse: 3292.3853\n",
      "Epoch 2/10\n",
      "15860/15860 [==============================] - 36s 2ms/step - loss: 127.7380 - mse: 127.7380\n",
      "Epoch 3/10\n",
      "15860/15860 [==============================] - 34s 2ms/step - loss: 113.5446 - mse: 113.5446\n",
      "Epoch 4/10\n",
      "15860/15860 [==============================] - 33s 2ms/step - loss: 103.2507 - mse: 103.2507\n",
      "Epoch 5/10\n",
      "15860/15860 [==============================] - 33s 2ms/step - loss: 96.1091 - mse: 96.1091\n",
      "Epoch 6/10\n",
      "15860/15860 [==============================] - 33s 2ms/step - loss: 91.8679 - mse: 91.8679\n",
      "Epoch 7/10\n",
      "15860/15860 [==============================] - 35s 2ms/step - loss: 88.7714 - mse: 88.7714\n",
      "Epoch 8/10\n",
      "15860/15860 [==============================] - 37s 2ms/step - loss: 85.9207 - mse: 85.9207\n",
      "Epoch 9/10\n",
      "15860/15860 [==============================] - 31s 2ms/step - loss: 83.6709 - mse: 83.6709\n",
      "Epoch 10/10\n",
      "15860/15860 [==============================] - 35s 2ms/step - loss: 134.5690 - mse: 134.5690\n",
      "Building and training Random Forest Regressor model...\n",
      "Building Random Forest Regressor model...\n",
      "Random Forest Regressor model built.\n",
      "Evaluating models...\n",
      "6797/6797 [==============================] - 2s 277us/step\n",
      "6797/6797 [==============================] - 4s 603us/step\n",
      "CNN Predictions: [[909.46234]\n",
      " [872.4376 ]\n",
      " [807.4353 ]\n",
      " ...\n",
      " [892.77045]\n",
      " [897.59424]\n",
      " [895.8862 ]]\n",
      "RNN Predictions: [[914.83734]\n",
      " [871.7553 ]\n",
      " [815.7453 ]\n",
      " ...\n",
      " [903.05554]\n",
      " [908.0399 ]\n",
      " [895.85657]]\n",
      "Random Forest Predictions: [909.05662712 866.66324717 816.19465849 ... 893.27651642 899.76216258\n",
      " 897.26084839]\n",
      "Evaluation results:\n",
      "CNN RMSE: 9.245142839740357\n",
      "CNN MAE: 6.637019836743903\n",
      "CNN MSE: 85.47266612720239\n",
      "RNN RMSE: 10.605155744449837\n",
      "RNN MAE: 8.619757988468692\n",
      "RNN MSE: 112.46932836403737\n",
      "Random Forest RMSE: 6.313692570932389\n",
      "Random Forest MAE: 3.966240451426452\n",
      "Random Forest MSE: 39.86271388024684\n",
      "Model saved at: /Users/kunalpathak9826/cnn_model.h5\n",
      "Model saved at: /Users/kunalpathak9826/rnn_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunalpathak9826/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RandomForestRegressor' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 190\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mabsolute_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 190\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[1], line 184\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    182\u001b[0m models \u001b[38;5;241m=\u001b[39m [cnn_model, rnn_model, rf_model]\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model, filename \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(models, model_filenames):\n\u001b[0;32m--> 184\u001b[0m     model\u001b[38;5;241m.\u001b[39msave(filename)\n\u001b[1;32m    185\u001b[0m     absolute_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(filename)\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mabsolute_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RandomForestRegressor' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, LSTM\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_and_preprocess_data(directory):\n",
    "    dfs = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            print(f\"Loading data from: {filepath}\")\n",
    "            df = pd.read_csv(filepath)\n",
    "            print(f\"Loaded data shape: {df.shape}\")\n",
    "            dfs.append(df)\n",
    "    data = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"Concatenated data shape: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# Function to perform PCA\n",
    "def perform_pca(data):\n",
    "    print(\"Performing PCA...\")\n",
    "    scaler = StandardScaler()\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    data_imputed = imputer.fit_transform(data)\n",
    "    scaled_data = scaler.fit_transform(data_imputed)\n",
    "    pca = PCA(n_components=10)  # You can change the number of components as per your requirement\n",
    "    pca_data = pca.fit_transform(scaled_data)\n",
    "    print(f\"Data shape after PCA: {pca_data.shape}\")\n",
    "    print(\"PCA completed.\")\n",
    "    return pca_data\n",
    "\n",
    "# Function to build CNN model\n",
    "def build_cnn_model(input_shape):\n",
    "    print(\"Building CNN model...\")\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dense(50, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "    print(\"CNN model built.\")\n",
    "    return model\n",
    "\n",
    "# Function to build RNN model\n",
    "def build_rnn_model(input_shape):\n",
    "    print(\"Building RNN model...\")\n",
    "    model = Sequential([\n",
    "        LSTM(50, activation='relu', input_shape=input_shape),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "    print(\"RNN model built.\")\n",
    "    return model\n",
    "\n",
    "# Function to build Random Forest Regressor model\n",
    "def build_rf_model():\n",
    "    print(\"Building Random Forest Regressor model...\")\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    print(\"Random Forest Regressor model built.\")\n",
    "    return model\n",
    "\n",
    "# Function to classify rainfall intensity based on threshold\n",
    "def classify_rainfall_intensity(predictions, no_rainfall_thresh, moderate_rainfall_thresh):\n",
    "    intensities = []\n",
    "    for prediction in predictions:\n",
    "        if prediction < no_rainfall_thresh:\n",
    "            intensities.append(\"No Rainfall\")\n",
    "        elif prediction < moderate_rainfall_thresh:\n",
    "            intensities.append(\"Moderate Rainfall\")\n",
    "        else:\n",
    "            intensities.append(\"Heavy Rainfall\")\n",
    "    return intensities\n",
    "cnn_model = None\n",
    "rnn_model = None\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    directory = \"/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050\"\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    data = load_and_preprocess_data(directory)\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "    \n",
    "    # Remove NaN values from y\n",
    "    print(\"Removing NaN values from target variable (y)...\")\n",
    "    not_nan_indices = ~np.isnan(y)\n",
    "    X = X[not_nan_indices]\n",
    "    y = y[not_nan_indices]\n",
    "    \n",
    "    # Print data shape after removing NaN values\n",
    "    print(f\"Data shape after removing NaN values: {X.shape}, {y.shape}\")\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca_data = perform_pca(X)\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    print(\"Splitting data into training and testing sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(pca_data, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Build and train CNN model\n",
    "    print(\"Building and training CNN model...\")\n",
    "    input_shape = (X_train.shape[1], 1)\n",
    "    cnn_model = build_cnn_model(input_shape)\n",
    "    cnn_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "    \n",
    "    # Build and train RNN model\n",
    "    print(\"Building and training RNN model...\")\n",
    "    rnn_model = build_rnn_model(input_shape)\n",
    "    rnn_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "    \n",
    "    # Build and train Random Forest Regressor model\n",
    "    print(\"Building and training Random Forest Regressor model...\")\n",
    "    rf_model = build_rf_model()\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate models\n",
    "    print(\"Evaluating models...\")\n",
    "    cnn_pred = cnn_model.predict(X_test)\n",
    "    rnn_pred = rnn_model.predict(X_test)\n",
    "    rf_pred = rf_model.predict(X_test)\n",
    "    \n",
    "    # Print predictions\n",
    "    print(\"CNN Predictions:\", cnn_pred)\n",
    "    print(\"RNN Predictions:\", rnn_pred)\n",
    "    print(\"Random Forest Predictions:\", rf_pred)\n",
    "    \n",
    "    # Calculate RMSE and MAE\n",
    "    cnn_rmse = np.sqrt(mean_squared_error(y_test, cnn_pred))\n",
    "    rnn_rmse = np.sqrt(mean_squared_error(y_test, rnn_pred))\n",
    "    rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "    \n",
    "    cnn_mae = mean_absolute_error(y_test, cnn_pred)\n",
    "    rnn_mae = mean_absolute_error(y_test, rnn_pred)\n",
    "    rf_mae = mean_absolute_error(y_test, rf_pred)\n",
    "       \n",
    "    # Calculate MSE\n",
    "    cnn_mse = mean_squared_error(y_test, cnn_pred)\n",
    "    rnn_mse = mean_squared_error(y_test, rnn_pred)\n",
    "    rf_mse = mean_squared_error(y_test, rf_pred)\n",
    "\n",
    "    # Print evaluation results\n",
    "    print(\"Evaluation results:\")\n",
    "    print(\"CNN RMSE:\", cnn_rmse)\n",
    "    print(\"CNN MAE:\", cnn_mae)\n",
    "    print(\"CNN MSE:\", cnn_mse)\n",
    "    \n",
    "    print(\"RNN RMSE:\", rnn_rmse)\n",
    "    print(\"RNN MAE:\", rnn_mae)\n",
    "    print(\"RNN MSE:\", rnn_mse)\n",
    "    \n",
    "    print(\"Random Forest RMSE:\", rf_rmse)\n",
    "    print(\"Random Forest MAE:\", rf_mae)\n",
    "    print(\"Random Forest MSE:\", rf_mse)\n",
    "    \n",
    "    # Classify rainfall intensity for each model's predictions\n",
    "    #no_rainfall_threshold = 1  # Example threshold for no rainfall\n",
    "    #moderate_rainfall_threshold = 30  # Example threshold for moderate rainfall\n",
    "    \n",
    "    #cnn_intensity = classify_rainfall_intensity(cnn_pred.flatten(), no_rainfall_threshold, moderate_rainfall_threshold)\n",
    "    #rnn_intensity = classify_rainfall_intensity(rnn_pred.flatten(), no_rainfall_threshold, moderate_rainfall_threshold)\n",
    "    #rf_intensity = classify_rainfall_intensity(rf_pred, no_rainfall_threshold, moderate_rainfall_threshold)\n",
    "    \n",
    "    # Print the classified intensity\n",
    "    #print(\"\\nIntensity Classification:\")\n",
    "    #print(\"CNN Intensity:\", cnn_intensity)\n",
    "    #print(\"RNN Intensity:\", rnn_intensity)\n",
    "    #print(\"Random Forest Intensity:\", rf_intensity)\n",
    "    \n",
    "    model_filenames = ['cnn_model.h5', 'rnn_model.h5', 'rf_model.pkl']\n",
    "    models = [cnn_model, rnn_model, rf_model]\n",
    "    for model, filename in zip(models, model_filenames):\n",
    "        model.save(filename)\n",
    "        absolute_path = os.path.abspath(filename)\n",
    "        print(f\"Model saved at: {absolute_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7583ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
