{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf38ed4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode Values:\n",
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0]\n",
      "Index: []\n",
      "No mode values found.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, LSTM\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Initialize variables to store information about the data trained before\n",
    "trained_data_info = []\n",
    "\n",
    "# Function to print data trained before\n",
    "def print_trained_data_info():\n",
    "    print(\"Data trained before:\")\n",
    "    for folder, files in trained_data_info:\n",
    "        print(f\"Folder: {folder}, Files: {files}\")\n",
    "\n",
    "# Function to calculate and print evaluation metrics\n",
    "def print_evaluation_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r_squared = r2_score(y_true, y_pred)\n",
    "    print(f\"MSE: {mse:.4f}, MAE: {mae:.4f}, RMSE: {rmse:.4f}, R-squared: {r_squared:.4f}\")\n",
    "\n",
    "# Load initial dataset\n",
    "def load_initial_data(data_dir, encoding='iso-8859-1'):\n",
    "    data = pd.concat([pd.read_csv(os.path.join(data_dir, file), encoding=encoding) for file in os.listdir(data_dir)], ignore_index=True)\n",
    "    return data\n",
    "\n",
    "# Handle missing values by imputing or removing rows with missing values\n",
    "def handle_missing_values(data):\n",
    "    # Impute missing values for numerical columns with mean\n",
    "    numerical_cols = data.select_dtypes(include='number').columns\n",
    "    data[numerical_cols] = data[numerical_cols].fillna(data[numerical_cols].mean())\n",
    "    \n",
    "    # Impute missing values for categorical columns with mode\n",
    "    categorical_cols = data.select_dtypes(exclude='number').columns\n",
    "    mode_values = data[categorical_cols].mode()\n",
    "    print(\"Mode Values:\")\n",
    "    print(mode_values)\n",
    "    if not mode_values.empty:\n",
    "        data[categorical_cols] = data[categorical_cols].fillna(mode_values.iloc[0])\n",
    "    else:\n",
    "        print(\"No mode values found.\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Update models with new data and validate\n",
    "def update_models_with_new_data_and_validate(X_train, X_test, y_train, y_test, models, model_names, initial_data_dir, new_data_dir=None):\n",
    "    if new_data_dir:\n",
    "        data_dir = new_data_dir\n",
    "    else:\n",
    "        data_dir = initial_data_dir\n",
    "    \n",
    "    for year_folder in os.listdir(data_dir):\n",
    "        if os.path.isdir(os.path.join(data_dir, year_folder)):\n",
    "            print(f\"\\nTraining models for year {year_folder}:\")\n",
    "            trained_data_info.append((year_folder, []))\n",
    "            year_folder_path = os.path.join(data_dir, year_folder)\n",
    "            for idx, file_name in enumerate(os.listdir(year_folder_path)):\n",
    "                if idx % 10 == 0:\n",
    "                    print_trained_data_info()\n",
    "                if idx >= 10:\n",
    "                    break\n",
    "                file_path = os.path.join(year_folder_path, file_name)\n",
    "                new_data = pd.read_csv(file_path)\n",
    "                new_data = handle_missing_values(new_data)\n",
    "                X_new = new_data[X_train.columns]\n",
    "                y_new = new_data['precipitationCal']\n",
    "\n",
    "                for model, model_name in zip(models, model_names):\n",
    "                    # Update model\n",
    "                    model.fit(X_train, y_train)\n",
    "                    print(f\"\\n{model_name}:\")\n",
    "                    # Evaluate on validation data\n",
    "                    print_evaluation_metrics(y_test, model.predict(X_test))\n",
    "\n",
    "                # Append trained file info to trained_data_info\n",
    "                trained_data_info[-1][1].append(file_name)\n",
    "\n",
    "# Main function\n",
    "def main(new_data_dir=None, initial_data_dir=None, batch_size=10):\n",
    "    if initial_data_dir is None:\n",
    "        initial_data_dir = '/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated_CSV'\n",
    "\n",
    "    # Load initial dataset\n",
    "    data = load_initial_data(initial_data_dir)\n",
    "\n",
    "    # Handle missing values in initial dataset\n",
    "    data = handle_missing_values(data)\n",
    "    data_with_precipitation = data[data['precipitationCal'].notna()]\n",
    "\n",
    "    # Split features and target variable\n",
    "    X_columns = [col for col in data.columns if 'IMG_TIR1' in col or 'IMG_TIR2' in col or 'IMG_WV' in col]\n",
    "    X = data[X_columns]\n",
    "    y = data['precipitationCal']\n",
    "\n",
    "    # Split initial data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, train_size=0.7, random_state=42)\n",
    "\n",
    "    # Initialize models\n",
    "    rf_model = RandomForestRegressor()\n",
    "    xgb_model = XGBRegressor()\n",
    "    lgbm_model = LGBMRegressor()\n",
    "    catboost_model = CatBoostRegressor(verbose=0)\n",
    "    lasso_model = Lasso(alpha=0.1)\n",
    "    cnn_model = Sequential([\n",
    "        Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    rnn_model = Sequential([\n",
    "        LSTM(32, input_shape=(X_train.shape[1], 1)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    models = [rf_model, xgb_model, lgbm_model, catboost_model, lasso_model, cnn_model, rnn_model]\n",
    "    model_names = ['Random Forest', 'XGBoost', 'LightGBM', 'CatBoost', 'Lasso', 'CNN', 'RNN']\n",
    "\n",
    "    # Update models with new data and validate\n",
    "    update_models_with_new_data_and_validate(X_train, X_test, y_train, y_test, models, model_names, initial_data_dir, new_data_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    new_data_dir = ''  # Provide the new data directory path here if needed\n",
    "    initial_data_dir = '/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated_CSV'  # Define the initial data directory\n",
    "    main(new_data_dir, initial_data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "695a4756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, LSTM\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Initialize variables to store information about the data trained before\n",
    "trained_data_info = []\n",
    "\n",
    "# Function to print data trained before\n",
    "def print_trained_data_info():\n",
    "    print(\"Data trained before:\")\n",
    "    for folder, files in trained_data_info:\n",
    "        print(f\"Folder: {folder}, Files: {files}\")\n",
    "\n",
    "# Function to calculate and print evaluation metrics\n",
    "def print_evaluation_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r_squared = r2_score(y_true, y_pred)\n",
    "    print(f\"MSE: {mse:.4f}, MAE: {mae:.4f}, RMSE: {rmse:.4f}, R-squared: {r_squared:.4f}\")\n",
    "\n",
    "# Load initial dataset\n",
    "def load_initial_data(data_dir, encoding='iso-8859-1'):\n",
    "    data = pd.concat([pd.read_csv(os.path.join(data_dir, file), encoding=encoding) for file in os.listdir(data_dir)], ignore_index=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Handle missing values by imputing or removing rows with missing values\n",
    "def handle_missing_values(data):\n",
    "    # Impute missing values for numerical columns with mean\n",
    "    numerical_cols = data.select_dtypes(include='number').columns\n",
    "    imputer_num = SimpleImputer(strategy='mean')\n",
    "    data[numerical_cols] = imputer_num.fit_transform(data[numerical_cols])\n",
    "\n",
    "    # Scale the numerical columns\n",
    "    scaler = StandardScaler()\n",
    "    data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
    "\n",
    "    # Impute missing values for categorical columns with mode\n",
    "    categorical_cols = data.select_dtypes(exclude='number').columns\n",
    "    for col in categorical_cols:\n",
    "        if data[col].isnull().any():\n",
    "            imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "            data[col] = imputer_cat.fit_transform(data[[col]])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Update models with new data and validate\n",
    "def update_models_with_new_data_and_validate(X_train, X_test, y_train, y_test, models, model_names,initial_data_dir, new_data_dir=None):\n",
    "    if new_data_dir:\n",
    "        data_dir = new_data_dir\n",
    "    else:\n",
    "        data_dir = initial_data_dir\n",
    "    \n",
    "    for year_folder in os.listdir(data_dir):\n",
    "        if os.path.isdir(os.path.join(data_dir, year_folder)):\n",
    "            print(f\"\\nTraining models for year {year_folder}:\")\n",
    "            trained_data_info.append((year_folder, []))\n",
    "            year_folder_path = os.path.join(data_dir, year_folder)\n",
    "            for idx, file_name in enumerate(os.listdir(year_folder_path)):\n",
    "                if idx % 10 == 0:\n",
    "                    print_trained_data_info()\n",
    "                if idx >= 10:\n",
    "                    break\n",
    "                file_path = os.path.join(year_folder_path, file_name)\n",
    "                new_data = pd.read_csv(file_path)\n",
    "                new_data = handle_missing_values(new_data)\n",
    "                X_new = new_data[X_train.columns]\n",
    "                y_new = new_data['precipitationCal']\n",
    "\n",
    "                for model, model_name in zip(models, model_names):\n",
    "                    # Update model\n",
    "                    model.fit(X_train, y_train)\n",
    "                    print(f\"\\n{model_name}:\")\n",
    "                    # Evaluate on validation data\n",
    "                    print_evaluation_metrics(y_test, model.predict(X_test))\n",
    "\n",
    "                # Append trained file info to trained_data_info\n",
    "                trained_data_info[-1][1].append(file_name)\n",
    "\n",
    "# Main function\n",
    "def main(new_data_dir=None, batch_size=10):\n",
    "    # Define directory paths\n",
    "    initial_data_dir = '/Users/kunalpathak9826/Desktop/ISRO/Data/2018'\n",
    "\n",
    "    # Load initial dataset\n",
    "    data = load_initial_data(initial_data_dir)\n",
    "\n",
    "    # Handle missing values in initial dataset\n",
    "    data = handle_missing_values(data)\n",
    "    data_with_precipitation = data[data['precipitationCal'].notna()]\n",
    "\n",
    "    # Split features and target variable\n",
    "    X_columns = [col for col in data.columns if 'IMG_TIR1' in col or 'IMG_TIR2' in col or 'IMG_WV' in col]\n",
    "    X = data[X_columns]\n",
    "    y = data['precipitationCal']\n",
    "\n",
    "    # Split initial data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, train_size=0.7, random_state=42)\n",
    "\n",
    "    # Initialize models\n",
    "    rf_model = RandomForestRegressor()\n",
    "    xgb_model = XGBRegressor()\n",
    "    lgbm_model = LGBMRegressor()\n",
    "    catboost_model = CatBoostRegressor(verbose=0)\n",
    "    lasso_model = Lasso(alpha=0.1)\n",
    "    cnn_model = Sequential([\n",
    "        Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    rnn_model = Sequential([\n",
    "        LSTM(32, input_shape=(X_train.shape[1], 1)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    models = [rf_model, xgb_model, lgbm_model, catboost_model, lasso_model, cnn_model, rnn_model]\n",
    "    model_names = ['Random Forest', 'XGBoost', 'LightGBM', 'CatBoost', 'Lasso', 'CNN', 'RNN']\n",
    "\n",
    "    # Update models with new data and validate\n",
    "    update_models_with_new_data_and_validate(X_train, X_test, y_train, y_test, models, model_names, new_data_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    new_data_dir = '/Users/kunalpathak9826/Desktop/ISRO/Data/New_Interpolated_CSV'  # Provide the new data directory path here if needed\n",
    "    main(new_data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64044d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, LSTM\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Initialize variables to store information about the data trained before\n",
    "trained_data_info = []\n",
    "\n",
    "# Function to set the initial data directory\n",
    "def set_initial_data_dir(initial_data_dir):\n",
    "    global initial_data_dir_global\n",
    "    initial_data_dir_global = initial_data_dir\n",
    "\n",
    "# Function to set the new data directory\n",
    "def set_new_data_dir(new_data_dir):\n",
    "    global new_data_dir_global\n",
    "    new_data_dir_global = new_data_dir\n",
    "\n",
    "# Function to print data trained before\n",
    "def print_trained_data_info():\n",
    "    print(\"\\nData trained before:\")\n",
    "    for folder, files in trained_data_info:\n",
    "        print(f\"Folder: {folder}, Files: {files}\")\n",
    "\n",
    "# Function to calculate and print evaluation metrics\n",
    "def print_evaluation_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r_squared = r2_score(y_true, y_pred)\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"MSE: {mse:.4f}, MAE: {mae:.4f}, RMSE: {rmse:.4f}, R-squared: {r_squared:.4f}\")\n",
    "\n",
    "# Load initial dataset\n",
    "def load_initial_data(encoding='iso-8859-1'):\n",
    "    data = pd.concat([pd.read_csv(os.path.join(initial_data_dir_global, file), encoding=encoding) for file in os.listdir(initial_data_dir_global)], ignore_index=True)\n",
    "    return data\n",
    "\n",
    "# Load new dataset\n",
    "def load_new_data(data_dir, encoding='iso-8859-1'):\n",
    "    data = pd.concat([pd.read_csv(os.path.join(data_dir, file), encoding=encoding) for file in os.listdir(data_dir)], ignore_index=True)\n",
    "    return data\n",
    "\n",
    "# Handle missing values by imputing or removing rows with missing values\n",
    "def handle_missing_values(data):\n",
    "    # Impute missing values for numerical columns with mean\n",
    "    numerical_cols = data.select_dtypes(include='number').columns\n",
    "    imputer_num = SimpleImputer(strategy='mean')\n",
    "    data[numerical_cols] = imputer_num.fit_transform(data[numerical_cols])\n",
    "\n",
    "    # Scale the numerical columns\n",
    "    scaler = StandardScaler()\n",
    "    data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
    "\n",
    "    # Impute missing values for categorical columns with mode\n",
    "    categorical_cols = data.select_dtypes(exclude='number').columns\n",
    "    for col in categorical_cols:\n",
    "        if data[col].isnull().any():\n",
    "            imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "            data[col] = imputer_cat.fit_transform(data[[col]])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Update models with new data and validate\n",
    "def update_models_with_new_data_and_validate(X_train, X_test, y_train, y_test, models, model_names, new_data_dir=None):\n",
    "    if new_data_dir:\n",
    "        data_dir = new_data_dir\n",
    "    else:\n",
    "        data_dir = initial_data_dir_global\n",
    "    \n",
    "    for year_folder in os.listdir(data_dir):\n",
    "        if os.path.isdir(os.path.join(data_dir, year_folder)):\n",
    "            print(f\"\\nTraining models for year {year_folder}:\")\n",
    "            trained_data_info.append((year_folder, []))\n",
    "            year_folder_path = os.path.join(data_dir, year_folder)\n",
    "            for idx, file_name in enumerate(os.listdir(year_folder_path)):\n",
    "                if idx % 10 == 0:\n",
    "                    print_trained_data_info()\n",
    "                if idx >= 10:\n",
    "                    break\n",
    "                file_path = os.path.join(year_folder_path, file_name)\n",
    "                new_data = pd.read_csv(file_path)\n",
    "                new_data = handle_missing_values(new_data)\n",
    "                X_new = new_data[X_train.columns]\n",
    "                y_new = new_data['precipitationCal']\n",
    "\n",
    "                for model, model_name in zip(models, model_names):\n",
    "                    # Update model\n",
    "                    model.fit(X_train, y_train)\n",
    "                    print(f\"\\nTraining {model_name} on file: {file_name}\")\n",
    "                    # Evaluate on validation data\n",
    "                    y_pred = model.predict(X_test)\n",
    "                    print_evaluation_metrics(model_name, y_test, y_pred)\n",
    "\n",
    "                # Append trained file info to trained_data_info\n",
    "                trained_data_info[-1][1].append(file_name)\n",
    "\n",
    "# Main function\n",
    "def main(initial_data_dir=None, new_data_dir=None, batch_size=10):\n",
    "    if initial_data_dir:\n",
    "        set_initial_data_dir(initial_data_dir)\n",
    "    if new_data_dir:\n",
    "        set_new_data_dir(new_data_dir)\n",
    "\n",
    "    # Load initial dataset\n",
    "    data = load_initial_data()\n",
    "\n",
    "    # Handle missing values in initial dataset\n",
    "    data = handle_missing_values(data)\n",
    "    data_with_precipitation = data[data['precipitationCal'].notna()]\n",
    "\n",
    "    # Split features and target variable\n",
    "    X_columns = [col for col in data.columns if 'IMG_TIR1' in col or 'IMG_TIR2' in col or 'IMG_WV' in col]\n",
    "    X = data[X_columns]\n",
    "    y = data['precipitationCal']\n",
    "\n",
    "    # Split initial data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, train_size=0.7, random_state=42)\n",
    "\n",
    "    # Initialize models\n",
    "    rf_model = RandomForestRegressor()\n",
    "    xgb_model = XGBRegressor()\n",
    "    lgbm_model = LGBMRegressor()\n",
    "    catboost_model = CatBoostRegressor(verbose=0)\n",
    "    lasso_model = Lasso(alpha=0.1)\n",
    "    cnn_model = Sequential([\n",
    "        Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    rnn_model = Sequential([\n",
    "        LSTM(32, input_shape=(X_train.shape[1], 1)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    models = [rf_model, xgb_model, lgbm_model, catboost_model, lasso_model, cnn_model, rnn_model]\n",
    "    model_names = ['Random Forest', 'XGBoost', 'LightGBM', 'CatBoost', 'Lasso', 'CNN', 'RNN']\n",
    "\n",
    "    # Update models with new data and validate\n",
    "    update_models_with_new_data_and_validate(X_train, X_test, y_train, y_test, models, model_names, new_data_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(initial_data_dir='/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2017', new_data_dir='/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2018')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc1eacc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting main function...\n",
      "Loading initial data...\n",
      "Handling missing values...\n",
      "Updating models with new data and validating...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, LSTM\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Initialize variables to store information about the data trained before\n",
    "trained_data_info = []\n",
    "\n",
    "# Function to set the initial data directory\n",
    "def set_initial_data_dir(initial_data_dir):\n",
    "    global initial_data_dir_global\n",
    "    initial_data_dir_global = initial_data_dir\n",
    "\n",
    "# Function to set the new data directory\n",
    "def set_new_data_dir(new_data_dir):\n",
    "    global new_data_dir_global\n",
    "    new_data_dir_global = new_data_dir\n",
    "\n",
    "# Function to print data trained before\n",
    "def print_trained_data_info():\n",
    "    print(\"\\nData trained before:\")\n",
    "    for folder, files in trained_data_info:\n",
    "        print(f\"Folder: {folder}, Files: {files}\")\n",
    "\n",
    "# Function to calculate and print evaluation metrics\n",
    "def print_evaluation_metrics(model_name, y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r_squared = r2_score(y_true, y_pred)\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"MSE: {mse:.4f}, MAE: {mae:.4f}, RMSE: {rmse:.4f}, R-squared: {r_squared:.4f}\")\n",
    "\n",
    "# Load initial dataset\n",
    "def load_initial_data(encoding='iso-8859-1'):\n",
    "    print(\"Loading initial data...\")\n",
    "    data = pd.concat([pd.read_csv(os.path.join(initial_data_dir_global, file), encoding=encoding) for file in os.listdir(initial_data_dir_global)], ignore_index=True)\n",
    "    return data\n",
    "\n",
    "# Load new dataset\n",
    "def load_new_data(data_dir, encoding='iso-8859-1'):\n",
    "    print(\"Loading new data...\")\n",
    "    data = pd.concat([pd.read_csv(os.path.join(data_dir, file), encoding=encoding) for file in os.listdir(data_dir)], ignore_index=True)\n",
    "    return data\n",
    "\n",
    "# Handle missing values by imputing or removing rows with missing values\n",
    "def handle_missing_values(data):\n",
    "    print(\"Handling missing values...\")\n",
    "    # Impute missing values for numerical columns with mean\n",
    "    numerical_cols = data.select_dtypes(include='number').columns\n",
    "    imputer_num = SimpleImputer(strategy='mean')\n",
    "    data[numerical_cols] = imputer_num.fit_transform(data[numerical_cols])\n",
    "\n",
    "    # Scale the numerical columns\n",
    "    scaler = StandardScaler()\n",
    "    data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
    "\n",
    "    # Impute missing values for categorical columns with mode\n",
    "    categorical_cols = data.select_dtypes(exclude='number').columns\n",
    "    for col in categorical_cols:\n",
    "        if data[col].isnull().any():\n",
    "            imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "            data[col] = imputer_cat.fit_transform(data[[col]])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Update models with new data and validate\n",
    "def update_models_with_new_data_and_validate(X_train, X_test, y_train, y_test, models, model_names, new_data_dir=None):\n",
    "    if new_data_dir:\n",
    "        data_dir = new_data_dir\n",
    "    else:\n",
    "        data_dir = initial_data_dir_global\n",
    "    \n",
    "    print(\"Updating models with new data and validating...\")\n",
    "    for year_folder in os.listdir(data_dir):\n",
    "        if os.path.isdir(os.path.join(data_dir, year_folder)):\n",
    "            print(f\"\\nTraining models for year {year_folder}:\")\n",
    "            trained_data_info.append((year_folder, []))\n",
    "            year_folder_path = os.path.join(data_dir, year_folder)\n",
    "            for idx, file_name in enumerate(os.listdir(year_folder_path)):\n",
    "                if idx % 10 == 0:\n",
    "                    print_trained_data_info()\n",
    "                if idx >= 10:\n",
    "                    break\n",
    "                print(f\"\\nTraining on file: {file_name}\")\n",
    "                file_path = os.path.join(year_folder_path, file_name)\n",
    "                new_data = pd.read_csv(file_path)\n",
    "                new_data = handle_missing_values(new_data)\n",
    "                X_new = new_data[X_train.columns]\n",
    "                y_new = new_data['precipitationCal']\n",
    "\n",
    "                for model, model_name in zip(models, model_names):\n",
    "                    # Update model\n",
    "                    model.fit(X_new, y_new)\n",
    "                    print(f\"\\nTraining {model_name} on file: {file_name}\")\n",
    "                    # Evaluate on validation data\n",
    "                    y_pred = model.predict(X_new)\n",
    "                    print_evaluation_metrics(model_name, y_new, y_pred)\n",
    "\n",
    "                # Append trained file info to trained_data_info\n",
    "                trained_data_info[-1][1].append(file_name)\n",
    "\n",
    "# Main function\n",
    "def main(initial_data_dir=None, new_data_dir=None, batch_size=10):\n",
    "    if initial_data_dir:\n",
    "        set_initial_data_dir(initial_data_dir)\n",
    "    if new_data_dir:\n",
    "        set_new_data_dir(new_data_dir)\n",
    "\n",
    "    # Load initial dataset\n",
    "    print(\"Starting main function...\")\n",
    "    data = load_initial_data()\n",
    "\n",
    "    # Handle missing values in initial dataset\n",
    "    data = handle_missing_values(data)\n",
    "    data_with_precipitation = data[data['precipitationCal'].notna()]\n",
    "\n",
    "    # Split features and target variable\n",
    "    X_columns = [col for col in data.columns if 'IMG_TIR1' in col or 'IMG_TIR2' in col or 'IMG_WV' in col]\n",
    "    X = data[X_columns]\n",
    "    y = data['precipitationCal']\n",
    "\n",
    "    # Split initial data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, train_size=0.7, random_state=42)\n",
    "\n",
    "    # Initialize models\n",
    "    rf_model = RandomForestRegressor()\n",
    "    xgb_model = XGBRegressor()\n",
    "    lgbm_model = LGBMRegressor()\n",
    "    catboost_model = CatBoostRegressor(verbose=0)\n",
    "    lasso_model = Lasso(alpha=0.1)\n",
    "    cnn_model = Sequential([\n",
    "        Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    rnn_model = Sequential([\n",
    "        LSTM(32, input_shape=(X_train.shape[1], 1)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    models = [rf_model, xgb_model, lgbm_model, catboost_model, lasso_model, cnn_model, rnn_model]\n",
    "    model_names = ['Random Forest', 'XGBoost', 'LightGBM', 'CatBoost', 'Lasso', 'CNN', 'RNN']\n",
    "\n",
    "     # Update models with new data and validate\n",
    "    update_models_with_new_data_and_validate(X_train, X_test, y_train, y_test, models, model_names, new_data_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(initial_data_dir='/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2017', new_data_dir='/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2018')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afe6dd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting main function...\n",
      "Loading initial data...\n",
      "Handling missing values...\n",
      "X_train size: 248850000\n",
      "X_test size: 106650000\n",
      "y_train size: 175000\n",
      "y_test size: 75000\n",
      "Updating models with new data and validating...\n",
      "Data directory: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2017\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, LSTM\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Initialize variables to store information about the data trained before\n",
    "trained_data_info = []\n",
    "initial_data_dir_global = None \n",
    "new_data_dir_global = None  \n",
    "\n",
    "# Function to set the initial data directory\n",
    "def set_initial_data_dir(initial_data_dir):\n",
    "    global initial_data_dir_global\n",
    "    initial_data_dir_global = initial_data_dir\n",
    "\n",
    "# Function to set the new data directory\n",
    "def set_new_data_dir(new_data_dir):\n",
    "    global new_data_dir_global\n",
    "    new_data_dir_global = new_data_dir\n",
    "\n",
    "# Function to print data trained before\n",
    "def print_trained_data_info():\n",
    "    print(\"\\nData trained before:\")\n",
    "    for folder, files in trained_data_info:\n",
    "        print(f\"Folder: {folder}, Files: {files}\")\n",
    "\n",
    "# Function to calculate and print evaluation metrics\n",
    "def print_evaluation_metrics(model_name, y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r_squared = r2_score(y_true, y_pred)\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"MSE: {mse:.4f}, MAE: {mae:.4f}, RMSE: {rmse:.4f}, R-squared: {r_squared:.4f}\")\n",
    "\n",
    "# Load initial dataset\n",
    "def load_initial_data(encoding='iso-8859-1'):\n",
    "    print(\"Loading initial data...\")\n",
    "    data = pd.concat([pd.read_csv(os.path.join(initial_data_dir_global, file), encoding=encoding) for file in os.listdir(initial_data_dir_global)], ignore_index=True)\n",
    "    return data\n",
    "\n",
    "# Load new dataset\n",
    "def load_new_data(data_dir, encoding='iso-8859-1'):\n",
    "    print(\"Loading new data...\")\n",
    "    data = pd.concat([pd.read_csv(os.path.join(data_dir, file), encoding=encoding) for file in os.listdir(data_dir)], ignore_index=True)\n",
    "    return data\n",
    "\n",
    "# Handle missing values by imputing or removing rows with missing values\n",
    "def handle_missing_values(data):\n",
    "    print(\"Handling missing values...\")\n",
    "    # Impute missing values for numerical columns with mean\n",
    "    numerical_cols = data.select_dtypes(include='number').columns\n",
    "    imputer_num = SimpleImputer(strategy='mean')\n",
    "    data[numerical_cols] = imputer_num.fit_transform(data[numerical_cols])\n",
    "\n",
    "    # Scale the numerical columns\n",
    "    scaler = StandardScaler()\n",
    "    data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
    "\n",
    "    # Impute missing values for categorical columns with mode\n",
    "    categorical_cols = data.select_dtypes(exclude='number').columns\n",
    "    for col in categorical_cols:\n",
    "        if data[col].isnull().any():\n",
    "            imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "            data[col] = imputer_cat.fit_transform(data[[col]])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def update_models_with_new_data_and_validate(X_train, X_test, y_train, y_test, models, model_names, new_data_dir=None):\n",
    "    if new_data_dir:\n",
    "        data_dir = new_data_dir\n",
    "    else:\n",
    "        data_dir = initial_data_dir_global\n",
    "    \n",
    "    print(\"Updating models with new data and validating...\")\n",
    "    print(\"Data directory:\", data_dir)  # Print data directory\n",
    "    \n",
    "    # Combine initial and new data\n",
    "    combined_X = pd.concat([X_train, X_test], axis=0)\n",
    "    combined_y = pd.concat([y_train, y_test], axis=0)\n",
    "    \n",
    "    for year_folder in os.listdir(data_dir):\n",
    "        if os.path.isdir(os.path.join(data_dir, year_folder)):\n",
    "            print(f\"\\nTraining models for year {year_folder}:\")\n",
    "            trained_data_info.append((year_folder, []))\n",
    "            year_folder_path = os.path.join(data_dir, year_folder)\n",
    "            for idx, file_name in enumerate(os.listdir(year_folder_path)):\n",
    "                print(f\"\\nTraining on file: {file_name}\")\n",
    "                file_path = os.path.join(year_folder_path, file_name)\n",
    "                new_data = pd.read_csv(file_path)\n",
    "                new_data = handle_missing_values(new_data)\n",
    "                X_new = new_data[X_train.columns]\n",
    "                y_new = new_data['precipitationCal']\n",
    "\n",
    "                print(f\"New data shape: {X_new.shape}, {y_new.shape}\")  # Print new data shape\n",
    "                \n",
    "                # Combine new data with existing data\n",
    "                combined_X = pd.concat([combined_X, X_new], axis=0)\n",
    "                combined_y = pd.concat([combined_y, y_new], axis=0)\n",
    "                \n",
    "                for model, model_name in zip(models, model_names):\n",
    "                    # Update model\n",
    "                    model.fit(X_train, y_train)  # Fit on combined data\n",
    "                    print(f\"\\nTraining {model_name} on file: {file_name}\")\n",
    "                    # Evaluate on new data\n",
    "                    y_pred = model.predict(X_test)\n",
    "                    print_evaluation_metrics(model_name, y_test, y_pred)\n",
    "\n",
    "                # Append trained file info to trained_data_info\n",
    "                trained_data_info[-1][1].append(file_name)\n",
    "\n",
    "\n",
    "\n",
    "# Main function\n",
    "def main(initial_data_dir=None, new_data_dir=None, batch_size=10):\n",
    "    if initial_data_dir:\n",
    "        set_initial_data_dir(initial_data_dir)\n",
    "    if new_data_dir:\n",
    "        set_new_data_dir(new_data_dir)\n",
    "\n",
    "    # Load initial dataset\n",
    "    print(\"Starting main function...\")\n",
    "    data = load_initial_data()\n",
    "\n",
    "    # Handle missing values in initial dataset\n",
    "    data = handle_missing_values(data)\n",
    "    data_with_precipitation = data[data['precipitationCal'].notna()]\n",
    "\n",
    "    # Split features and target variable\n",
    "    X_columns = [col for col in data.columns if 'IMG_TIR1' in col or 'IMG_TIR2' in col or 'IMG_WV' in col]\n",
    "    X = data[X_columns]\n",
    "    y = data['precipitationCal']\n",
    "\n",
    "    # Split initial data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, train_size=0.7, random_state=42)\n",
    "    print(\"X_train size:\", X_train.size)\n",
    "    print(\"X_test size:\", X_test.size)\n",
    "    print(\"y_train size:\", y_train.size)\n",
    "    print(\"y_test size:\", y_test.size)\n",
    "\n",
    "\n",
    "    # Initialize models\n",
    "    rf_model = RandomForestRegressor()\n",
    "    xgb_model = XGBRegressor()\n",
    "    lgbm_model = LGBMRegressor()\n",
    "    catboost_model = CatBoostRegressor(verbose=0)\n",
    "    lasso_model = Lasso(alpha=0.1)\n",
    "    cnn_model = Sequential([\n",
    "        Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    rnn_model = Sequential([\n",
    "        LSTM(32, input_shape=(X_train.shape[1], 1)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    models = [rf_model, xgb_model, lgbm_model, catboost_model, lasso_model, cnn_model, rnn_model]\n",
    "    model_names = ['Random Forest', 'XGBoost', 'LightGBM', 'CatBoost', 'Lasso', 'CNN', 'RNN']\n",
    "\n",
    "     # Update models with new data and validate\n",
    "    update_models_with_new_data_and_validate(X_train, X_test, y_train, y_test, models, model_names, new_data_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(initial_data_dir='/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2017')#, new_data_dir='/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2018')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ec5a5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [[8.38181607e-01]\n",
      " [7.40191107e-01]\n",
      " [8.49364521e-01]\n",
      " [3.85454260e-01]\n",
      " [6.87884540e-01]\n",
      " [8.47988012e-01]\n",
      " [6.99003804e-01]\n",
      " [3.86299011e-01]\n",
      " [7.78082562e-01]\n",
      " [9.04354156e-01]\n",
      " [6.44373166e-02]\n",
      " [9.35996291e-01]\n",
      " [6.60155290e-01]\n",
      " [4.02882508e-02]\n",
      " [5.56640809e-01]\n",
      " [9.17078665e-01]\n",
      " [1.35881883e-01]\n",
      " [4.17283779e-01]\n",
      " [2.95392848e-01]\n",
      " [1.37612806e-01]\n",
      " [2.03337222e-01]\n",
      " [4.55249434e-01]\n",
      " [9.33910632e-01]\n",
      " [9.05238748e-01]\n",
      " [8.50795709e-01]\n",
      " [4.13012769e-01]\n",
      " [8.06816930e-01]\n",
      " [6.78973614e-01]\n",
      " [2.69273694e-01]\n",
      " [4.77465219e-01]\n",
      " [5.55743229e-03]\n",
      " [3.82213505e-01]\n",
      " [5.42730111e-01]\n",
      " [4.59294803e-01]\n",
      " [1.72751282e-01]\n",
      " [3.95672077e-01]\n",
      " [7.38890718e-01]\n",
      " [4.09550056e-01]\n",
      " [8.72827098e-01]\n",
      " [2.47000752e-02]\n",
      " [1.12642301e-01]\n",
      " [9.53502331e-02]\n",
      " [2.39771563e-01]\n",
      " [1.33606646e-01]\n",
      " [7.51006622e-01]\n",
      " [3.59075844e-01]\n",
      " [5.14342489e-01]\n",
      " [9.59657162e-01]\n",
      " [7.63467728e-04]\n",
      " [3.03920458e-01]\n",
      " [1.85762779e-01]\n",
      " [5.25003325e-01]\n",
      " [5.90588745e-01]\n",
      " [4.61664815e-01]\n",
      " [4.19498181e-01]\n",
      " [2.97393194e-01]\n",
      " [8.93988712e-01]\n",
      " [1.61405903e-01]\n",
      " [5.91990051e-01]\n",
      " [1.95821269e-01]\n",
      " [8.37566020e-01]\n",
      " [2.59579708e-01]\n",
      " [9.14404337e-01]\n",
      " [5.05894981e-01]\n",
      " [1.46107845e-01]\n",
      " [5.75982371e-01]\n",
      " [7.72360356e-01]\n",
      " [5.96509151e-01]\n",
      " [7.66688976e-01]\n",
      " [5.57177439e-01]\n",
      " [9.67156979e-01]\n",
      " [6.61117357e-01]\n",
      " [7.88080020e-01]\n",
      " [9.45304537e-01]\n",
      " [3.03882433e-01]\n",
      " [8.10769732e-01]\n",
      " [8.63630476e-01]\n",
      " [3.51282460e-02]\n",
      " [9.93932190e-01]\n",
      " [4.63672062e-01]\n",
      " [5.01167660e-02]\n",
      " [3.45178828e-02]\n",
      " [6.93919212e-01]\n",
      " [2.76230398e-01]\n",
      " [5.02764994e-01]\n",
      " [2.90982187e-01]\n",
      " [1.96637911e-01]\n",
      " [8.27161388e-02]\n",
      " [2.90511011e-02]\n",
      " [2.44898520e-01]\n",
      " [7.67463586e-01]\n",
      " [7.03951450e-01]\n",
      " [2.66699358e-01]\n",
      " [2.39482825e-01]\n",
      " [4.80642304e-01]\n",
      " [9.25075458e-01]\n",
      " [6.53613608e-01]\n",
      " [6.06279597e-01]\n",
      " [4.04806365e-01]\n",
      " [5.48849033e-01]]\n",
      "y: [ 1.60724426  0.84135719  0.27471201  0.08472515  0.63333114  1.79639056\n",
      "  2.43110954  0.93548651  2.09689611  1.73753501  0.72318427  2.0187974\n",
      "  0.72720837  2.19105941  1.91175954  2.01754472 -0.58338659 -1.44662693\n",
      "  1.93982012  0.2858978   1.98120547 -1.0386318   1.88829237  3.70537829\n",
      "  2.80530829  0.46712304  1.74879783  2.0901472  -0.6935829   2.06035972\n",
      " -1.83584572 -0.60768068  2.09318634  1.78875521  0.51099258  0.19336756\n",
      "  2.27525946 -0.55016987  0.95017104 -2.06577884  0.02569859  0.27366037\n",
      "  1.81048453  1.62911872  1.6017656   0.59023874  1.2286524   1.94726485\n",
      " -0.69665     1.07192441  0.25273617  0.96999595  0.62554294  2.44403875\n",
      "  0.24662992  1.78168682  1.75996012 -0.27697337  2.23527903 -0.59946262\n",
      "  2.97853791  0.77040038  3.24360353  0.85115496  0.44292367  0.73583582\n",
      " -0.61994195  1.56922502 -0.35751743  0.96682725  1.92772472  0.08302368\n",
      "  0.75645191  1.0861675   0.87661843  1.65324549 -0.3035649   0.23720956\n",
      "  1.42970252  1.00955522  0.69042015  0.01760529  0.89625935  0.14757837\n",
      "  2.57371975 -0.58595887 -0.52824446  1.1772814  -0.42874412  0.12252096\n",
      "  3.82378659  0.50447363 -0.07108392 -0.98859331  1.17498781  1.23860897\n",
      "  0.02926862  1.14258217 -0.01118254  0.95361735]\n",
      "X_train size: 70\n",
      "X_test size: 30\n",
      "y_train size: 70\n",
      "y_test size: 30\n",
      "model: SGDRegressor(learning_rate='constant', random_state=42)\n",
      "Final MSE: 1.0775\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "import numpy as np\n",
    "\n",
    "X = np.random.rand(100, 1)\n",
    "y = 2 * X.squeeze() + np.random.randn(100)\n",
    "print(\"X:\", X)\n",
    "print(\"y:\", y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, train_size=0.7, random_state=42)\n",
    "print(\"X_train size:\", X_train.size)\n",
    "print(\"X_test size:\", X_test.size)\n",
    "print(\"y_train size:\", y_train.size)\n",
    "print(\"y_test size:\", y_test.size)\n",
    "\n",
    "model = SGDRegressor(learning_rate='constant', eta0=0.01, random_state=42)\n",
    "print(\"model:\", model)\n",
    "\n",
    "# Fitting the model on the entire training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Final MSE: {mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "730813a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for interpolated_insat_on_imerg_20170109.csv: 253984880455632230893499550091182080.0000\n",
      "MSE for interpolated_insat_on_imerg_20170108.csv: 327540727142447713759345235420250112.0000\n",
      "MSE for interpolated_insat_on_imerg_20170101.csv: 263161542804419823748713252160274432.0000\n",
      "MSE for interpolated_insat_on_imerg_20170103.csv: 190461397001886902001456359235649536.0000\n",
      "MSE for interpolated_insat_on_imerg_20170102.csv: 189969750731698592919319278086258688.0000\n",
      "MSE for interpolated_insat_on_imerg_20170106.csv: 127546023269829882025434495013879808.0000\n",
      "MSE for interpolated_insat_on_imerg_20170107.csv: 1013536208962113048387980810518528.0000\n",
      "MSE for interpolated_insat_on_imerg_20170105.csv: 82501416541071600488745311443877888.0000\n",
      "MSE for interpolated_insat_on_imerg_20170104.csv: 166396601923241961520893336936775680.0000\n",
      "MSE for interpolated_insat_on_imerg_20170110.csv: 246280042441083642643090214743965696.0000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# Directory containing your CSV files\n",
    "folder_path = '/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2017/'\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Loop through each CSV file\n",
    "for csv_file in csv_files:\n",
    "    # Load the data from CSV file\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))\n",
    "\n",
    "    # Assuming your data is formatted appropriately, extract features (X) and target (y)\n",
    "    X = df.drop(columns=['precipitationCal'])  # Adjust 'precipitationCal' to your target column name\n",
    "    y = df['precipitationCal']\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, train_size=0.7, random_state=42)\n",
    "\n",
    "    # Create and train your model\n",
    "    model = SGDRegressor(learning_rate='constant', eta0=0.01, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate the mean squared error\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"MSE for {csv_file}: {mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "479d510c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: interpolated_insat_on_imerg_20170109.csv, Number of columns: 144\n",
      "File: interpolated_insat_on_imerg_20170108.csv, Number of columns: 147\n",
      "File: interpolated_insat_on_imerg_20170101.csv, Number of columns: 147\n",
      "File: interpolated_insat_on_imerg_20170103.csv, Number of columns: 147\n",
      "File: interpolated_insat_on_imerg_20170102.csv, Number of columns: 138\n",
      "File: interpolated_insat_on_imerg_20170106.csv, Number of columns: 144\n",
      "File: interpolated_insat_on_imerg_20170107.csv, Number of columns: 147\n",
      "File: interpolated_insat_on_imerg_20170105.csv, Number of columns: 147\n",
      "File: interpolated_insat_on_imerg_20170104.csv, Number of columns: 144\n",
      "File: interpolated_insat_on_imerg_20170110.csv, Number of columns: 147\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the directory where your CSV files are located\n",
    "directory = '/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2017/'\n",
    "\n",
    "# Get a list of all CSV files in the directory\n",
    "csv_files = [file for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "\n",
    "# Iterate through each CSV file and count the columns\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(directory, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    num_columns = len(df.columns)\n",
    "    print(f\"File: {file}, Number of columns: {num_columns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80933ed7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- 1015 IMG_TIR1\n- 1015 IMG_TIR2\n- 1015 IMG_WV\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m y_test \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecipitationCal\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Make predictions on the test set\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Calculate the mean squared error for the current CSV file\u001b[39;00m\n\u001b[1;32m     52\u001b[0m mse \u001b[38;5;241m=\u001b[39m mean_squared_error(y_test, y_pred)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_stochastic_gradient.py:1629\u001b[0m, in \u001b[0;36mBaseSGDRegressor.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1616\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m   1617\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Predict using the linear model.\u001b[39;00m\n\u001b[1;32m   1618\u001b[0m \n\u001b[1;32m   1619\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1627\u001b[0m \u001b[38;5;124;03m       Predicted target values per element in X.\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decision_function(X)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_stochastic_gradient.py:1611\u001b[0m, in \u001b[0;36mBaseSGDRegressor._decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1598\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict using the linear model\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m \n\u001b[1;32m   1600\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1607\u001b[0m \u001b[38;5;124;03m   Predicted target values per element in X.\u001b[39;00m\n\u001b[1;32m   1608\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1609\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1611\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1613\u001b[0m scores \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[1;32m   1614\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mravel()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:548\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    485\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[1;32m    490\u001b[0m ):\n\u001b[1;32m    491\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \n\u001b[1;32m    493\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 548\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_feature_names(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    551\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    552\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    553\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    554\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:481\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[1;32m    477\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    479\u001b[0m     )\n\u001b[0;32m--> 481\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- 1015 IMG_TIR1\n- 1015 IMG_TIR2\n- 1015 IMG_WV\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# Directory containing your CSV files\n",
    "folder_path = '/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2017/'\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Shuffle the list of CSV files\n",
    "import random\n",
    "random.shuffle(csv_files)\n",
    "\n",
    "# Calculate the index to split files into training and testing sets\n",
    "split_index = int(len(csv_files) * 0.7)\n",
    "\n",
    "# Training set and Testing set\n",
    "train_files = csv_files[:split_index]\n",
    "test_files = csv_files[split_index:]\n",
    "\n",
    "total_mse = 0  # Variable to accumulate MSE for all CSV files\n",
    "\n",
    "# Loop through each CSV file in the training set\n",
    "for csv_file in train_files:\n",
    "    # Load the data from CSV file\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))\n",
    "\n",
    "    # Assuming your data is formatted appropriately, extract features (X) and target (y)\n",
    "    X = df.drop(columns=['precipitationCal'])  # Adjust 'precipitationCal' to your target column name\n",
    "    y = df['precipitationCal']\n",
    "\n",
    "    # Create and train your model\n",
    "    model = SGDRegressor(learning_rate='constant', eta0=0.01, random_state=42)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Loop through each CSV file in the testing set\n",
    "for csv_file in test_files:\n",
    "    # Load the data from CSV file\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))\n",
    "\n",
    "    # Assuming your data is formatted appropriately, extract features (X) and target (y)\n",
    "    X_test = df.drop(columns=['precipitationCal'])  # Adjust 'precipitationCal' to your target column name\n",
    "    y_test = df['precipitationCal']\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate the mean squared error for the current CSV file\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"MSE for {csv_file}: {mse:.4f}\")\n",
    "\n",
    "    # Accumulate MSE for all CSV files\n",
    "    total_mse += mse\n",
    "\n",
    "# Calculate average MSE\n",
    "average_mse = total_mse / len(test_files)\n",
    "print(f\"Average MSE for all CSV files in the testing set: {average_mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77fe390b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '2017-01-01'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# Create and train your model\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     model \u001b[38;5;241m=\u001b[39m SGDRegressor(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m'\u001b[39m, eta0\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Loop through each CSV file in the testing set\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m csv_file \u001b[38;5;129;01min\u001b[39;00m test_files:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Load the data from CSV file\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_stochastic_gradient.py:1585\u001b[0m, in \u001b[0;36mBaseSGDRegressor.fit\u001b[0;34m(self, X, y, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m   1582\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_more_validate_params()\n\u001b[0;32m-> 1585\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[1;32m   1586\u001b[0m     X,\n\u001b[1;32m   1587\u001b[0m     y,\n\u001b[1;32m   1588\u001b[0m     alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha,\n\u001b[1;32m   1589\u001b[0m     C\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m   1590\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss,\n\u001b[1;32m   1591\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate,\n\u001b[1;32m   1592\u001b[0m     coef_init\u001b[38;5;241m=\u001b[39mcoef_init,\n\u001b[1;32m   1593\u001b[0m     intercept_init\u001b[38;5;241m=\u001b[39mintercept_init,\n\u001b[1;32m   1594\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m   1595\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_stochastic_gradient.py:1530\u001b[0m, in \u001b[0;36mBaseSGDRegressor._fit\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m   1527\u001b[0m \u001b[38;5;66;03m# Clear iteration count for multiple call to fit.\u001b[39;00m\n\u001b[1;32m   1528\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m-> 1530\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_partial_fit(\n\u001b[1;32m   1531\u001b[0m     X,\n\u001b[1;32m   1532\u001b[0m     y,\n\u001b[1;32m   1533\u001b[0m     alpha,\n\u001b[1;32m   1534\u001b[0m     C,\n\u001b[1;32m   1535\u001b[0m     loss,\n\u001b[1;32m   1536\u001b[0m     learning_rate,\n\u001b[1;32m   1537\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter,\n\u001b[1;32m   1538\u001b[0m     sample_weight,\n\u001b[1;32m   1539\u001b[0m     coef_init,\n\u001b[1;32m   1540\u001b[0m     intercept_init,\n\u001b[1;32m   1541\u001b[0m )\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1544\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1545\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf\n\u001b[1;32m   1546\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter\n\u001b[1;32m   1547\u001b[0m ):\n\u001b[1;32m   1548\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1549\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaximum number of iteration reached before \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1550\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvergence. Consider increasing max_iter to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1551\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimprove the fit.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1552\u001b[0m         ConvergenceWarning,\n\u001b[1;32m   1553\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_stochastic_gradient.py:1435\u001b[0m, in \u001b[0;36mBaseSGDRegressor._partial_fit\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, max_iter, sample_weight, coef_init, intercept_init)\u001b[0m\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_partial_fit\u001b[39m(\n\u001b[1;32m   1422\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1423\u001b[0m     X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1432\u001b[0m     intercept_init,\n\u001b[1;32m   1433\u001b[0m ):\n\u001b[1;32m   1434\u001b[0m     first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoef_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1435\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m   1436\u001b[0m         X,\n\u001b[1;32m   1437\u001b[0m         y,\n\u001b[1;32m   1438\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1439\u001b[0m         copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1440\u001b[0m         order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1441\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64,\n\u001b[1;32m   1442\u001b[0m         accept_large_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1443\u001b[0m         reset\u001b[38;5;241m=\u001b[39mfirst_call,\n\u001b[1;32m   1444\u001b[0m     )\n\u001b[1;32m   1445\u001b[0m     y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1447\u001b[0m     n_samples, n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1101\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1104\u001b[0m     )\n\u001b[0;32m-> 1106\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1107\u001b[0m     X,\n\u001b[1;32m   1108\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   1109\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[1;32m   1110\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1111\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[1;32m   1112\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m   1113\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[1;32m   1114\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[1;32m   1115\u001b[0m     allow_nd\u001b[38;5;241m=\u001b[39mallow_nd,\n\u001b[1;32m   1116\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39mensure_min_samples,\n\u001b[1;32m   1117\u001b[0m     ensure_min_features\u001b[38;5;241m=\u001b[39mensure_min_features,\n\u001b[1;32m   1118\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m   1119\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1120\u001b[0m )\n\u001b[1;32m   1122\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    877\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 879\u001b[0m         array \u001b[38;5;241m=\u001b[39m _asarray_with_order(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    883\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    182\u001b[0m     xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(array)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy.array_api\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:2070\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   2069\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: npt\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m-> 2070\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '2017-01-01'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# Directory containing your CSV files\n",
    "folder_path = '/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2017/'\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Shuffle the list of CSV files\n",
    "import random\n",
    "random.shuffle(csv_files)\n",
    "\n",
    "# Calculate the index to split files into training and testing sets\n",
    "split_index = int(len(csv_files) * 0.7)\n",
    "\n",
    "# Training set and Testing set\n",
    "train_files = csv_files[:split_index]\n",
    "test_files = csv_files[split_index:]\n",
    "\n",
    "total_mse = 0  # Variable to accumulate MSE for all CSV files\n",
    "\n",
    "# Initialize sets to store all feature column names\n",
    "all_feature_columns = set()\n",
    "\n",
    "# Loop through each CSV file in the training set to collect feature column names\n",
    "for csv_file in train_files:\n",
    "    # Load the data from CSV file\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))\n",
    "    \n",
    "    # Extract all feature columns\n",
    "    feature_columns = set(df.columns) - {'precipitationCal'}  # Exclude target column\n",
    "    all_feature_columns.update(feature_columns)\n",
    "\n",
    "# Convert the set of all feature column names to a list\n",
    "all_feature_columns = list(all_feature_columns)\n",
    "\n",
    "# Loop through each CSV file in the training set again to train the model\n",
    "for csv_file in train_files:\n",
    "    # Load the data from CSV file\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))\n",
    "\n",
    "    # Assuming your data is formatted appropriately, extract features (X) and target (y)\n",
    "    X = df[all_feature_columns]  # Select all feature columns\n",
    "    y = df['precipitationCal']\n",
    "\n",
    "    # Create and train your model\n",
    "    model = SGDRegressor(learning_rate='constant', eta0=0.01, random_state=42)\n",
    "    model.fit(X, y)\n",
    "\n",
    "# Loop through each CSV file in the testing set\n",
    "for csv_file in test_files:\n",
    "    # Load the data from CSV file\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))\n",
    "\n",
    "    # Assuming your data is formatted appropriately, extract features (X) and target (y)\n",
    "    X_test = df[all_feature_columns]  # Select all feature columns\n",
    "    y_test = df['precipitationCal']\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate the mean squared error for the current CSV file\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"MSE for {csv_file}: {mse:.4f}\")\n",
    "\n",
    "    # Accumulate MSE for all CSV files\n",
    "    total_mse += mse\n",
    "\n",
    "# Calculate average MSE\n",
    "average_mse = total_mse / len(test_files)\n",
    "print(f\"Average MSE for all CSV files in the testing set: {average_mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39f46415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Names: Index(['longitude', 'latitude', '20170106 0015 IMG_TIR1',\n",
      "       '20170106 0015 IMG_TIR2', '20170106 0015 IMG_WV',\n",
      "       '20170106 0045 IMG_TIR1', '20170106 0045 IMG_TIR2',\n",
      "       '20170106 0045 IMG_WV', '20170106 0115 IMG_TIR1',\n",
      "       '20170106 0115 IMG_TIR2',\n",
      "       ...\n",
      "       '20170106 2245 IMG_TIR1', '20170106 2245 IMG_TIR2',\n",
      "       '20170106 2245 IMG_WV', '20170106 2315 IMG_TIR1',\n",
      "       '20170106 2315 IMG_TIR2', '20170106 2315 IMG_WV',\n",
      "       '20170106 2345 IMG_TIR1', '20170106 2345 IMG_TIR2',\n",
      "       '20170106 2345 IMG_WV', 'precipitationCal'],\n",
      "      dtype='object', length=144)\n",
      "Missing Columns: ['column1', 'column2', 'column3']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['column1', 'column2', 'column3'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll feature columns present.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Now you can proceed with selecting features and target\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m X \u001b[38;5;241m=\u001b[39m df[all_feature_columns]\n\u001b[1;32m     26\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecipitationCal\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3813\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3812\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3813\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3815\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6070\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6067\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6068\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6070\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6072\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6074\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6130\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   6129\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 6130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6132\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['column1', 'column2', 'column3'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the folder path and CSV file name\n",
    "folder_path = \"/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2017/\"\n",
    "#csv_file = \"your_data.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(os.path.join(folder_path, csv_file))\n",
    "\n",
    "# Print the column names in the DataFrame\n",
    "print(\"Column Names:\", df.columns)\n",
    "\n",
    "# Double-check that all feature columns are present in the DataFrame\n",
    "all_feature_columns = ['column1', 'column2', 'column3']  # Replace with your actual feature column names\n",
    "\n",
    "missing_columns = [col for col in all_feature_columns if col not in df.columns]\n",
    "if missing_columns:\n",
    "    print(\"Missing Columns:\", missing_columns)\n",
    "    # Handle missing columns as needed\n",
    "else:\n",
    "    print(\"All feature columns present.\")\n",
    "\n",
    "# Now you can proceed with selecting features and target\n",
    "X = df[all_feature_columns]\n",
    "y = df['precipitationCal']\n",
    "\n",
    "# Proceed with model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7f2c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "import re\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = '/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2017/'\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Shuffle the list of CSV files\n",
    "import random\n",
    "random.shuffle(csv_files)\n",
    "\n",
    "# Define a function to extract the common part of column names using regular expressions\n",
    "def extract_variable_name(column_name):\n",
    "    match = re.match(r'\\d+\\s+\\d+\\s+IMG_(.*)', column_name)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create a mapping of old column names to new standardized names\n",
    "rename_mapping = {}\n",
    "\n",
    "# Loop through each CSV file to extract variable names and create the mapping\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))\n",
    "    variable_names = df.columns.map(extract_variable_name).unique()\n",
    "    for variable_name in variable_names:\n",
    "        if variable_name not in rename_mapping.values():\n",
    "            standard_name = f\"IMG_{variable_name}\"\n",
    "            rename_mapping[variable_name] = standard_name\n",
    "\n",
    "total_mse = 0  # Variable to accumulate MSE for all CSV files\n",
    "\n",
    "# Training set and Testing set\n",
    "split_index = int(len(csv_files) * 0.7)\n",
    "train_files = csv_files[:split_index]\n",
    "test_files = csv_files[split_index:]\n",
    "\n",
    "# Loop through each CSV file in the training set\n",
    "for csv_file in train_files:\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))\n",
    "    df.rename(columns=rename_mapping, inplace=True)\n",
    "    # Assuming your data is formatted appropriately, extract features (X) and target (y)\n",
    "    X = df.drop(columns=['precipitationCal'])  # Adjust 'precipitationCal' to your target column name\n",
    "    y = df['precipitationCal']\n",
    "\n",
    "    # Create and train your model\n",
    "    model = SGDRegressor(learning_rate='constant', eta0=0.01, random_state=42)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Loop through each CSV file in the testing set\n",
    "    for csv_file_test in test_files:\n",
    "        df_test = pd.read_csv(os.path.join(folder_path, csv_file_test))\n",
    "        df_test.rename(columns=rename_mapping, inplace=True)\n",
    "        # Assuming your data is formatted appropriately, extract features (X) and target (y)\n",
    "        X_test = df_test.drop(columns=['precipitationCal'])  # Adjust 'precipitationCal' to your target column name\n",
    "        y_test = df_test['precipitationCal']\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate the mean squared error for the current CSV file\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        print(f\"MSE for {csv_file_test}: {mse:.4f}\")\n",
    "\n",
    "        # Accumulate MSE for all CSV files\n",
    "        total_mse += mse\n",
    "\n",
    "# Calculate average MSE\n",
    "average_mse = total_mse / (len(test_files) * len(train_files))\n",
    "print(f\"Average MSE for all CSV files in the testing set: {average_mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67525fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
