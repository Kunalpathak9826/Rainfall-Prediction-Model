{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "06c0fbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Common Variables (Batch 1):\n",
      "{'Date', '1315 IMG_TIR2', '2315 IMG_TIR2', '1645 IMG_TIR1', '1645 IMG_WV', '0245 IMG_TIR2', '0245 IMG_WV', '0715 IMG_TIR2', '1315 IMG_WV', '1645 IMG_TIR2', '0245 IMG_TIR1', 'latitude', 'precipitationCal', '0715 IMG_WV', '2315 IMG_TIR1', 'longitude', '0715 IMG_TIR1', '1315 IMG_TIR1', '2315 IMG_WV'}\n",
      "\n",
      "Uncommon Variables (Batch 1):\n",
      "{'0215 IMG_TIR2', '1045 IMG_WV', '0915 IMG_TIR1', '0115 IMG_TIR1', '0445 IMG_WV', '0815 IMG_WV', '1845 IMG_WV', '1545 IMG_TIR1', '1845 IMG_TIR2', '2245 IMG_TIR2', '0115 IMG_TIR2', '0815 IMG_TIR1', '0515 IMG_TIR1', '0545 IMG_TIR1', '2145 IMG_TIR1', '0645 IMG_TIR2', '1445 IMG_TIR1', '0845 IMG_TIR1', '0145 IMG_WV', '0945 IMG_WV', '0215 IMG_TIR1', '1915 IMG_WV', '1245 IMG_TIR1', '1015 IMG_TIR1', '1045 IMG_TIR2', '1745 IMG_TIR2', '0815 IMG_TIR2', '2345 IMG_TIR1', '2045 IMG_WV', '1615 IMG_WV', '0645 IMG_TIR1', '2145 IMG_TIR2', '1615 IMG_TIR2', '1615 IMG_TIR1', '2345 IMG_TIR2', '1045 IMG_TIR1', '0945 IMG_TIR1', '1915 IMG_TIR2', '0915 IMG_WV', '0915 IMG_TIR2', '1345 IMG_TIR2', '2345 IMG_WV', '0145 IMG_TIR1', '0045 IMG_TIR2', '2245 IMG_TIR1', '1115 IMG_WV', '1845 IMG_TIR1', '0315 IMG_WV', '0745 IMG_TIR2', '0445 IMG_TIR2', '1145 IMG_TIR1', '1145 IMG_TIR2', '1345 IMG_TIR1', '2115 IMG_TIR2', '0515 IMG_TIR2', '0645 IMG_WV', '1815 IMG_TIR1', '0415 IMG_TIR2', '1715 IMG_TIR1', '2215 IMG_TIR1', '1815 IMG_TIR2', '1945 IMG_TIR1', '1545 IMG_WV', '2015 IMG_TIR2', '2215 IMG_WV', '2145 IMG_WV', '0215 IMG_WV', '2015 IMG_TIR1', '1815 IMG_WV', '0515 IMG_WV', '0345 IMG_TIR1', '1715 IMG_TIR2', '0545 IMG_WV', '0745 IMG_WV', '1245 IMG_WV', '0845 IMG_WV', '1945 IMG_TIR2', '1245 IMG_TIR2', '0015 IMG_TIR1', '1015 IMG_WV', '1745 IMG_TIR1', '1515 IMG_TIR1', '0545 IMG_TIR2', '0845 IMG_TIR2', '1445 IMG_WV', '1545 IMG_TIR2', '1145 IMG_WV', '1915 IMG_TIR1', '1215 IMG_TIR2', '1015 IMG_TIR2', '0045 IMG_TIR1', '1415 IMG_WV', '1215 IMG_WV', '2115 IMG_WV', '0345 IMG_TIR2', '1415 IMG_TIR2', '0115 IMG_WV', '0315 IMG_TIR2', '1445 IMG_TIR2', '0615 IMG_WV', '1515 IMG_TIR2', '2215 IMG_TIR2', '2115 IMG_TIR1', '0415 IMG_TIR1', '1515 IMG_WV', '0945 IMG_TIR2', '2045 IMG_TIR1', '2015 IMG_WV', '0315 IMG_TIR1', '1345 IMG_WV', '1115 IMG_TIR2', '1945 IMG_WV', '1415 IMG_TIR1', '2245 IMG_WV', '0145 IMG_TIR2', '0615 IMG_TIR2', '0345 IMG_WV', '0415 IMG_WV', '1715 IMG_WV', '0615 IMG_TIR1', '1115 IMG_TIR1', '0015 IMG_WV', '0045 IMG_WV', '0445 IMG_TIR1', '0745 IMG_TIR1', '0015 IMG_TIR2', '1215 IMG_TIR1', '2045 IMG_TIR2', '1745 IMG_WV'}\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 29.3917 - val_loss: 4.8184\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 0s 792us/step - loss: 29.3089 - val_loss: 5.2077\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 0s 785us/step - loss: 29.3108 - val_loss: 5.0436\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 0s 787us/step - loss: 29.2935 - val_loss: 4.1505\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 0s 784us/step - loss: 29.2987 - val_loss: 4.9089\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 0s 791us/step - loss: 29.3020 - val_loss: 6.1641\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 0s 791us/step - loss: 29.3043 - val_loss: 6.3770\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 0s 788us/step - loss: 29.2980 - val_loss: 6.6171\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 1s 894us/step - loss: 29.2991 - val_loss: 4.7372\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 1s 962us/step - loss: 29.3077 - val_loss: 6.5110\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 137.3000 - val_loss: 18.8962\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 1s 928us/step - loss: 136.5134 - val_loss: 20.5858\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 1s 837us/step - loss: 136.4948 - val_loss: 28.6922\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 1s 842us/step - loss: 136.5418 - val_loss: 22.4898\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 1s 850us/step - loss: 136.5258 - val_loss: 23.7670\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 1s 922us/step - loss: 136.5127 - val_loss: 23.3873\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 1s 835us/step - loss: 136.5197 - val_loss: 24.3711\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 1s 853us/step - loss: 136.5757 - val_loss: 22.7490\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 1s 973us/step - loss: 136.4855 - val_loss: 25.7802\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 1s 935us/step - loss: 136.5215 - val_loss: 21.7770\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 89.7131 - val_loss: 45.0154\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 1s 931us/step - loss: 88.3451 - val_loss: 45.0852\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 1s 885us/step - loss: 88.3289 - val_loss: 47.1791\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 1s 858us/step - loss: 88.3172 - val_loss: 43.5899\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 1s 858us/step - loss: 88.3273 - val_loss: 46.9166\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 1s 869us/step - loss: 88.3579 - val_loss: 45.8035\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 1s 835us/step - loss: 88.3263 - val_loss: 45.0173\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 1s 807us/step - loss: 88.3003 - val_loss: 46.4201\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 1s 835us/step - loss: 88.3531 - val_loss: 42.2568\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 0s 789us/step - loss: 88.3459 - val_loss: 49.1143\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 2066.7488 - val_loss: 856.0507\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 1s 831us/step - loss: 1845.4254 - val_loss: 917.2050\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 1s 855us/step - loss: 1846.3660 - val_loss: 874.8899\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 1s 873us/step - loss: 1846.6761 - val_loss: 853.0383\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 1s 930us/step - loss: 1846.4279 - val_loss: 892.3815\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 1s 846us/step - loss: 1846.1610 - val_loss: 872.1692\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 1s 839us/step - loss: 1845.8386 - val_loss: 956.3992\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 1s 933us/step - loss: 1846.4084 - val_loss: 901.1636\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 1s 893us/step - loss: 1846.0658 - val_loss: 909.1747\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 1s 940us/step - loss: 1846.4938 - val_loss: 870.0417\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 233.1213 - val_loss: 81.7163\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 1s 866us/step - loss: 229.8292 - val_loss: 84.6058\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 1s 885us/step - loss: 229.9301 - val_loss: 82.5727\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 1s 857us/step - loss: 229.7964 - val_loss: 89.1843\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 1s 866us/step - loss: 229.8236 - val_loss: 80.5426\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 1s 805us/step - loss: 229.8719 - val_loss: 86.2263\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 1s 831us/step - loss: 229.8063 - val_loss: 85.9814\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 1s 808us/step - loss: 229.7900 - val_loss: 78.2747\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 0s 794us/step - loss: 229.8633 - val_loss: 86.9641\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 1s 852us/step - loss: 229.8111 - val_loss: 79.2882\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 2462.5903 - val_loss: 992.3364\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 0s 794us/step - loss: 2218.3621 - val_loss: 955.5673\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 0s 796us/step - loss: 2218.3401 - val_loss: 999.9661\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 0s 788us/step - loss: 2218.1570 - val_loss: 1019.8638\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 0s 786us/step - loss: 2218.3547 - val_loss: 891.3070\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 0s 790us/step - loss: 2218.5527 - val_loss: 991.4450\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 0s 787us/step - loss: 2218.3159 - val_loss: 1038.1899\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 0s 786us/step - loss: 2218.4854 - val_loss: 960.3028\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 0s 789us/step - loss: 2218.2395 - val_loss: 959.8775\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 0s 786us/step - loss: 2218.3982 - val_loss: 1060.6260\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 95.1692 - val_loss: 38.1800\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 1s 802us/step - loss: 93.3390 - val_loss: 43.4202\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 0s 785us/step - loss: 93.3225 - val_loss: 39.5483\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 0s 779us/step - loss: 93.3318 - val_loss: 39.1228\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 0s 782us/step - loss: 93.2532 - val_loss: 36.8469\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 0s 782us/step - loss: 93.2793 - val_loss: 43.2902\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 0s 778us/step - loss: 93.3038 - val_loss: 39.1251\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 0s 777us/step - loss: 93.3422 - val_loss: 40.9012\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 0s 775us/step - loss: 93.2934 - val_loss: 38.3812\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 0s 773us/step - loss: 93.2626 - val_loss: 40.8302\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 2s 1ms/step - loss: 449.7130 - val_loss: 454.3745\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 0s 798us/step - loss: 447.3071 - val_loss: 452.4500\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 0s 793us/step - loss: 447.4881 - val_loss: 455.4861\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 0s 781us/step - loss: 447.5559 - val_loss: 455.3502\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 0s 781us/step - loss: 447.3314 - val_loss: 451.4319\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 0s 789us/step - loss: 447.4636 - val_loss: 454.1336\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 0s 784us/step - loss: 447.3296 - val_loss: 452.6082\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 0s 778us/step - loss: 447.2881 - val_loss: 457.2906\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 0s 781us/step - loss: 447.3858 - val_loss: 455.9066\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 0s 780us/step - loss: 447.3772 - val_loss: 455.7610\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 791.8998 - val_loss: 257.5140\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 0s 786us/step - loss: 758.2842 - val_loss: 250.0303\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 0s 794us/step - loss: 758.0431 - val_loss: 258.4505\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 1s 804us/step - loss: 758.3102 - val_loss: 240.3179\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 0s 791us/step - loss: 758.2326 - val_loss: 247.8331\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 0s 782us/step - loss: 758.1940 - val_loss: 232.8736\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 0s 783us/step - loss: 758.4910 - val_loss: 242.7569\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 0s 785us/step - loss: 758.1619 - val_loss: 256.4138\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 0s 785us/step - loss: 758.0887 - val_loss: 254.6140\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 0s 783us/step - loss: 758.2606 - val_loss: 243.8040\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 192.7896 - val_loss: 65.9720\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 0s 792us/step - loss: 188.1096 - val_loss: 66.4734\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 0s 785us/step - loss: 188.1632 - val_loss: 61.7122\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 0s 782us/step - loss: 188.1575 - val_loss: 57.5556\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 0s 783us/step - loss: 188.1369 - val_loss: 60.7885\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 0s 787us/step - loss: 188.1613 - val_loss: 63.6552\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 0s 785us/step - loss: 188.1166 - val_loss: 70.4110\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 0s 783us/step - loss: 188.1361 - val_loss: 66.2618\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 0s 785us/step - loss: 188.1792 - val_loss: 63.9765\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 0s 782us/step - loss: 188.1615 - val_loss: 59.4586\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 2151.2419 - val_loss: 1558.0367\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 0s 781us/step - loss: 1856.9558 - val_loss: 1550.0195\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 0s 783us/step - loss: 1857.0106 - val_loss: 1574.9191\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 0s 781us/step - loss: 1856.8594 - val_loss: 1546.4362\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 0s 782us/step - loss: 1855.2482 - val_loss: 1563.6714\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 0s 777us/step - loss: 1857.1820 - val_loss: 1592.0370\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 0s 780us/step - loss: 1857.1254 - val_loss: 1580.3651\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 0s 784us/step - loss: 1856.6379 - val_loss: 1552.5950\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 0s 793us/step - loss: 1857.0826 - val_loss: 1580.4581\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 0s 783us/step - loss: 1856.8926 - val_loss: 1566.4395\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 860.3123 - val_loss: 1850.7535\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 0s 786us/step - loss: 818.2390 - val_loss: 1830.9774\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 0s 780us/step - loss: 818.4050 - val_loss: 1847.7704\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 0s 783us/step - loss: 818.2883 - val_loss: 1861.5162\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 0s 781us/step - loss: 818.0792 - val_loss: 1849.9786\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 0s 779us/step - loss: 818.1667 - val_loss: 1829.6403\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 0s 782us/step - loss: 818.1115 - val_loss: 1878.8630\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 0s 781us/step - loss: 818.4009 - val_loss: 1828.9344\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 0s 782us/step - loss: 818.1641 - val_loss: 1838.7136\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 0s 780us/step - loss: 818.2905 - val_loss: 1848.6050\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 2574.7170 - val_loss: 759.4714\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 0s 787us/step - loss: 2465.0603 - val_loss: 756.7468\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 0s 782us/step - loss: 2464.2180 - val_loss: 769.0278\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 1s 802us/step - loss: 2464.2075 - val_loss: 754.3856\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 0s 783us/step - loss: 2465.3279 - val_loss: 754.4036\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 0s 780us/step - loss: 2464.9221 - val_loss: 755.0493\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 0s 782us/step - loss: 2464.9172 - val_loss: 754.4859\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 0s 780us/step - loss: 2464.5420 - val_loss: 755.5162\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 0s 780us/step - loss: 2464.9380 - val_loss: 755.7480\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 0s 783us/step - loss: 2464.6665 - val_loss: 757.2774\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 1642.4214 - val_loss: 646.7191\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 0s 788us/step - loss: 1458.1108 - val_loss: 610.0257\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 0s 787us/step - loss: 1458.7665 - val_loss: 658.3287\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 0s 784us/step - loss: 1457.9508 - val_loss: 636.1478\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 0s 786us/step - loss: 1458.5322 - val_loss: 711.3256\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 0s 780us/step - loss: 1458.5803 - val_loss: 648.8995\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 0s 783us/step - loss: 1458.5829 - val_loss: 666.3491\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 0s 781us/step - loss: 1458.2040 - val_loss: 653.4357\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 0s 781us/step - loss: 1458.2734 - val_loss: 632.4703\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 0s 777us/step - loss: 1458.4120 - val_loss: 626.2632\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 160.1223 - val_loss: 74.5542\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 0s 788us/step - loss: 155.6039 - val_loss: 76.9423\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 0s 788us/step - loss: 155.6279 - val_loss: 84.9254\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 0s 783us/step - loss: 155.6138 - val_loss: 67.2470\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 0s 784us/step - loss: 155.6096 - val_loss: 70.1292\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 0s 782us/step - loss: 155.5859 - val_loss: 74.0978\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 0s 780us/step - loss: 155.6331 - val_loss: 79.1974\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 0s 778us/step - loss: 155.6570 - val_loss: 78.6825\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 0s 779us/step - loss: 155.5659 - val_loss: 82.1624\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 0s 782us/step - loss: 155.6032 - val_loss: 77.9442\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 686.8954 - val_loss: 106.7079\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 0s 788us/step - loss: 675.8700 - val_loss: 111.6632\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 0s 782us/step - loss: 675.6800 - val_loss: 108.6258\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 0s 799us/step - loss: 675.6531 - val_loss: 114.0770\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 0s 780us/step - loss: 675.8593 - val_loss: 106.2026\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 0s 783us/step - loss: 675.7449 - val_loss: 105.8906\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 0s 784us/step - loss: 675.7059 - val_loss: 103.7118\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 0s 778us/step - loss: 675.8078 - val_loss: 114.4356\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 0s 780us/step - loss: 675.7763 - val_loss: 109.0917\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 0s 781us/step - loss: 675.6279 - val_loss: 111.0902\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 1548.8551 - val_loss: 929.2292\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 0s 791us/step - loss: 1516.8231 - val_loss: 929.4279\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 0s 785us/step - loss: 1516.5403 - val_loss: 932.8457\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 0s 781us/step - loss: 1517.5651 - val_loss: 929.2674\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 0s 789us/step - loss: 1517.5962 - val_loss: 929.2942\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 0s 781us/step - loss: 1516.9543 - val_loss: 929.3652\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 0s 777us/step - loss: 1517.0892 - val_loss: 930.2266\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 0s 780us/step - loss: 1517.4514 - val_loss: 929.9621\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 0s 780us/step - loss: 1516.5925 - val_loss: 929.3187\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 0s 785us/step - loss: 1517.2375 - val_loss: 930.6079\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 398.4872 - val_loss: 178.5552\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 0s 776us/step - loss: 374.3942 - val_loss: 188.7205\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 0s 789us/step - loss: 374.4854 - val_loss: 201.3679\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 0s 794us/step - loss: 374.4869 - val_loss: 197.8542\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 0s 785us/step - loss: 374.3372 - val_loss: 206.2583\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 0s 784us/step - loss: 374.6239 - val_loss: 222.4030\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 0s 775us/step - loss: 374.3561 - val_loss: 192.3579\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 0s 780us/step - loss: 374.6033 - val_loss: 202.0171\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 0s 780us/step - loss: 374.3637 - val_loss: 210.7865\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 0s 777us/step - loss: 374.3981 - val_loss: 206.0870\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 289.5952 - val_loss: 236.3908\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 0s 785us/step - loss: 264.9383 - val_loss: 258.2262\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 0s 781us/step - loss: 264.8690 - val_loss: 250.4097\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 0s 781us/step - loss: 264.9464 - val_loss: 249.6160\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 0s 781us/step - loss: 264.8991 - val_loss: 244.5645\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 0s 786us/step - loss: 264.9054 - val_loss: 245.1991\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 0s 778us/step - loss: 264.9735 - val_loss: 242.1460\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 0s 782us/step - loss: 264.9261 - val_loss: 266.8345\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 0s 780us/step - loss: 264.9859 - val_loss: 240.2232\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 0s 783us/step - loss: 264.8597 - val_loss: 263.7313\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 2s 1ms/step - loss: 189.5685 - val_loss: 109.1109\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 0s 785us/step - loss: 182.8299 - val_loss: 123.8053\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 0s 782us/step - loss: 182.8062 - val_loss: 107.1927\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 0s 785us/step - loss: 182.9095 - val_loss: 122.6167\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 0s 780us/step - loss: 182.8864 - val_loss: 111.0075\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 0s 782us/step - loss: 182.8720 - val_loss: 111.2226\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 0s 779us/step - loss: 182.7859 - val_loss: 95.3808\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 0s 780us/step - loss: 182.9096 - val_loss: 108.6591\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 0s 778us/step - loss: 182.9217 - val_loss: 115.8294\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 0s 785us/step - loss: 182.8314 - val_loss: 111.1417\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 92.9206 - val_loss: 1245.7513\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 0s 784us/step - loss: 92.7654 - val_loss: 1242.8428\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 0s 781us/step - loss: 92.7599 - val_loss: 1251.9727\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 0s 776us/step - loss: 92.7877 - val_loss: 1254.3092\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 0s 780us/step - loss: 92.7722 - val_loss: 1253.9363\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 0s 779us/step - loss: 92.7531 - val_loss: 1253.2271\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 0s 773us/step - loss: 92.7559 - val_loss: 1252.9640\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 0s 777us/step - loss: 92.7615 - val_loss: 1243.8405\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 0s 776us/step - loss: 92.7585 - val_loss: 1237.2611\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 0s 782us/step - loss: 92.7170 - val_loss: 1247.9607\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 122.0811 - val_loss: 196.2560\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 0s 784us/step - loss: 120.9703 - val_loss: 194.7744\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 0s 786us/step - loss: 120.8635 - val_loss: 197.0565\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 0s 779us/step - loss: 120.9485 - val_loss: 196.7464\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 0s 782us/step - loss: 120.9822 - val_loss: 194.8636\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 0s 781us/step - loss: 120.9534 - val_loss: 195.3156\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 0s 784us/step - loss: 120.9765 - val_loss: 194.4042\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 0s 783us/step - loss: 120.9691 - val_loss: 196.4705\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 0s 781us/step - loss: 120.9198 - val_loss: 194.6163\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 0s 780us/step - loss: 120.9666 - val_loss: 196.9603\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 317.9323 - val_loss: 153.0801\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 0s 788us/step - loss: 311.7365 - val_loss: 147.4793\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 0s 788us/step - loss: 311.8383 - val_loss: 150.3836\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 0s 788us/step - loss: 311.8733 - val_loss: 135.6010\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 0s 792us/step - loss: 311.7942 - val_loss: 153.9515\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 0s 786us/step - loss: 311.6837 - val_loss: 171.9846\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 0s 781us/step - loss: 311.8995 - val_loss: 136.3051\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 0s 780us/step - loss: 311.6920 - val_loss: 147.5235\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 0s 792us/step - loss: 311.9562 - val_loss: 171.1063\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 0s 784us/step - loss: 311.7656 - val_loss: 132.5217\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 69.1360 - val_loss: 25.2457\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 0s 788us/step - loss: 68.6696 - val_loss: 25.0206\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 0s 794us/step - loss: 68.6547 - val_loss: 26.1793\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 0s 783us/step - loss: 68.6454 - val_loss: 26.8924\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 0s 782us/step - loss: 68.7050 - val_loss: 26.5685\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 0s 784us/step - loss: 68.6472 - val_loss: 25.2635\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 0s 786us/step - loss: 68.6599 - val_loss: 26.0940\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 0s 786us/step - loss: 68.6724 - val_loss: 26.5161\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 0s 784us/step - loss: 68.6897 - val_loss: 24.6972\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 0s 791us/step - loss: 68.6861 - val_loss: 27.4850\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 119.7597 - val_loss: 31.7099\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 0s 789us/step - loss: 118.1419 - val_loss: 34.4953\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 0s 788us/step - loss: 118.1516 - val_loss: 37.3627\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 0s 785us/step - loss: 118.1971 - val_loss: 35.3474\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 0s 784us/step - loss: 118.1687 - val_loss: 30.6709\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 0s 784us/step - loss: 118.1730 - val_loss: 32.5088\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 0s 782us/step - loss: 118.1779 - val_loss: 38.3113\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 0s 782us/step - loss: 118.1515 - val_loss: 34.5329\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 0s 778us/step - loss: 118.1928 - val_loss: 48.1463\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 0s 783us/step - loss: 118.2246 - val_loss: 29.4102\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 70.6239 - val_loss: 19.0258\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 0s 787us/step - loss: 69.8359 - val_loss: 20.8577\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 0s 784us/step - loss: 69.8733 - val_loss: 19.8726\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 0s 783us/step - loss: 69.8326 - val_loss: 21.8425\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 0s 782us/step - loss: 69.8304 - val_loss: 20.8811\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 0s 782us/step - loss: 69.8590 - val_loss: 18.6583\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 0s 782us/step - loss: 69.8568 - val_loss: 20.0334\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 0s 783us/step - loss: 69.8559 - val_loss: 18.6622\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 0s 785us/step - loss: 69.8909 - val_loss: 18.3896\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 0s 790us/step - loss: 69.8453 - val_loss: 19.8792\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 30.0142 - val_loss: 51.0653\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 0s 784us/step - loss: 30.0112 - val_loss: 50.8202\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 0s 789us/step - loss: 30.0305 - val_loss: 50.9114\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 0s 789us/step - loss: 30.0122 - val_loss: 51.8275\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 0s 786us/step - loss: 30.0149 - val_loss: 51.1531\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 0s 781us/step - loss: 30.0031 - val_loss: 51.4664\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 0s 780us/step - loss: 30.0038 - val_loss: 51.2358\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 0s 778us/step - loss: 30.0124 - val_loss: 51.3708\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 0s 781us/step - loss: 30.0028 - val_loss: 51.0784\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 0s 788us/step - loss: 30.0098 - val_loss: 51.1739\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 117.7254 - val_loss: 10.6902\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 0s 784us/step - loss: 117.2604 - val_loss: 11.0539\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 0s 785us/step - loss: 117.3531 - val_loss: 10.4846\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 0s 779us/step - loss: 117.3323 - val_loss: 15.0200\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 0s 776us/step - loss: 117.3852 - val_loss: 10.4265\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 0s 785us/step - loss: 117.3466 - val_loss: 9.2517\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 0s 778us/step - loss: 117.3337 - val_loss: 9.4221\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 1s 817us/step - loss: 117.3038 - val_loss: 11.6134\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 0s 782us/step - loss: 117.3009 - val_loss: 7.9152\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 0s 781us/step - loss: 117.3166 - val_loss: 9.9238\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 87.6027 - val_loss: 7.1304\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 0s 786us/step - loss: 87.4305 - val_loss: 6.5054\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 0s 782us/step - loss: 87.3694 - val_loss: 6.9575\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 0s 785us/step - loss: 87.3784 - val_loss: 6.6304\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 0s 789us/step - loss: 87.3822 - val_loss: 10.5315\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 0s 783us/step - loss: 87.4005 - val_loss: 7.7126\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 0s 781us/step - loss: 87.3876 - val_loss: 6.6438\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 0s 786us/step - loss: 87.3592 - val_loss: 7.0539\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 0s 788us/step - loss: 87.3619 - val_loss: 8.0193\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 0s 782us/step - loss: 87.3708 - val_loss: 7.3469\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 2s 1ms/step - loss: 66.1521 - val_loss: 9.7087\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 0s 791us/step - loss: 66.0096 - val_loss: 11.2238\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 0s 791us/step - loss: 66.0221 - val_loss: 10.3550\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 0s 795us/step - loss: 65.9882 - val_loss: 9.0813\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 0s 787us/step - loss: 65.9947 - val_loss: 12.3058\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 0s 783us/step - loss: 65.9927 - val_loss: 6.6928\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 0s 783us/step - loss: 66.0617 - val_loss: 9.5257\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 0s 783us/step - loss: 66.0570 - val_loss: 11.3564\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 0s 789us/step - loss: 66.0211 - val_loss: 11.3569\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 0s 785us/step - loss: 65.9805 - val_loss: 17.6017\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 37.7653 - val_loss: 8.4499\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 0s 779us/step - loss: 37.5579 - val_loss: 8.2787\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 0s 780us/step - loss: 37.5626 - val_loss: 6.0271\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 0s 794us/step - loss: 37.5367 - val_loss: 7.3246\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 0s 777us/step - loss: 37.5616 - val_loss: 7.8603\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 0s 790us/step - loss: 37.5636 - val_loss: 7.5303\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 0s 780us/step - loss: 37.5515 - val_loss: 7.9719\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 0s 779us/step - loss: 37.5623 - val_loss: 6.5190\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 0s 780us/step - loss: 37.5555 - val_loss: 7.4885\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 0s 782us/step - loss: 37.5655 - val_loss: 6.2598\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 44.2178 - val_loss: 5.3391\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 0s 782us/step - loss: 44.1777 - val_loss: 6.0722\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 0s 783us/step - loss: 44.1334 - val_loss: 5.7232\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 0s 788us/step - loss: 44.1326 - val_loss: 5.2434\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 0s 780us/step - loss: 44.1520 - val_loss: 5.7047\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 0s 778us/step - loss: 44.1299 - val_loss: 5.7593\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 0s 786us/step - loss: 44.1294 - val_loss: 5.4967\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 0s 776us/step - loss: 44.1413 - val_loss: 6.7970\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 0s 780us/step - loss: 44.1170 - val_loss: 5.3637\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 0s 783us/step - loss: 44.1179 - val_loss: 6.0726\n",
      "\n",
      "Processed files: ['interpolated_insat_on_imerg_20170109.csv', 'interpolated_insat_on_imerg_20170108.csv', 'interpolated_insat_on_imerg_20190101.csv', 'interpolated_insat_on_imerg_20180104.csv', 'interpolated_insat_on_imerg_20180110.csv', 'interpolated_insat_on_imerg_20180105.csv', 'interpolated_insat_on_imerg_20190102.csv', 'interpolated_insat_on_imerg_20180107.csv', 'interpolated_insat_on_imerg_20180106.csv', 'interpolated_insat_on_imerg_20190103.csv', 'interpolated_insat_on_imerg_20190107.csv', 'interpolated_insat_on_imerg_20180102.csv', 'interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20190106.csv', 'interpolated_insat_on_imerg_20190110.csv', 'interpolated_insat_on_imerg_20190104.csv', 'interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20190105.csv', 'interpolated_insat_on_imerg_20190108.csv', 'interpolated_insat_on_imerg_20190109.csv', 'interpolated_insat_on_imerg_20180108.csv', 'interpolated_insat_on_imerg_20180109.csv', 'interpolated_insat_on_imerg_20230204.csv', 'interpolated_insat_on_imerg_20170101.csv', 'interpolated_insat_on_imerg_20170103.csv', 'interpolated_insat_on_imerg_20170102.csv', 'interpolated_insat_on_imerg_20230415.csv', 'interpolated_insat_on_imerg_20170106.csv', 'interpolated_insat_on_imerg_20170107.csv', 'interpolated_insat_on_imerg_20170105.csv', 'interpolated_insat_on_imerg_20170104.csv', 'interpolated_insat_on_imerg_20170110.csv']\n",
      "49219/49219 [==============================] - 16s 319us/step\n",
      "\n",
      "Expected Error (Ground Truth):\n",
      "[4.302059  4.302059  2.0000336 2.0000336 2.1598122 2.1598122 1.9036447\n",
      " 1.9036447 3.3310275 3.3310275]\n",
      "Mean Squared Error on Test Set: 648.4125\n",
      "Root Mean Squared Error on Test Set: 25.4639\n",
      "Percentage Error on Test Set: -58.90%\n",
      "Model saved as RNN_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunalpathak9826/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Directory containing your CSV files\n",
    "folder_path = '/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/'\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Batch size\n",
    "batch_size = 34\n",
    "\n",
    "# Function for timestamp comparison\n",
    "def compare_timestamps(var1, var2):\n",
    "  try:\n",
    "    timestamp1 = int(var1.split(\"_\")[0])\n",
    "    timestamp2 = int(var2.split(\"_\")[0])\n",
    "  except ValueError:\n",
    "    return False\n",
    "  threshold = 1  # You can adjust this threshold as needed\n",
    "  return abs(timestamp1 - timestamp2) <= threshold\n",
    "\n",
    "batch_wise_results = []\n",
    "\n",
    "# Loop through files in batches\n",
    "for start_index in range(0, len(csv_files), batch_size):\n",
    "  # Get current batch of files\n",
    "  batch_files = csv_files[start_index:start_index + batch_size]\n",
    "\n",
    "  # Initialize common columns\n",
    "  common_columns = set(pd.read_csv(os.path.join(folder_path, batch_files[0])).columns)\n",
    "\n",
    "  # Find common columns within the batch\n",
    "  for csv_file in batch_files[1:]:\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))\n",
    "    common_columns = common_columns.intersection(df.columns)\n",
    "    \n",
    "  # Check if common variables are less than 5, skip the batch\n",
    "  if len(common_columns) < 5:\n",
    "    print(f\"\\nSkipping Batch {start_index // batch_size + 1} (Less than 5 common variables):\")\n",
    "    for file in batch_files:\n",
    "      print(f\"- {file}\")\n",
    "    continue\n",
    "\n",
    "  # Print common variables for this batch\n",
    "  print(f\"\\nCommon Variables (Batch {start_index // batch_size + 1}):\")\n",
    "  print(common_columns)\n",
    "\n",
    "  # Initialize all columns\n",
    "  all_columns = set()\n",
    "\n",
    "  # Find all unique columns within the batch\n",
    "  for csv_file in batch_files:\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))\n",
    "    all_columns.update(df.columns)\n",
    "\n",
    "  # Find uncommon variables for this batch\n",
    "  uncommon_variables = all_columns - common_columns\n",
    "  print(f\"\\nUncommon Variables (Batch {start_index // batch_size + 1}):\")\n",
    "  print(uncommon_variables)\n",
    "\n",
    "  # Process data within the batch\n",
    "  concatenated_data = None\n",
    "  for idx, csv_file in enumerate(batch_files):\n",
    "    # Load data from CSV file\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))[list(common_columns) + ['precipitationCal']]\n",
    "\n",
    "    # Extract features (X) and target (y) from the data\n",
    "    X = df.drop(columns=['precipitationCal'])\n",
    "    y = df['precipitationCal']\n",
    "\n",
    "    # Ensure y contains only one column\n",
    "    if isinstance(y, pd.DataFrame):\n",
    "      y = y.values.reshape(-1)\n",
    "      y = y[:len(X)]\n",
    "\n",
    "    # Reshape X for LSTM input\n",
    "    X = X.values.reshape(X.shape[0], 1, X.shape[1])\n",
    "\n",
    "    # Initialize the model (can be moved outside the loop if desired for efficiency)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, input_shape=(X.shape[1], X.shape[2])))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))  # Output layer\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    # Fit the model on the new data\n",
    "    model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)\n",
    "    \n",
    "    # Concatenate data for all iterations\n",
    "    concatenated_data = pd.concat([concatenated_data, df], ignore_index=True)\n",
    "\n",
    "    # Concatenate data for test set if first iteration\n",
    "    if idx == 0:\n",
    "      concatenated_data = df.copy()\n",
    "    else:\n",
    "      concatenated_data = pd.concat([concatenated_data, df], ignore_index=True)\n",
    "\n",
    "  # Print processed files\n",
    "  print(f\"\\nProcessed files: {batch_files}\")\n",
    "\n",
    "# After processing all batches, prepare and evaluate the test set\n",
    "\n",
    "# Extract features (X_test) and target (y_test) from concatenated data\n",
    "X_test = concatenated_data.drop(columns=['precipitationCal'])\n",
    "y_test = concatenated_data['precipitationCal']\n",
    "\n",
    "if isinstance(y_test, pd.DataFrame):\n",
    "  y_test = y_test.values.reshape(-1)\n",
    "  y_test = y_test[:len(X_test)]\n",
    "\n",
    "# Reshape X_test for LSTM input\n",
    "X_test = X_test.values.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print expected error (ground truth)\n",
    "print(f\"\\nExpected Error (Ground Truth):\\n{y_test[-10:]}\")\n",
    "\n",
    "# Calculate the mean squared error for the testing set\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "percent_error = ((np.mean(y_test) -  rmse) / rmse) * 100\n",
    "print(f\"Mean Squared Error on Test Set: {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error on Test Set: {rmse:.4f}\")\n",
    "print(f\"Percentage Error on Test Set: {percent_error:.2f}%\")\n",
    "\n",
    "# Save the trained model as a .h5 file\n",
    "model_filename = 'RNN_model.h5'\n",
    "model.save(model_filename)\n",
    "print(f\"Model saved as {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfe4b95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 1s 449us/step\n",
      "Unseen Data Predictions:\n",
      "File: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2019/interpolated_insat_on_imerg_20190101.csv, MSE: 166.9442\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import load_model\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "\n",
    "# Path to the unseen data file\n",
    "unseen_data_path = '/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2019/interpolated_insat_on_imerg_20190101.csv'  # Replace with your file path\n",
    "\n",
    "# Path to the saved model file\n",
    "model_filename = '/Users/kunalpathak9826/RNN_model.h5'\n",
    "\n",
    "# Batch size (set to 1 for single file testing)\n",
    "batch_size = 1\n",
    "\n",
    "# Function for timestamp comparison (assuming timestamps are not relevant for testing)\n",
    "def compare_timestamps(var1, var2):\n",
    "  return True  # Can be removed if timestamps aren't used in unseen data\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model(model_filename)\n",
    "\n",
    "# List to store unseen data predictions\n",
    "unseen_data_predictions = []\n",
    "\n",
    "# Load unseen data\n",
    "df = pd.read_csv(unseen_data_path)\n",
    "\n",
    "# Required features to include\n",
    "required_features = ['Date', 'latitude', 'longitude', 'precipitationCal']\n",
    "\n",
    "# Randomly select 122 additional features\n",
    "random_features = np.random.choice(df.columns.difference(required_features), size=122, replace=False)\n",
    "\n",
    "# Combine required and randomly selected features\n",
    "selected_features = required_features + random_features.tolist()\n",
    "\n",
    "# Extract selected features from unseen data\n",
    "X = df[selected_features]\n",
    "\n",
    "# Get maximum feature length from training data (replace with your calculation)\n",
    "max_features = 126  # Modify this value based on your training data\n",
    "\n",
    "# Pad unseen data with zeros if it has fewer features\n",
    "if X.shape[1] < max_features:\n",
    "  padding_length = max_features - X.shape[1]\n",
    "  X = np.pad(X, ((0, 0), (0, padding_length)), mode='constant')\n",
    "\n",
    "# Reshape for LSTM input (assuming X is a 2D array)\n",
    "X = X.values.reshape(X.shape[0], 1, X.shape[1])\n",
    "\n",
    "# Make predictions on unseen data\n",
    "y_pred_unseen = model.predict(X)\n",
    "\n",
    "# Calculate MSE (modify for other evaluation metrics if needed)\n",
    "mse = mean_squared_error(df['precipitationCal'], y_pred_unseen)\n",
    "\n",
    "# Store predictions and error\n",
    "unseen_data_predictions.append({'filename': unseen_data_path, 'predictions': y_pred_unseen.squeeze(), 'mse': mse})\n",
    "\n",
    "# Print unseen data predictions and error\n",
    "print(\"Unseen Data Predictions:\")\n",
    "for prediction in unseen_data_predictions:\n",
    "  print(f\"File: {prediction['filename']}, MSE: {prediction['mse']:.4f}\")\n",
    "\n",
    "# You can further save the predictions to a CSV file for later analysis\n",
    "unseen_predictions_df = pd.DataFrame(unseen_data_predictions)\n",
    "unseen_predictions_df.to_csv('unseen_data_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78ec72d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 1s 409us/step\n",
      "Unseen Data Validation Results:\n",
      "File: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2019/interpolated_insat_on_imerg_20190101.csv, MSE: 166.9442\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import load_model\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "\n",
    "# Path to the unseen data file\n",
    "unseen_data_path = '/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2019/interpolated_insat_on_imerg_20190101.csv'\n",
    "# Path to the saved model file\n",
    "model_filename = '/Users/kunalpathak9826/RNN_model.h5'\n",
    "\n",
    "# Batch size (set to 1 for single file testing)\n",
    "batch_size = 1\n",
    "\n",
    "# Function for timestamp comparison (assuming timestamps are not relevant for testing)\n",
    "def compare_timestamps(var1, var2):\n",
    "  return True  # Can be removed if timestamps aren't used in unseen data\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model(model_filename)\n",
    "\n",
    "\n",
    "# Validate unseen data\n",
    "def validate_unseen_data(data_path):\n",
    "  # Load unseen data\n",
    "  df = pd.read_csv(data_path)\n",
    "\n",
    "  # Required features to include\n",
    "  required_features = ['Date', 'latitude', 'longitude', 'precipitationCal']\n",
    "\n",
    "  # Randomly select 122 additional features\n",
    "  random_features = np.random.choice(df.columns.difference(required_features), size=122, replace=False)\n",
    "\n",
    "  # Combine required and randomly selected features\n",
    "  selected_features = required_features + random_features.tolist()\n",
    "\n",
    "  # Extract selected features from unseen data\n",
    "  X = df[selected_features]\n",
    "\n",
    "  # Get maximum feature length from training data (replace with your calculation)\n",
    "  max_features = 126  # Modify this value based on your training data\n",
    "\n",
    "  # Pad unseen data with zeros if it has fewer features\n",
    "  if X.shape[1] < max_features:\n",
    "    padding_length = max_features - X.shape[1]\n",
    "    X = np.pad(X, ((0, 0), (0, padding_length)), mode='constant')\n",
    "\n",
    "  # Reshape for LSTM input (assuming X is a 2D array)\n",
    "  X = X.values.reshape(X.shape[0], 1, X.shape[1])\n",
    "\n",
    "  # Make predictions on unseen data\n",
    "  y_pred_unseen = model.predict(X)\n",
    "\n",
    "  # Calculate and return evaluation metrics (modify for your needs)\n",
    "  mse = mean_squared_error(df['precipitationCal'], y_pred_unseen)\n",
    "  return {'filename': data_path, 'mse': mse}\n",
    "\n",
    "\n",
    "# Validate the unseen data\n",
    "validation_results = validate_unseen_data(unseen_data_path)\n",
    "\n",
    "# Print validation results\n",
    "print(\"Unseen Data Validation Results:\")\n",
    "print(f\"File: {validation_results['filename']}, MSE: {validation_results['mse']:.4f}\")\n",
    "\n",
    "# You can further extend this code to validate multiple unseen data files\n",
    "# by iterating through a list of file paths. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a1990dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 0s 371us/step\n",
      "Unseen Data Predictions:\n",
      "File: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2023/interpolated_insat_on_imerg_20230905.csv, MSE: 0.4290, Hit%: 95.97%, Miss%: 4.03%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import load_model\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "# Path to the unseen data file\n",
    "unseen_data_path = '/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2023/interpolated_insat_on_imerg_20230905.csv'\n",
    "\n",
    "# Path to the saved model file\n",
    "model_filename = '/Users/kunalpathak9826/Desktop/ISRO/Model/RNN_model_new.h5'\n",
    "\n",
    "# Batch size (set to 1 for single file testing)\n",
    "batch_size = 1\n",
    "\n",
    "# Function for timestamp comparison (assuming timestamps are not relevant for testing)\n",
    "def compare_timestamps(var1, var2):\n",
    "  return True  # Can be removed if timestamps aren't used in unseen data\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model(model_filename)\n",
    "\n",
    "# List to store unseen data predictions\n",
    "unseen_data_predictions = []\n",
    "\n",
    "# Load unseen data\n",
    "df = pd.read_csv(unseen_data_path)\n",
    "\n",
    "# Required features to include\n",
    "required_features = ['Date', 'latitude', 'longitude', 'precipitationCal']\n",
    "\n",
    "# Randomly select 122 additional features\n",
    "random_features = np.random.choice(df.columns.difference(required_features), size=53, replace=True)\n",
    "\n",
    "# Combine required and randomly selected features\n",
    "selected_features = required_features + random_features.tolist()\n",
    "\n",
    "# Extract selected features from unseen data\n",
    "X = df[selected_features]\n",
    "\n",
    "# Get maximum feature length from training data (replace with your calculation)\n",
    "max_features = 57  # Modify this value based on your training data\n",
    "\n",
    "# Pad unseen data with zeros if it has fewer features\n",
    "if X.shape[1] < max_features:\n",
    "  padding_length = max_features - X.shape[1]\n",
    "  X = np.pad(X, ((0, 0), (0, padding_length)), mode='constant')\n",
    "\n",
    "# Reshape for LSTM input (assuming X is a 2D array)\n",
    "X = X.values.reshape(X.shape[0], 1, X.shape[1])\n",
    "\n",
    "# Define threshold for classification (modify based on your data)\n",
    "threshold = 0.5\n",
    "\n",
    "# Initialize hit and miss counters\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Make predictions on unseen data\n",
    "y_pred_unseen = model.predict(X)\n",
    "\n",
    "# Calculate MSE (modify for other evaluation metrics if needed)\n",
    "mse = mean_squared_error(df['precipitationCal'], y_pred_unseen)\n",
    "\n",
    "# Convert predictions to binary based on threshold\n",
    "predicted_classes = np.where(y_pred_unseen.squeeze() > threshold, 1, 0)\n",
    "\n",
    "# Update hit/miss count based on prediction vs actual class\n",
    "for i in range(len(predicted_classes)):\n",
    "  actual_class = df['precipitationCal'].values[i]\n",
    "  if predicted_classes[i].round() == actual_class.round():\n",
    "    hits += 1\n",
    "  else:\n",
    "    misses += 1\n",
    "\n",
    "# Calculate hit and miss percentages\n",
    "hit_percentage = (hits / len(y_pred_unseen)) * 100\n",
    "miss_percentage = (misses / len(y_pred_unseen)) * 100\n",
    "\n",
    "# Store predictions, error, and hit/miss percentages\n",
    "unseen_data_predictions.append({\n",
    "  'filename': unseen_data_path,\n",
    "  'predictions': y_pred_unseen.squeeze(),\n",
    "  'mse': mse,\n",
    "  'hit_percentage': hit_percentage,\n",
    "  'miss_percentage': miss_percentage\n",
    "})\n",
    "\n",
    "# Print unseen data predictions and error\n",
    "print(\"Unseen Data Predictions:\")\n",
    "for prediction in unseen_data_predictions:\n",
    "  print(f\"File: {prediction['filename']}, MSE: {prediction['mse']:.4f}, Hit%: {prediction['hit_percentage']:.2f}%, Miss%: {prediction['miss_percentage']:.2f}%\")\n",
    "\n",
    "# You can further save the predictions to a CSV file for later analysis\n",
    "unseen_predictions_df = pd.DataFrame(unseen_data_predictions)\n",
    "unseen_predictions_df.to_csv('unseen_data_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a86a3464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot scatter plot\n",
    "def plot_scatter(predictions, actual, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(actual, predictions, color='blue', label='Predictions')\n",
    "    plt.plot(actual, actual, color='red', label='Perfect Prediction')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Actual Precipitation')\n",
    "    plt.ylabel('Predicted Precipitation')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Extract predictions, actual values, and error rates\n",
    "predictions = unseen_data_predictions[0]['predictions']  # Assuming there's only one prediction in the list\n",
    "actual = df['precipitationCal'].values\n",
    "error_rate = unseen_data_predictions[0]['miss_percentage'] / 100  # Convert percentage to rate\n",
    "\n",
    "# Plot scatter plot for perfect prediction and error rate\n",
    "plot_scatter(predictions, actual, f'Scatter Plot (Error Rate: {error_rate:.2f})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea03ecbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Common Variables:\n",
      "{'0945 IMG_TIR1', '0115 IMG_WV', '0715 IMG_TIR2', '1915 IMG_WV', '0645 IMG_WV', '1415 IMG_WV', '2045 IMG_WV', '1315 IMG_TIR1', '0145 IMG_TIR1', '2045 IMG_TIR1', '0045 IMG_TIR1', '1245 IMG_TIR2', '1715 IMG_WV', '1615 IMG_TIR2', '0045 IMG_WV', '2145 IMG_TIR1', '1115 IMG_TIR2', '2145 IMG_TIR2', '0115 IMG_TIR2', '1015 IMG_WV', '2245 IMG_TIR1', '2015 IMG_TIR2', '0945 IMG_TIR2', '2015 IMG_WV', '1145 IMG_TIR2', '0145 IMG_TIR2', '1515 IMG_TIR1', '0245 IMG_WV', '1515 IMG_WV', '0315 IMG_WV', '1815 IMG_TIR2', '2345 IMG_TIR2', '1645 IMG_WV', '1645 IMG_TIR1', '1445 IMG_WV', '1715 IMG_TIR1', '0215 IMG_TIR1', '0315 IMG_TIR1', '2315 IMG_TIR2', '0745 IMG_TIR1', '1045 IMG_TIR1', '1315 IMG_WV', '2215 IMG_WV', '1115 IMG_TIR1', 'latitude', '0345 IMG_WV', '1015 IMG_TIR1', '0145 IMG_WV', '0015 IMG_WV', '2345 IMG_WV', '2245 IMG_WV', '0345 IMG_TIR1', '1345 IMG_TIR1', '1415 IMG_TIR1', '0615 IMG_TIR2', '0445 IMG_TIR1', '1315 IMG_TIR2', '0245 IMG_TIR2', '2015 IMG_TIR1', '1115 IMG_WV', '0645 IMG_TIR2', '2315 IMG_WV', '1445 IMG_TIR2', '2215 IMG_TIR1', 'longitude', '0215 IMG_WV', '0615 IMG_WV', '1015 IMG_TIR2', '2115 IMG_WV', '1945 IMG_WV', '1845 IMG_TIR2', '1545 IMG_TIR1', '0715 IMG_WV', '1845 IMG_TIR1', '2245 IMG_TIR2', '1715 IMG_TIR2', '1615 IMG_TIR1', '1845 IMG_WV', 'Date', '0545 IMG_WV', '1545 IMG_WV', '2345 IMG_TIR1', '0615 IMG_TIR1', '0645 IMG_TIR1', '1815 IMG_WV', '1745 IMG_TIR2', '1815 IMG_TIR1', '2215 IMG_TIR2', '2115 IMG_TIR1', '1615 IMG_WV', '1915 IMG_TIR2', '2045 IMG_TIR2', '1745 IMG_TIR1', '2315 IMG_TIR1', '1045 IMG_TIR2', '1245 IMG_WV', '0545 IMG_TIR2', '0745 IMG_WV', '2115 IMG_TIR2', '1945 IMG_TIR2', '0745 IMG_TIR2', '0015 IMG_TIR2', '0215 IMG_TIR2', '0445 IMG_TIR2', '0045 IMG_TIR2', '1945 IMG_TIR1', '1645 IMG_TIR2', '1045 IMG_WV', '1245 IMG_TIR1', '0545 IMG_TIR1', '1415 IMG_TIR2', '1915 IMG_TIR1', '1345 IMG_WV', '1545 IMG_TIR2', '0445 IMG_WV', '0945 IMG_WV', '2145 IMG_WV', '1515 IMG_TIR2', '0115 IMG_TIR1', '0315 IMG_TIR2', '0015 IMG_TIR1', '0715 IMG_TIR1', 'precipitationCal', '1145 IMG_TIR1', '1445 IMG_TIR1', '0245 IMG_TIR1', '1745 IMG_WV', '1145 IMG_WV', '0345 IMG_TIR2', '1345 IMG_TIR2'}\n",
      "\n",
      "Uncommon Variables:\n",
      "{'0515 IMG_TIR2', '1215 IMG_TIR2', '0515 IMG_WV', '0515 IMG_TIR1', '0845 IMG_TIR1', '0915 IMG_TIR1', '0845 IMG_WV', '0815 IMG_TIR2', '1215 IMG_TIR1', '0845 IMG_TIR2', '0915 IMG_WV', '0915 IMG_TIR2', '1215 IMG_WV', '0415 IMG_TIR2', '0415 IMG_WV', '0815 IMG_WV', '0415 IMG_TIR1', '0815 IMG_TIR1'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunalpathak9826/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.673e+05, tolerance: 3.391e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old files: []\n",
      "New file: interpolated_insat_on_imerg_20180109.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunalpathak9826/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.777e+07, tolerance: 5.305e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old files: ['interpolated_insat_on_imerg_20180109.csv']\n",
      "New file: interpolated_insat_on_imerg_20180103.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunalpathak9826/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.092e+07, tolerance: 3.497e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old files: ['interpolated_insat_on_imerg_20180109.csv', 'interpolated_insat_on_imerg_20180103.csv']\n",
      "New file: interpolated_insat_on_imerg_20180101.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunalpathak9826/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.603e+07, tolerance: 4.108e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old files: ['interpolated_insat_on_imerg_20180109.csv', 'interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180101.csv']\n",
      "New file: interpolated_insat_on_imerg_20180104.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunalpathak9826/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.983e+06, tolerance: 7.978e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old files: ['interpolated_insat_on_imerg_20180109.csv', 'interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20180104.csv']\n",
      "New file: interpolated_insat_on_imerg_20180108.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunalpathak9826/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.325e+06, tolerance: 1.121e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old files: ['interpolated_insat_on_imerg_20180109.csv', 'interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20180104.csv', 'interpolated_insat_on_imerg_20180108.csv']\n",
      "New file: interpolated_insat_on_imerg_20180107.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunalpathak9826/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.133e+06, tolerance: 2.539e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old files: ['interpolated_insat_on_imerg_20180109.csv', 'interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20180104.csv', 'interpolated_insat_on_imerg_20180108.csv', 'interpolated_insat_on_imerg_20180107.csv']\n",
      "New file: interpolated_insat_on_imerg_20180102.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunalpathak9826/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.502e+07, tolerance: 4.834e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old files: ['interpolated_insat_on_imerg_20180109.csv', 'interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20180104.csv', 'interpolated_insat_on_imerg_20180108.csv', 'interpolated_insat_on_imerg_20180107.csv', 'interpolated_insat_on_imerg_20180102.csv']\n",
      "New file: interpolated_insat_on_imerg_20180105.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunalpathak9826/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.866e+06, tolerance: 1.622e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old files: ['interpolated_insat_on_imerg_20180109.csv', 'interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20180104.csv', 'interpolated_insat_on_imerg_20180108.csv', 'interpolated_insat_on_imerg_20180107.csv', 'interpolated_insat_on_imerg_20180102.csv', 'interpolated_insat_on_imerg_20180105.csv']\n",
      "New file: interpolated_insat_on_imerg_20180106.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180109.csv', 'interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20180104.csv', 'interpolated_insat_on_imerg_20180108.csv', 'interpolated_insat_on_imerg_20180107.csv', 'interpolated_insat_on_imerg_20180102.csv', 'interpolated_insat_on_imerg_20180105.csv', 'interpolated_insat_on_imerg_20180106.csv']\n",
      "New file: interpolated_insat_on_imerg_20180110.csv\n",
      "All files in memory: ['interpolated_insat_on_imerg_20180109.csv', 'interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20180104.csv', 'interpolated_insat_on_imerg_20180108.csv', 'interpolated_insat_on_imerg_20180107.csv', 'interpolated_insat_on_imerg_20180102.csv', 'interpolated_insat_on_imerg_20180105.csv', 'interpolated_insat_on_imerg_20180106.csv', 'interpolated_insat_on_imerg_20180110.csv']\n",
      "Mean Squared Error on Test Set: 839.8433\n",
      "Model saved as Lasso_model.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunalpathak9826/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.701e+05, tolerance: 4.996e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Lasso\n",
    "from joblib import dump\n",
    "\n",
    "# Directory containing your CSV files\n",
    "folder_path = '/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2018/'\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Batch size\n",
    "batch_size = 10\n",
    "\n",
    "# Shuffle the list of CSV files\n",
    "import random\n",
    "random.shuffle(csv_files)\n",
    "\n",
    "# Calculate the index to split files into training and testing sets\n",
    "split_index = int(len(csv_files) * 0.7)\n",
    "\n",
    "# Training set and Testing set\n",
    "train_files = csv_files[:split_index]\n",
    "test_files = csv_files[split_index:]\n",
    "\n",
    "# Initialize common columns with columns from the first CSV file\n",
    "common_columns = set(pd.read_csv(os.path.join(folder_path, csv_files[0])).columns)\n",
    "\n",
    "# Loop through each CSV file in the folder to find common columns\n",
    "for csv_file in csv_files[1:]:\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))\n",
    "    common_columns = common_columns.intersection(df.columns)\n",
    "    \n",
    "    # Check if common variables are less than 5, skip the batch\n",
    "if len(common_columns) < 5:\n",
    "    print(f\"\\nSkipping Batch {start_index // batch_size + 1} (Less than 5 common variables):\")\n",
    "    for file in batch_files:\n",
    "      print(f\"- {file}\")\n",
    "      continue\n",
    "\n",
    "# Print the common variables found in all files\n",
    "print(\"\\nCommon Variables:\")\n",
    "print(common_columns)\n",
    "\n",
    "# Initialize an empty set to store all unique columns\n",
    "all_columns = set()\n",
    "\n",
    "# Loop through each CSV file in the folder to find all unique columns\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))\n",
    "    all_columns.update(df.columns)\n",
    "\n",
    "# Find the uncommon variables\n",
    "uncommon_variables = all_columns - common_columns\n",
    "\n",
    "# Print the uncommon variables\n",
    "print(\"\\nUncommon Variables:\")\n",
    "print(uncommon_variables)\n",
    "\n",
    "# Initialize an empty list to store the files in memory\n",
    "files_in_memory = []\n",
    "\n",
    "# Initialize the model\n",
    "model = Lasso(alpha=0.1, max_iter=10000, random_state=42)\n",
    "\n",
    "# Loop through each CSV file in the folder\n",
    "for idx, csv_file in enumerate(csv_files):\n",
    "    # Load the data from CSV file, keeping only the common columns\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))[list(common_columns) + ['precipitationCal']]\n",
    "    \n",
    "    # Concatenate the data to the DataFrame\n",
    "    if idx == 0:\n",
    "        concatenated_data = df\n",
    "    else:\n",
    "        concatenated_data = pd.concat([concatenated_data, df], ignore_index=True)\n",
    "\n",
    "    # Extract features (X) and target (y) from the data\n",
    "    X = df.drop(columns=['precipitationCal'])\n",
    "    y = df['precipitationCal']\n",
    "\n",
    "    # Ensure y contains only one column\n",
    "    if isinstance(y, pd.DataFrame):\n",
    "        y = y.values.reshape(-1)\n",
    "        y = y[:len(X)]\n",
    "        #y = y.iloc[:, 0]\n",
    "\n",
    "    # Fit the model on the new data\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Print old and new files\n",
    "    print(f\"Old files: {files_in_memory}\")\n",
    "    print(f\"New file: {csv_file}\")\n",
    "    \n",
    "    # Update files in memory\n",
    "    files_in_memory.append(csv_file)\n",
    "\n",
    "# Print all files in memory\n",
    "print(f\"All files in memory: {files_in_memory}\")\n",
    "\n",
    "# Extract features (X) and target (y) from concatenated data for test set\n",
    "X_test = concatenated_data.drop(columns=['precipitationCal'])\n",
    "y_test = concatenated_data['precipitationCal']\n",
    "\n",
    "# Ensure y contains only one column\n",
    "if isinstance(y_test, pd.DataFrame):\n",
    "    y_test = y_test.values.reshape(-1)\n",
    "    y_test = y_test[:len(X_test)]\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error for the testing set\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error on Test Set: {mse:.4f}\")\n",
    "\n",
    "# Save the trained model as a .pkl file\n",
    "model_filename = 'Lasso_model.pkl'\n",
    "dump(model, model_filename)\n",
    "print(f\"Model saved as {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "09763458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Common Variables for Batch 1:\n",
      "{'0945 IMG_TIR1', '0115 IMG_WV', '0715 IMG_TIR2', '1915 IMG_WV', '0645 IMG_WV', '1415 IMG_WV', '2045 IMG_WV', '1315 IMG_TIR1', '0145 IMG_TIR1', '2045 IMG_TIR1', '0045 IMG_TIR1', '1245 IMG_TIR2', '1715 IMG_WV', '1615 IMG_TIR2', '0045 IMG_WV', '2145 IMG_TIR1', '1115 IMG_TIR2', '2145 IMG_TIR2', '0115 IMG_TIR2', '1015 IMG_WV', '2245 IMG_TIR1', '2015 IMG_TIR2', '0945 IMG_TIR2', '2015 IMG_WV', '1145 IMG_TIR2', '0145 IMG_TIR2', '1515 IMG_TIR1', '0245 IMG_WV', '1515 IMG_WV', '0315 IMG_WV', '1815 IMG_TIR2', '2345 IMG_TIR2', '1645 IMG_WV', '1645 IMG_TIR1', '1445 IMG_WV', '1715 IMG_TIR1', '0215 IMG_TIR1', '0315 IMG_TIR1', '2315 IMG_TIR2', '0745 IMG_TIR1', '1045 IMG_TIR1', '1315 IMG_WV', '2215 IMG_WV', '1115 IMG_TIR1', 'latitude', '0345 IMG_WV', '1015 IMG_TIR1', '0145 IMG_WV', '0015 IMG_WV', '2345 IMG_WV', '2245 IMG_WV', '0345 IMG_TIR1', '1345 IMG_TIR1', '1415 IMG_TIR1', '0615 IMG_TIR2', '0445 IMG_TIR1', '1315 IMG_TIR2', '0245 IMG_TIR2', '2015 IMG_TIR1', '1115 IMG_WV', '0645 IMG_TIR2', '2315 IMG_WV', '1445 IMG_TIR2', '2215 IMG_TIR1', 'longitude', '0215 IMG_WV', '0615 IMG_WV', '1015 IMG_TIR2', '2115 IMG_WV', '1945 IMG_WV', '1845 IMG_TIR2', '1545 IMG_TIR1', '0715 IMG_WV', '1845 IMG_TIR1', '2245 IMG_TIR2', '1715 IMG_TIR2', '1615 IMG_TIR1', '1845 IMG_WV', 'Date', '0545 IMG_WV', '1545 IMG_WV', '2345 IMG_TIR1', '0615 IMG_TIR1', '0645 IMG_TIR1', '1815 IMG_WV', '1745 IMG_TIR2', '1815 IMG_TIR1', '2215 IMG_TIR2', '2115 IMG_TIR1', '1615 IMG_WV', '1915 IMG_TIR2', '2045 IMG_TIR2', '1745 IMG_TIR1', '2315 IMG_TIR1', '1045 IMG_TIR2', '1245 IMG_WV', '0545 IMG_TIR2', '0745 IMG_WV', '2115 IMG_TIR2', '1945 IMG_TIR2', '0745 IMG_TIR2', '0015 IMG_TIR2', '0215 IMG_TIR2', '0445 IMG_TIR2', '0045 IMG_TIR2', '1945 IMG_TIR1', '1645 IMG_TIR2', '1045 IMG_WV', '1245 IMG_TIR1', '0545 IMG_TIR1', '1415 IMG_TIR2', '1915 IMG_TIR1', '1345 IMG_WV', '1545 IMG_TIR2', '0445 IMG_WV', '0945 IMG_WV', '2145 IMG_WV', '1515 IMG_TIR2', '0115 IMG_TIR1', '0315 IMG_TIR2', '0015 IMG_TIR1', '0715 IMG_TIR1', 'precipitationCal', '1145 IMG_TIR1', '1445 IMG_TIR1', '0245 IMG_TIR1', '1745 IMG_WV', '1145 IMG_WV', '0345 IMG_TIR2', '1345 IMG_TIR2'}\n",
      "\n",
      "Uncommon Variables for Batch 1:\n",
      "{'0515 IMG_TIR2', '1215 IMG_TIR2', '0515 IMG_WV', '0515 IMG_TIR1', '0845 IMG_TIR1', '0915 IMG_TIR1', '0845 IMG_WV', '0815 IMG_TIR2', '1215 IMG_TIR1', '0845 IMG_TIR2', '0915 IMG_WV', '0915 IMG_TIR2', '1215 IMG_WV', '0415 IMG_TIR2', '0415 IMG_WV', '0815 IMG_WV', '0415 IMG_TIR1', '0815 IMG_TIR1'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunalpathak9826/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.502e+07, tolerance: 4.834e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old files: []\n",
      "New file: interpolated_insat_on_imerg_20180105.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunalpathak9826/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.777e+07, tolerance: 5.305e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old files: ['interpolated_insat_on_imerg_20180105.csv']\n",
      "New file: interpolated_insat_on_imerg_20180103.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunalpathak9826/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.092e+07, tolerance: 3.497e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old files: ['interpolated_insat_on_imerg_20180105.csv', 'interpolated_insat_on_imerg_20180103.csv']\n",
      "New file: interpolated_insat_on_imerg_20180101.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunalpathak9826/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.673e+05, tolerance: 3.391e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old files: ['interpolated_insat_on_imerg_20180105.csv', 'interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180101.csv']\n",
      "New file: interpolated_insat_on_imerg_20180109.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunalpathak9826/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.325e+06, tolerance: 1.121e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old files: ['interpolated_insat_on_imerg_20180105.csv', 'interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20180109.csv']\n",
      "New file: interpolated_insat_on_imerg_20180107.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunalpathak9826/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.603e+07, tolerance: 4.108e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old files: ['interpolated_insat_on_imerg_20180105.csv', 'interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20180109.csv', 'interpolated_insat_on_imerg_20180107.csv']\n",
      "New file: interpolated_insat_on_imerg_20180104.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunalpathak9826/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.133e+06, tolerance: 2.539e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old files: ['interpolated_insat_on_imerg_20180105.csv', 'interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20180109.csv', 'interpolated_insat_on_imerg_20180107.csv', 'interpolated_insat_on_imerg_20180104.csv']\n",
      "New file: interpolated_insat_on_imerg_20180102.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunalpathak9826/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.983e+06, tolerance: 7.978e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old files: ['interpolated_insat_on_imerg_20180105.csv', 'interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20180109.csv', 'interpolated_insat_on_imerg_20180107.csv', 'interpolated_insat_on_imerg_20180104.csv', 'interpolated_insat_on_imerg_20180102.csv']\n",
      "New file: interpolated_insat_on_imerg_20180108.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunalpathak9826/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.866e+06, tolerance: 1.622e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old files: ['interpolated_insat_on_imerg_20180105.csv', 'interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20180109.csv', 'interpolated_insat_on_imerg_20180107.csv', 'interpolated_insat_on_imerg_20180104.csv', 'interpolated_insat_on_imerg_20180102.csv', 'interpolated_insat_on_imerg_20180108.csv']\n",
      "New file: interpolated_insat_on_imerg_20180106.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180105.csv', 'interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20180109.csv', 'interpolated_insat_on_imerg_20180107.csv', 'interpolated_insat_on_imerg_20180104.csv', 'interpolated_insat_on_imerg_20180102.csv', 'interpolated_insat_on_imerg_20180108.csv', 'interpolated_insat_on_imerg_20180106.csv']\n",
      "New file: interpolated_insat_on_imerg_20180110.csv\n",
      "All files processed in Batch 1: ['interpolated_insat_on_imerg_20180105.csv', 'interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20180109.csv', 'interpolated_insat_on_imerg_20180107.csv', 'interpolated_insat_on_imerg_20180104.csv', 'interpolated_insat_on_imerg_20180102.csv', 'interpolated_insat_on_imerg_20180108.csv', 'interpolated_insat_on_imerg_20180106.csv', 'interpolated_insat_on_imerg_20180110.csv']\n",
      "Mean Squared Error on Test Set: 829.1384\n",
      "Model saved as Lasso_model.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunalpathak9826/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.701e+05, tolerance: 4.996e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Lasso\n",
    "from joblib import dump\n",
    "\n",
    "# Directory containing your CSV files\n",
    "folder_path = '/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2018/'\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Batch size\n",
    "batch_size = 10\n",
    "\n",
    "# Shuffle the list of CSV files\n",
    "import random\n",
    "random.shuffle(csv_files)\n",
    "\n",
    "# Loop through batches of CSV files\n",
    "for start_index in range(0, len(csv_files), batch_size):\n",
    "  batch_files = csv_files[start_index:start_index + batch_size]\n",
    "\n",
    "  # Initialize common columns with columns from the first file in the batch\n",
    "  common_columns = set(pd.read_csv(os.path.join(folder_path, batch_files[0])).columns)\n",
    "\n",
    "  # Loop through each CSV file in the batch to find common columns\n",
    "  for csv_file in batch_files[1:]:\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))\n",
    "    common_columns = common_columns.intersection(df.columns)\n",
    "\n",
    "  # Check if common variables are less than 5, skip the batch\n",
    "  if len(common_columns) < 5:\n",
    "    print(f\"\\nSkipping Batch {start_index // batch_size + 1} (Less than 5 common variables):\")\n",
    "    for file in batch_files:\n",
    "      print(f\"- {file}\")\n",
    "    continue\n",
    "\n",
    "  # Print the common variables found in all files in this batch\n",
    "  print(f\"\\nCommon Variables for Batch {start_index // batch_size + 1}:\")\n",
    "  print(common_columns)\n",
    "\n",
    "  # Initialize an empty list to store all unique columns for this batch\n",
    "  all_columns = set()\n",
    "\n",
    "  # Loop through each CSV file in the batch to find all unique columns\n",
    "  for csv_file in batch_files:\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))\n",
    "    all_columns.update(df.columns)\n",
    "\n",
    "  # Find the uncommon variables for this batch\n",
    "  uncommon_variables = all_columns - common_columns\n",
    "\n",
    "  # Print the uncommon variables for this batch\n",
    "  print(f\"\\nUncommon Variables for Batch {start_index // batch_size + 1}:\")\n",
    "  print(uncommon_variables)\n",
    "\n",
    "  # Initialize an empty list to store the files in memory for this batch\n",
    "  files_in_memory = []\n",
    "\n",
    "  # Initialize the model\n",
    "  model = Lasso(alpha=0.1, max_iter=10000, random_state=42)\n",
    "\n",
    "  # Loop through each CSV file in the batch\n",
    "  for idx, csv_file in enumerate(batch_files):\n",
    "    # Load the data from CSV file, keeping only the common columns\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))[list(common_columns) + ['precipitationCal']]\n",
    "\n",
    "    # Concatenate the data to the DataFrame\n",
    "    if idx == 0:\n",
    "      concatenated_data = df\n",
    "    else:\n",
    "      concatenated_data = pd.concat([concatenated_data, df], ignore_index=True)\n",
    "\n",
    "    # Extract features (X) and target (y) from the data\n",
    "    X = df.drop(columns=['precipitationCal'])\n",
    "    y = df['precipitationCal']\n",
    "\n",
    "    # Ensure y contains only one column\n",
    "    if isinstance(y, pd.DataFrame):\n",
    "      y = y.values.reshape(-1)\n",
    "      y = y[:len(X)]\n",
    "\n",
    "    # Fit the model on the new data\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Print old and new files\n",
    "    print(f\"Old files: {files_in_memory}\")\n",
    "    print(f\"New file: {csv_file}\")\n",
    "\n",
    "    # Update files in memory for this batch\n",
    "    files_in_memory.append(csv_file)\n",
    "\n",
    "  # Print all files processed in this batch\n",
    "  print(f\"All files processed in Batch {start_index // batch_size + 1}: {files_in_memory}\")\n",
    "\n",
    "  # Extract features (X) and target (y) from concatenated data for test set\n",
    "  X_test = concatenated_data.drop(columns=['precipitationCal'])\n",
    "  y_test = concatenated_data['precipitationCal']\n",
    "\n",
    "# Ensure y contains only one column\n",
    "  if isinstance(y_test, pd.DataFrame):\n",
    "        y_test = y_test.values.reshape(-1)\n",
    "        y_test = y_test[:len(X_test)]\n",
    "        \n",
    "# Make predictions on the test set\n",
    "  y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error for the testing set\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error on Test Set: {mse:.4f}\")\n",
    "\n",
    "# Save the trained model as a .pkl file\n",
    "model_filename = 'Lasso_model.pkl'\n",
    "dump(model, model_filename)\n",
    "print(f\"Model saved as {model_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59108300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunalpathak9826/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Create an empty Sequential model\n",
    "model2017 = Sequential()\n",
    "\n",
    "# Add layers to the model manually based on the configuration\n",
    "model2017.add(LSTM(units=64, input_shape=(1, 90), activation='tanh', recurrent_activation='sigmoid', name='lstm_299'))\n",
    "model2017.add(Dense(units=128, activation='relu', name='dense_598'))\n",
    "model2017.add(Dense(units=1, activation='linear', name='dense_599'))\n",
    "\n",
    "# Load the weights of the first model\n",
    "model2017.load_weights('/Users/kunalpathak9826/Desktop/ISRO/Model/RNN_model2017.h5')\n",
    "\n",
    "# Similarly, create an empty Sequential model for the second model (model2018)\n",
    "model2018 = Sequential()\n",
    "model2018.add(LSTM(units=64, input_shape=(1, 129), activation='tanh', recurrent_activation='sigmoid', name='lstm_228'))\n",
    "model2018.add(Dense(units=128, activation='relu', name='dense_456'))\n",
    "model2018.add(Dense(units=1, activation='linear', name='dense_457'))\n",
    "model2018.load_weights('/Users/kunalpathak9826/Desktop/ISRO/Model/RNN_model2018.h5')\n",
    "\n",
    "# Create a new model with the architecture of the first model\n",
    "merged_model = Sequential([\n",
    "    LSTM(units=64, input_shape=(1, 90), activation='tanh', recurrent_activation='sigmoid'),\n",
    "    Dense(units=128, activation='relu'),\n",
    "    Dense(units=1, activation='linear')\n",
    "])\n",
    "\n",
    "# Transfer weights from model2017 to merged_model\n",
    "for layer in merged_model.layers:\n",
    "    if layer.name in ['lstm_299', 'dense_598', 'dense_599']:\n",
    "        weights = model2017.get_layer(layer.name).get_weights()\n",
    "        layer.set_weights(weights)\n",
    "\n",
    "# Transfer weights from model2018 to merged_model\n",
    "for layer in merged_model.layers:\n",
    "    if layer.name in ['lstm_228', 'dense_456', 'dense_457']:\n",
    "        weights = model2018.get_layer(layer.name).get_weights()\n",
    "        layer.set_weights(weights)\n",
    "\n",
    "# Save the merged model\n",
    "merged_model.save('/Users/kunalpathak9826/Desktop/ISRO/Model/merged_rnn_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1a4a474a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20170109.csv\n",
      "Preprocessed data shape: (25000, 145)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20170108.csv\n",
      "Preprocessed data shape: (25000, 148)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20190101.csv\n",
      "Preprocessed data shape: (25000, 142)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20180104.csv\n",
      "Preprocessed data shape: (25000, 145)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20180110.csv\n",
      "Preprocessed data shape: (25000, 148)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20180105.csv\n",
      "Preprocessed data shape: (25000, 142)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20190102.csv\n",
      "Preprocessed data shape: (25000, 142)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20180107.csv\n",
      "Preprocessed data shape: (25000, 148)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20180106.csv\n",
      "Preprocessed data shape: (25000, 148)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20190103.csv\n",
      "Preprocessed data shape: (25000, 139)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20190107.csv\n",
      "Preprocessed data shape: (25000, 106)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20180102.csv\n",
      "Preprocessed data shape: (25000, 145)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20180103.csv\n",
      "Preprocessed data shape: (25000, 139)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20190106.csv\n",
      "Preprocessed data shape: (25000, 115)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20190110.csv\n",
      "Preprocessed data shape: (25000, 148)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20190104.csv\n",
      "Preprocessed data shape: (25000, 139)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20180101.csv\n",
      "Preprocessed data shape: (25000, 145)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20190105.csv\n",
      "Preprocessed data shape: (25000, 127)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20190108.csv\n",
      "Preprocessed data shape: (25000, 82)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20190109.csv\n",
      "Preprocessed data shape: (25000, 115)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20180108.csv\n",
      "Preprocessed data shape: (25000, 145)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20180109.csv\n",
      "Preprocessed data shape: (25000, 145)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20230204.csv\n",
      "Preprocessed data shape: (25000, 136)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20170101.csv\n",
      "Preprocessed data shape: (25000, 148)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20170103.csv\n",
      "Preprocessed data shape: (25000, 148)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20170102.csv\n",
      "Preprocessed data shape: (25000, 139)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20230415.csv\n",
      "Preprocessed data shape: (25000, 121)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20170106.csv\n",
      "Preprocessed data shape: (25000, 145)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20170107.csv\n",
      "Preprocessed data shape: (25000, 148)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20170105.csv\n",
      "Preprocessed data shape: (25000, 148)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20170104.csv\n",
      "Preprocessed data shape: (25000, 145)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20170110.csv\n",
      "Preprocessed data shape: (25000, 148)\n",
      "Concatenated data shape: (800000, 148)\n",
      "Performing PCA...\n",
      "Data shape after PCA: (800000, 147)\n",
      "PCA completed.\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 18s 876us/step - loss: 310.1029 - val_loss: 41.8548\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 17s 870us/step - loss: 273.8388 - val_loss: 52.7075\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 18s 917us/step - loss: 258.7249 - val_loss: 48.2907\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 19s 926us/step - loss: 248.7849 - val_loss: 48.5169\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 20s 1ms/step - loss: 239.2164 - val_loss: 44.5443\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 18s 909us/step - loss: 232.1847 - val_loss: 51.1634\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 18s 918us/step - loss: 224.3900 - val_loss: 51.3070\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 18s 887us/step - loss: 219.2220 - val_loss: 53.9185\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 18s 920us/step - loss: 212.7425 - val_loss: 49.7634\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 18s 881us/step - loss: 207.8387 - val_loss: 47.8583\n",
      "25000/25000 [==============================] - 9s 358us/step\n",
      "\n",
      "Expected Error (Ground Truth):\n",
      "799990    1.700497\n",
      "799991    1.765365\n",
      "799992    2.388650\n",
      "799993    2.675985\n",
      "799994    1.566219\n",
      "799995    1.337519\n",
      "799996    0.714538\n",
      "799997    0.735402\n",
      "799998    1.577156\n",
      "799999    0.563478\n",
      "Name: precipitationCal, dtype: float64\n",
      "Mean Squared Error on Test Set: 380.0848\n",
      "Root Mean Squared Error on Test Set: 19.4958\n",
      "Percentage Error on Test Set: -64.25%\n",
      "Model saved as RNN_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunalpathak9826/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Directory containing your CSV files\n",
    "folder_path = '/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/'\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Batch size\n",
    "batch_size = 34\n",
    "\n",
    "# Function for timestamp comparison\n",
    "def compare_timestamps(var1, var2):\n",
    "    try:\n",
    "        timestamp1 = int(var1.split(\"_\")[0])\n",
    "        timestamp2 = int(var2.split(\"_\")[0])\n",
    "    except ValueError:\n",
    "        return False\n",
    "    threshold = 1  # You can adjust this threshold as needed\n",
    "    return abs(timestamp1 - timestamp2) <= threshold\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_and_preprocess_data(folder_path):\n",
    "    dfs = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            print(f\"Loading data from: {filepath}\")\n",
    "            df = pd.read_csv(filepath)\n",
    "            # Perform preprocessing (e.g., removing NaN values)\n",
    "            df.dropna(inplace=True)\n",
    "            print(f\"Preprocessed data shape: {df.shape}\")\n",
    "            dfs.append(df)\n",
    "    data = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"Concatenated data shape: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# Function to perform PCA\n",
    "def perform_pca(data):\n",
    "    print(\"Performing PCA...\")\n",
    "    scaler = StandardScaler()\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    data_imputed = imputer.fit_transform(data)\n",
    "    scaled_data = scaler.fit_transform(data_imputed)\n",
    "    pca = PCA(n_components=147)  # You can change the number of components as per your requirement\n",
    "    pca_data = pca.fit_transform(scaled_data)\n",
    "    print(f\"Data shape after PCA: {pca_data.shape}\")\n",
    "    print(\"PCA completed.\")\n",
    "    return pca_data\n",
    "\n",
    "# Initialize the model outside the loop\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(1, 147)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))  # Output layer\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Loop through files in batches\n",
    "concatenated_data = None \n",
    "for start_index in range(0, len(csv_files), batch_size):\n",
    "    # Get current batch of files\n",
    "    batch_files = csv_files[start_index:start_index + batch_size]\n",
    "\n",
    "    # Load and preprocess data for this batch\n",
    "    data = load_and_preprocess_data(folder_path)\n",
    "    X = data.drop(columns=['precipitationCal'])\n",
    "    y = data['precipitationCal']\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca_data = perform_pca(X)\n",
    "    \n",
    "    # Reshape X for LSTM input\n",
    "    X = pca_data.reshape(pca_data.shape[0], 1, pca_data.shape[1])\n",
    "\n",
    "    # Fit the model on the new data\n",
    "    model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)\n",
    "    \n",
    "    # Append the processed data from this batch to concatenated_data\n",
    "    concatenated_data = pd.concat([concatenated_data, data], ignore_index=True)\n",
    "    \n",
    "# After processing all batches, prepare and evaluate the test set\n",
    "# Extract features (X_test) and target (y_test) from concatenated data\n",
    "X_test = concatenated_data.drop(columns=['precipitationCal'])\n",
    "y_test = concatenated_data['precipitationCal']\n",
    "\n",
    "if isinstance(y_test, pd.DataFrame):\n",
    "    y_test = y_test.values.reshape(-1)\n",
    "    y_test = y_test[:len(X_test)]\n",
    "\n",
    "# Reshape X_test for LSTM input\n",
    "X_test = X_test.values.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print expected error (ground truth)\n",
    "print(f\"\\nExpected Error (Ground Truth):\\n{y_test[-10:]}\")\n",
    "\n",
    "if np.isnan(y_test).any():\n",
    "    # If NaN values are present, impute them\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    y_test_imputed = imputer.fit_transform(y_test.values.reshape(-1, 1))\n",
    "    y_test = y_test_imputed.flatten()\n",
    "    \n",
    "# Check for NaN values in y_pred\n",
    "if np.isnan(y_pred).any():\n",
    "    # If NaN values are present, impute them\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    y_pred_imputed = imputer.fit_transform(y_pred)\n",
    "    y_pred = y_pred_imputed.flatten()\n",
    "\n",
    "\n",
    "# Calculate the mean squared error for the testing set\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "percent_error = ((np.mean(y_test) -  rmse) / rmse) * 100\n",
    "print(f\"Mean Squared Error on Test Set: {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error on Test Set: {rmse:.4f}\")\n",
    "print(f\"Percentage Error on Test Set: {percent_error:.2f}%\")\n",
    "\n",
    "# Save the trained model as a .h5 file\n",
    "model_filename = 'RNN_model.h5'\n",
    "model.save(model_filename)\n",
    "print(f\"Model saved as {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6599863c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20170109.csv\n",
      "Preprocessed data shape: (25000, 145)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20170108.csv\n",
      "Preprocessed data shape: (25000, 148)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20190101.csv\n",
      "Preprocessed data shape: (25000, 142)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20180104.csv\n",
      "Preprocessed data shape: (25000, 145)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20180110.csv\n",
      "Preprocessed data shape: (25000, 148)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20180105.csv\n",
      "Preprocessed data shape: (25000, 142)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20190102.csv\n",
      "Preprocessed data shape: (25000, 142)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20180107.csv\n",
      "Preprocessed data shape: (25000, 148)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20180106.csv\n",
      "Preprocessed data shape: (25000, 148)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20190103.csv\n",
      "Preprocessed data shape: (25000, 139)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20190107.csv\n",
      "Preprocessed data shape: (25000, 106)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20180102.csv\n",
      "Preprocessed data shape: (25000, 145)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20180103.csv\n",
      "Preprocessed data shape: (25000, 139)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20190106.csv\n",
      "Preprocessed data shape: (25000, 115)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20190110.csv\n",
      "Preprocessed data shape: (25000, 148)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20190104.csv\n",
      "Preprocessed data shape: (25000, 139)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20180101.csv\n",
      "Preprocessed data shape: (25000, 145)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20190105.csv\n",
      "Preprocessed data shape: (25000, 127)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20190108.csv\n",
      "Preprocessed data shape: (25000, 82)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20190109.csv\n",
      "Preprocessed data shape: (25000, 115)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20180108.csv\n",
      "Preprocessed data shape: (25000, 145)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20180109.csv\n",
      "Preprocessed data shape: (25000, 145)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20230204.csv\n",
      "Preprocessed data shape: (25000, 136)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20170101.csv\n",
      "Preprocessed data shape: (25000, 148)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20170103.csv\n",
      "Preprocessed data shape: (25000, 148)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20170102.csv\n",
      "Preprocessed data shape: (25000, 139)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20230415.csv\n",
      "Preprocessed data shape: (25000, 121)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20170106.csv\n",
      "Preprocessed data shape: (25000, 145)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20170107.csv\n",
      "Preprocessed data shape: (25000, 148)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20170105.csv\n",
      "Preprocessed data shape: (25000, 148)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20170104.csv\n",
      "Preprocessed data shape: (25000, 145)\n",
      "Loading data from: /Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/interpolated_insat_on_imerg_20170110.csv\n",
      "Preprocessed data shape: (25000, 148)\n",
      "Concatenated data shape: (800000, 148)\n",
      "Performing PCA...\n",
      "Data shape after PCA: (800000, 147)\n",
      "PCA completed.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. RandomForestRegressor expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 86\u001b[0m\n\u001b[1;32m     83\u001b[0m X \u001b[38;5;241m=\u001b[39m pca_data\u001b[38;5;241m.\u001b[39mreshape(pca_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m, pca_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Fit the model on the new data\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Append the processed data from this batch to concatenated_data\u001b[39;00m\n\u001b[1;32m     89\u001b[0m concatenated_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([concatenated_data, data], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:345\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 345\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    346\u001b[0m     X, y, multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mDTYPE\n\u001b[1;32m    347\u001b[0m )\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1101\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1104\u001b[0m     )\n\u001b[0;32m-> 1106\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1107\u001b[0m     X,\n\u001b[1;32m   1108\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   1109\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[1;32m   1110\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1111\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[1;32m   1112\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m   1113\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[1;32m   1114\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[1;32m   1115\u001b[0m     allow_nd\u001b[38;5;241m=\u001b[39mallow_nd,\n\u001b[1;32m   1116\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39mensure_min_samples,\n\u001b[1;32m   1117\u001b[0m     ensure_min_features\u001b[38;5;241m=\u001b[39mensure_min_features,\n\u001b[1;32m   1118\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m   1119\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1120\u001b[0m )\n\u001b[1;32m   1122\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:915\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    912\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    913\u001b[0m     )\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nd \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m--> 915\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    916\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    918\u001b[0m     )\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m    921\u001b[0m     _assert_all_finite(\n\u001b[1;32m    922\u001b[0m         array,\n\u001b[1;32m    923\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    924\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    925\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    926\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. RandomForestRegressor expected <= 2."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from joblib import dump\n",
    "\n",
    "\n",
    "# Directory containing your CSV files\n",
    "folder_path = '/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2050/'\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Batch size\n",
    "batch_size = 34\n",
    "\n",
    "# Function for timestamp comparison\n",
    "def compare_timestamps(var1, var2):\n",
    "    try:\n",
    "        timestamp1 = int(var1.split(\"_\")[0])\n",
    "        timestamp2 = int(var2.split(\"_\")[0])\n",
    "    except ValueError:\n",
    "        return False\n",
    "    threshold = 1  # You can adjust this threshold as needed\n",
    "    return abs(timestamp1 - timestamp2) <= threshold\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_and_preprocess_data(folder_path):\n",
    "    dfs = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            print(f\"Loading data from: {filepath}\")\n",
    "            df = pd.read_csv(filepath)\n",
    "            # Perform preprocessing (e.g., removing NaN values)\n",
    "            df.dropna(inplace=True)\n",
    "            print(f\"Preprocessed data shape: {df.shape}\")\n",
    "            dfs.append(df)\n",
    "    data = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"Concatenated data shape: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# Function to perform PCA\n",
    "def perform_pca(data):\n",
    "    print(\"Performing PCA...\")\n",
    "    scaler = StandardScaler()\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    data_imputed = imputer.fit_transform(data)\n",
    "    scaled_data = scaler.fit_transform(data_imputed)\n",
    "    pca = PCA(n_components=147)  # You can change the number of components as per your requirement\n",
    "    pca_data = pca.fit_transform(scaled_data)\n",
    "    print(f\"Data shape after PCA: {pca_data.shape}\")\n",
    "    print(\"PCA completed.\")\n",
    "    return pca_data\n",
    "\n",
    "# Initialize the model outside the loop\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)  # You can adjust hyperparameters\n",
    "\n",
    "# Loop through files in batches\n",
    "concatenated_data = None \n",
    "for start_index in range(0, len(csv_files), batch_size):\n",
    "    # Get current batch of files\n",
    "    batch_files = csv_files[start_index:start_index + batch_size]\n",
    "\n",
    "    # Load and preprocess data for this batch\n",
    "    data = load_and_preprocess_data(folder_path)\n",
    "    X = data.drop(columns=['precipitationCal'])\n",
    "    y = data['precipitationCal']\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca_data = perform_pca(X)\n",
    "    \n",
    "    # Reshape X for LSTM input\n",
    "    X = pca_data.reshape(pca_data.shape[0], 1, pca_data.shape[1])\n",
    "\n",
    "    # Fit the model on the new data\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Append the processed data from this batch to concatenated_data\n",
    "    concatenated_data = pd.concat([concatenated_data, data], ignore_index=True)\n",
    "    \n",
    "# After processing all batches, prepare and evaluate the test set\n",
    "# Extract features (X_test) and target (y_test) from concatenated data\n",
    "X_test = concatenated_data.drop(columns=['precipitationCal'])\n",
    "y_test = concatenated_data['precipitationCal']\n",
    "\n",
    "if isinstance(y_test, pd.DataFrame):\n",
    "    y_test = y_test.values.reshape(-1)\n",
    "    y_test = y_test[:len(X_test)]\n",
    "\n",
    "# Reshape X_test for LSTM input\n",
    "X_test = X_test.values.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print expected error (ground truth)\n",
    "print(f\"\\nExpected Error (Ground Truth):\\n{y_test[-10:]}\")\n",
    "\n",
    "if np.isnan(y_test).any():\n",
    "    # If NaN values are present, impute them\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    y_test_imputed = imputer.fit_transform(y_test.values.reshape(-1, 1))\n",
    "    y_test = y_test_imputed.flatten()\n",
    "    \n",
    "# Check for NaN values in y_pred\n",
    "if np.isnan(y_pred).any():\n",
    "    # If NaN values are present, impute them\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    y_pred_imputed = imputer.fit_transform(y_pred)\n",
    "    y_pred = y_pred_imputed.flatten()\n",
    "\n",
    "\n",
    "# Calculate the mean squared error for the testing set\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "percent_error = ((np.mean(y_test) -  rmse) / rmse) * 100\n",
    "print(f\"Mean Squared Error on Test Set: {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error on Test Set: {rmse:.4f}\")\n",
    "print(f\"Percentage Error on Test Set: {percent_error:.2f}%\")\n",
    "\n",
    "# Save the trained model as a .h5 file\n",
    "model_filename = 'RandomForest_model.pkl'\n",
    "dump(model, model_filename)\n",
    "print(f\"Model saved as {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5e0bf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
