{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "efdb9fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X: 250000\n",
      "Length of y: 250000\n",
      "Mean Squared Error on Test Set: 106400162754806805597051091170274285402041772367308390400.0000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# Directory containing your CSV files\n",
    "folder_path = '/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2017/'\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Shuffle the list of CSV files\n",
    "import random\n",
    "random.shuffle(csv_files)\n",
    "\n",
    "# Calculate the index to split files into training and testing sets\n",
    "split_index = int(len(csv_files) * 0.7)\n",
    "\n",
    "# Training set and Testing set\n",
    "train_files = csv_files[:split_index]\n",
    "test_files = csv_files[split_index:]\n",
    "\n",
    "# Initialize common columns with columns from the first CSV file\n",
    "common_columns = set(pd.read_csv(os.path.join(folder_path, csv_files[0])).columns)\n",
    "\n",
    "# Loop through each CSV file in the folder to find common columns\n",
    "for csv_file in csv_files[1:]:\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))\n",
    "    common_columns = common_columns.intersection(df.columns)\n",
    "\n",
    "# Initialize an empty DataFrame to store concatenated data\n",
    "concatenated_data = pd.DataFrame(columns=list(common_columns) + ['precipitationCal'])\n",
    "\n",
    "# Loop through each CSV file in the folder\n",
    "for csv_file in csv_files:\n",
    "    # Load the data from CSV file, keeping only the common columns\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))[list(common_columns) + ['precipitationCal']]\n",
    "    \n",
    "    # Concatenate the data to the DataFrame\n",
    "    concatenated_data = pd.concat([concatenated_data, df], ignore_index=True)\n",
    "\n",
    "# Extract features (X) and target (y) from concatenated data\n",
    "X = concatenated_data.drop(columns=['precipitationCal'])\n",
    "y = concatenated_data['precipitationCal']\n",
    "\n",
    "# Ensure y contains only one column\n",
    "if isinstance(y, pd.DataFrame):\n",
    "    y = y.values.reshape(-1)            #y.iloc[:, 0]\n",
    "    #y = y.iloc[:, 0]\n",
    "    # Remove duplicate samples from y\n",
    "    y = y[:len(X)]\n",
    "    \n",
    "print(\"Length of X:\", len(X))\n",
    "print(\"Length of y:\", len(y))\n",
    "    \n",
    "# Ensure X and y have the same number of samples\n",
    "if len(X) != len(y):\n",
    "    raise ValueError(\"Number of samples in X and y are not consistent\")\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create and train your model\n",
    "model = SGDRegressor(learning_rate='constant', eta0=0.01, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error for the testing set\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error on Test Set: {mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f4d0e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New File Name: interpolated_insat_on_imerg_20170109.csv\n",
      "Mean Squared Error on All Data (including new data): 472991466205821518038888200472377189177829655620179984384.0000\n",
      "Mean Squared Error on New Data: 472991466217742921480935521986214316448650752965707038720.0000\n",
      "Mean Squared Error on Data in Model's Memory: 472991466205821082477458541671143956058332142956869320704.0000\n",
      "\n",
      "\n",
      "Old File Name: interpolated_insat_on_imerg_20170109.csv\n",
      "New File Name: interpolated_insat_on_imerg_20170108.csv\n",
      "Mean Squared Error on All Data (including new data): 461475094201108891080363107272529068348001309391119712256.0000\n",
      "Mean Squared Error on New Data: 461475094216427238112319419604351294579923667896512806912.0000\n",
      "Mean Squared Error on Data in Model's Memory: 461475094201108281294361584950802541980704791662484783104.0000\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['0615 IMG_TIR2', '1045 IMG_WV', '1415 IMG_TIR1', '1215 IMG_TIR1',\\n       '1615 IMG_TIR2', '1315 IMG_TIR1', '0945 IMG_TIR2', '1145 IMG_TIR1',\\n       '1645 IMG_TIR2', '0715 IMG_WV',\\n       ...\\n       '1745 IMG_TIR1', '0315 IMG_TIR2', '0645 IMG_WV', '0045 IMG_TIR2',\\n       '1015 IMG_TIR1', '1615 IMG_WV', '0315 IMG_WV', '1215 IMG_TIR2',\\n       '0945 IMG_TIR1', 'precipitationCal'],\\n      dtype='object', length=146)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, csv_file \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(folder_path)):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Load the data from CSV file, keeping only the common columns\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, csv_file), encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m     df \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;28mlist\u001b[39m(common_columns) \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecipitationCal\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Extract features (X) and target (y) from the data\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     X \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecipitationCal\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3813\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3812\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3813\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3815\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6070\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6067\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6068\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6070\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6072\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6074\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6130\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   6129\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 6130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6132\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['0615 IMG_TIR2', '1045 IMG_WV', '1415 IMG_TIR1', '1215 IMG_TIR1',\\n       '1615 IMG_TIR2', '1315 IMG_TIR1', '0945 IMG_TIR2', '1145 IMG_TIR1',\\n       '1645 IMG_TIR2', '0715 IMG_WV',\\n       ...\\n       '1745 IMG_TIR1', '0315 IMG_TIR2', '0645 IMG_WV', '0045 IMG_TIR2',\\n       '1015 IMG_TIR1', '1615 IMG_WV', '0315 IMG_WV', '1215 IMG_TIR2',\\n       '0945 IMG_TIR1', 'precipitationCal'],\\n      dtype='object', length=146)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# Directory containing your CSV files\n",
    "folder_path = '/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2017/'\n",
    "\n",
    "# Initialize common columns with columns from the first CSV file\n",
    "first_csv_file = os.listdir(folder_path)[0]\n",
    "common_columns = set(pd.read_csv(os.path.join(folder_path, os.listdir(folder_path)[0])).columns)\n",
    "\n",
    "# Initialize the model\n",
    "model = SGDRegressor(learning_rate='constant', eta0=0.01, random_state=42)\n",
    "\n",
    "# Loop through each CSV file in the folder\n",
    "for idx, csv_file in enumerate(os.listdir(folder_path)):\n",
    "    # Load the data from CSV file, keeping only the common columns\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file), encoding = 'latin1')\n",
    "    df = df[list(common_columns) + ['precipitationCal']]\n",
    "\n",
    "    # Extract features (X) and target (y) from the data\n",
    "    X = df.drop(columns=['precipitationCal'])\n",
    "    y = df['precipitationCal']\n",
    "    \n",
    "    # Ensure y contains only one column\n",
    "    if isinstance(y, pd.DataFrame):\n",
    "        y = y.values.reshape(-1)            #y.iloc[:, 0]\n",
    "        #y = y.iloc[:, 0]\n",
    "        # Remove duplicate samples from y\n",
    "        y = y[:len(X)]\n",
    "\n",
    "    # Fit the model on the new data\n",
    "    model.partial_fit(X, y)\n",
    "\n",
    "    # Print old and new file names\n",
    "    if idx > 0:\n",
    "        old_file = os.listdir(folder_path)[idx - 1]\n",
    "        print(f\"Old File Name: {old_file}\")\n",
    "    new_file = csv_file\n",
    "    print(f\"New File Name: {new_file}\")\n",
    "\n",
    "    # Make predictions on the entire dataset\n",
    "    y_pred_all = model.predict(X)\n",
    "    mse_all = mean_squared_error(y, y_pred_all)\n",
    "    print(f\"Mean Squared Error on All Data (including new data): {mse_all:.4f}\")\n",
    "\n",
    "    # Make predictions on the new data only\n",
    "    y_pred_new = model.predict(X[-1:])\n",
    "    mse_new = mean_squared_error(y[-1:], y_pred_new)\n",
    "    print(f\"Mean Squared Error on New Data: {mse_new:.4f}\")\n",
    "\n",
    "    # Make predictions on the data model has in memory\n",
    "    y_pred_memory = model.predict(X[:-1])\n",
    "    mse_memory = mean_squared_error(y[:-1], y_pred_memory)\n",
    "    print(f\"Mean Squared Error on Data in Model's Memory: {mse_memory:.4f}\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Final prediction after processing all data\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Calculate the mean squared error for the entire dataset\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "print(f\"Mean Squared Error on Entire Dataset: {mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b742c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in Training Set:\n",
      "['interpolated_insat_on_imerg_20170103.csv', 'interpolated_insat_on_imerg_20170109.csv', 'interpolated_insat_on_imerg_20170104.csv', 'interpolated_insat_on_imerg_20170101.csv', 'interpolated_insat_on_imerg_20170102.csv', 'interpolated_insat_on_imerg_20170106.csv', 'interpolated_insat_on_imerg_20170107.csv']\n",
      "\n",
      "Files in Testing Set:\n",
      "['interpolated_insat_on_imerg_20170105.csv', 'interpolated_insat_on_imerg_20170110.csv', 'interpolated_insat_on_imerg_20170108.csv']\n",
      "\n",
      "Common Variables:\n",
      "{'1045 IMG_WV', '1415 IMG_TIR1', '1215 IMG_TIR1', '1315 IMG_TIR1', '0945 IMG_TIR2', '1145 IMG_TIR1', '1645 IMG_TIR2', '0715 IMG_WV', '2245 IMG_TIR2', '1815 IMG_WV', '1715 IMG_WV', '1315 IMG_TIR2', '1815 IMG_TIR1', '2145 IMG_WV', '0345 IMG_WV', '1115 IMG_TIR1', '2315 IMG_TIR1', '1145 IMG_WV', '2145 IMG_TIR1', '2015 IMG_TIR1', '0745 IMG_TIR1', '0045 IMG_WV', '0545 IMG_WV', '1245 IMG_TIR1', '0145 IMG_TIR2', '0715 IMG_TIR1', '1345 IMG_TIR2', 'longitude', '0445 IMG_TIR2', '0845 IMG_TIR1', '1545 IMG_WV', '1515 IMG_TIR2', '1215 IMG_WV', '0115 IMG_TIR1', '0815 IMG_TIR2', '1915 IMG_WV', '0245 IMG_TIR2', '2145 IMG_TIR2', '1045 IMG_TIR2', '1045 IMG_TIR1', '1445 IMG_TIR2', '2215 IMG_TIR2', '0115 IMG_TIR2', '1245 IMG_TIR2', '1845 IMG_WV', '1745 IMG_TIR2', '0245 IMG_WV', 'Date', '0345 IMG_TIR2', '1445 IMG_WV', '0645 IMG_TIR1', '0815 IMG_TIR1', '1415 IMG_TIR2', '1115 IMG_TIR2', '0515 IMG_TIR2', '1445 IMG_TIR1', '1245 IMG_WV', 'precipitationCal', '1545 IMG_TIR1', '2215 IMG_WV', '0345 IMG_TIR1', 'latitude', '1945 IMG_TIR1', '0545 IMG_TIR2', '1345 IMG_TIR1', '1415 IMG_WV', '0245 IMG_TIR1', '0445 IMG_TIR1', '0845 IMG_TIR2', '2045 IMG_TIR2', '2015 IMG_TIR2', '2245 IMG_TIR1', '0745 IMG_WV', '0415 IMG_TIR2', '0815 IMG_WV', '0115 IMG_WV', '1115 IMG_WV', '2345 IMG_TIR2', '1715 IMG_TIR2', '1715 IMG_TIR1', '2245 IMG_WV', '1315 IMG_WV', '1915 IMG_TIR2', '1945 IMG_TIR2', '2315 IMG_TIR2', '0145 IMG_TIR1', '1745 IMG_WV', '1815 IMG_TIR2', '0515 IMG_WV', '1945 IMG_WV', '0745 IMG_TIR2', '2015 IMG_WV', '0415 IMG_TIR1', '1645 IMG_WV', '1845 IMG_TIR2', '0845 IMG_WV', '2215 IMG_TIR1', '0415 IMG_WV', '0445 IMG_WV', '0315 IMG_TIR1', '0545 IMG_TIR1', '0715 IMG_TIR2', '1145 IMG_TIR2', '2045 IMG_TIR1', '0015 IMG_TIR1', '1845 IMG_TIR1', '1645 IMG_TIR1', '2045 IMG_WV', '2315 IMG_WV', '2345 IMG_TIR1', '0045 IMG_TIR1', '0015 IMG_TIR2', '1515 IMG_WV', '0645 IMG_TIR2', '0015 IMG_WV', '0515 IMG_TIR1', '1915 IMG_TIR1', '1545 IMG_TIR2', '1515 IMG_TIR1', '0145 IMG_WV', '1345 IMG_WV', '2345 IMG_WV', '0945 IMG_WV', '1745 IMG_TIR1', '0315 IMG_TIR2', '0645 IMG_WV', '0045 IMG_TIR2', '0315 IMG_WV', '1215 IMG_TIR2', '0945 IMG_TIR1'}\n",
      "\n",
      "Uncommon Variables:\n",
      "{'0615 IMG_TIR2', '0915 IMG_WV', '0615 IMG_WV', '0615 IMG_TIR1', '0215 IMG_WV', '0915 IMG_TIR1', '1615 IMG_TIR2', '2115 IMG_TIR2', '1615 IMG_TIR1', '1015 IMG_WV', '0215 IMG_TIR1', '0915 IMG_TIR2', '0215 IMG_TIR2', '2115 IMG_TIR1', '1015 IMG_TIR1', '1015 IMG_TIR2', '1615 IMG_WV', '2115 IMG_WV'}\n",
      "Length of X: 250000\n",
      "Length of y: 250000\n",
      "Mean Squared Error on Test Set: 249471894674354120221833677266507912256567828157260890112.0000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# Directory containing your CSV files\n",
    "folder_path = '/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2017/'\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Shuffle the list of CSV files\n",
    "import random\n",
    "random.shuffle(csv_files)\n",
    "\n",
    "# Calculate the index to split files into training and testing sets\n",
    "split_index = int(len(csv_files) * 0.7)\n",
    "\n",
    "# Training set and Testing set\n",
    "train_files = csv_files[:split_index]\n",
    "test_files = csv_files[split_index:]\n",
    "\n",
    "# Print the list of files in the training and testing sets\n",
    "print(\"Files in Training Set:\")\n",
    "print(train_files)\n",
    "print(\"\\nFiles in Testing Set:\")\n",
    "print(test_files)\n",
    "\n",
    "# Initialize common columns with columns from the first CSV file\n",
    "common_columns = set(pd.read_csv(os.path.join(folder_path, csv_files[0])).columns)\n",
    "\n",
    "# Loop through each CSV file in the folder to find common columns\n",
    "for csv_file in csv_files[1:]:\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))\n",
    "    common_columns = common_columns.intersection(df.columns)\n",
    "\n",
    "# Print the common variables found in all files\n",
    "print(\"\\nCommon Variables:\")\n",
    "print(common_columns)\n",
    "\n",
    "# Initialize an empty set to store all unique columns\n",
    "all_columns = set()\n",
    "\n",
    "# Loop through each CSV file in the folder to find all unique columns\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))\n",
    "    all_columns.update(df.columns)\n",
    "\n",
    "# Find the uncommon variables\n",
    "uncommon_variables = all_columns - common_columns\n",
    "\n",
    "# Print the uncommon variables\n",
    "print(\"\\nUncommon Variables:\")\n",
    "print(uncommon_variables)\n",
    "\n",
    "# Initialize an empty DataFrame to store concatenated data\n",
    "concatenated_data = pd.DataFrame(columns=list(common_columns) + ['precipitationCal'])\n",
    "\n",
    "# Loop through each CSV file in the folder\n",
    "for csv_file in csv_files:\n",
    "    # Load the data from CSV file, keeping only the common columns\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))[list(common_columns) + ['precipitationCal']]\n",
    "    \n",
    "    # Concatenate the data to the DataFrame\n",
    "    concatenated_data = pd.concat([concatenated_data, df], ignore_index=True)\n",
    "\n",
    "# Extract features (X) and target (y) from concatenated data\n",
    "X = concatenated_data.drop(columns=['precipitationCal'])\n",
    "y = concatenated_data['precipitationCal']\n",
    "\n",
    "# Ensure y contains only one column\n",
    "if isinstance(y, pd.DataFrame):\n",
    "    y = y.values.reshape(-1)            #y.iloc[:, 0]\n",
    "    #y = y.iloc[:, 0]\n",
    "    # Remove duplicate samples from y\n",
    "    y = y[:len(X)]\n",
    "    \n",
    "print(\"Length of X:\", len(X))\n",
    "print(\"Length of y:\", len(y))\n",
    "    \n",
    "# Ensure X and y have the same number of samples\n",
    "if len(X) != len(y):\n",
    "    raise ValueError(\"Number of samples in X and y are not consistent\")\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create and train your model\n",
    "model = SGDRegressor(learning_rate='constant', eta0=0.01, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error for the testing set\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error on Test Set: {mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7e9a19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Common Variables:\n",
      "{'1045 IMG_WV', '1415 IMG_TIR1', '1215 IMG_TIR1', '1315 IMG_TIR1', '0945 IMG_TIR2', '1145 IMG_TIR1', '1645 IMG_TIR2', '0715 IMG_WV', '2245 IMG_TIR2', '1815 IMG_WV', '1715 IMG_WV', '1315 IMG_TIR2', '1815 IMG_TIR1', '2145 IMG_WV', '0345 IMG_WV', '1115 IMG_TIR1', '2315 IMG_TIR1', '1145 IMG_WV', '2145 IMG_TIR1', '2015 IMG_TIR1', '0745 IMG_TIR1', '0045 IMG_WV', '0545 IMG_WV', '1245 IMG_TIR1', '0145 IMG_TIR2', '0715 IMG_TIR1', '1345 IMG_TIR2', 'longitude', '0445 IMG_TIR2', '0845 IMG_TIR1', '1545 IMG_WV', '1515 IMG_TIR2', '1215 IMG_WV', '0115 IMG_TIR1', '0815 IMG_TIR2', '1915 IMG_WV', '0245 IMG_TIR2', '2145 IMG_TIR2', '1045 IMG_TIR2', '1045 IMG_TIR1', '1445 IMG_TIR2', '2215 IMG_TIR2', '0115 IMG_TIR2', '1245 IMG_TIR2', '1845 IMG_WV', '1745 IMG_TIR2', '0245 IMG_WV', 'Date', '0345 IMG_TIR2', '1445 IMG_WV', '0645 IMG_TIR1', '0815 IMG_TIR1', '1415 IMG_TIR2', '1115 IMG_TIR2', '0515 IMG_TIR2', '1445 IMG_TIR1', '1245 IMG_WV', 'precipitationCal', '1545 IMG_TIR1', '2215 IMG_WV', '0345 IMG_TIR1', 'latitude', '1945 IMG_TIR1', '0545 IMG_TIR2', '1345 IMG_TIR1', '1415 IMG_WV', '0245 IMG_TIR1', '0445 IMG_TIR1', '0845 IMG_TIR2', '2045 IMG_TIR2', '2015 IMG_TIR2', '2245 IMG_TIR1', '0745 IMG_WV', '0415 IMG_TIR2', '0815 IMG_WV', '0115 IMG_WV', '1115 IMG_WV', '2345 IMG_TIR2', '1715 IMG_TIR2', '1715 IMG_TIR1', '2245 IMG_WV', '1315 IMG_WV', '1915 IMG_TIR2', '1945 IMG_TIR2', '2315 IMG_TIR2', '0145 IMG_TIR1', '1745 IMG_WV', '1815 IMG_TIR2', '0515 IMG_WV', '1945 IMG_WV', '0745 IMG_TIR2', '2015 IMG_WV', '0415 IMG_TIR1', '1645 IMG_WV', '1845 IMG_TIR2', '0845 IMG_WV', '2215 IMG_TIR1', '0415 IMG_WV', '0445 IMG_WV', '0315 IMG_TIR1', '0545 IMG_TIR1', '0715 IMG_TIR2', '1145 IMG_TIR2', '2045 IMG_TIR1', '0015 IMG_TIR1', '1845 IMG_TIR1', '1645 IMG_TIR1', '2045 IMG_WV', '2315 IMG_WV', '2345 IMG_TIR1', '0045 IMG_TIR1', '0015 IMG_TIR2', '1515 IMG_WV', '0645 IMG_TIR2', '0015 IMG_WV', '0515 IMG_TIR1', '1915 IMG_TIR1', '1545 IMG_TIR2', '1515 IMG_TIR1', '0145 IMG_WV', '1345 IMG_WV', '2345 IMG_WV', '0945 IMG_WV', '1745 IMG_TIR1', '0315 IMG_TIR2', '0645 IMG_WV', '0045 IMG_TIR2', '0315 IMG_WV', '1215 IMG_TIR2', '0945 IMG_TIR1'}\n",
      "\n",
      "Uncommon Variables:\n",
      "{'0615 IMG_TIR2', '0915 IMG_WV', '0615 IMG_WV', '0615 IMG_TIR1', '0215 IMG_WV', '0915 IMG_TIR1', '1615 IMG_TIR2', '2115 IMG_TIR2', '1615 IMG_TIR1', '1015 IMG_WV', '0215 IMG_TIR1', '0915 IMG_TIR2', '0215 IMG_TIR2', '2115 IMG_TIR1', '1015 IMG_TIR1', '1015 IMG_TIR2', '1615 IMG_WV', '2115 IMG_WV'}\n",
      "Mean Squared Error on Test Set: 383426978623794563078468181745364742807576726562393817088.0000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# Directory containing your CSV files\n",
    "folder_path = '/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2017/'\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Shuffle the list of CSV files\n",
    "import random\n",
    "random.shuffle(csv_files)\n",
    "\n",
    "# Calculate the index to split files into training and testing sets\n",
    "split_index = int(len(csv_files) * 0.7)\n",
    "\n",
    "# Training set and Testing set\n",
    "train_files = csv_files[:split_index]\n",
    "test_files = csv_files[split_index:]\n",
    "\n",
    "# Initialize common columns with columns from the first CSV file\n",
    "common_columns = set(pd.read_csv(os.path.join(folder_path, csv_files[0])).columns)\n",
    "\n",
    "# Loop through each CSV file in the folder to find common columns\n",
    "for csv_file in csv_files[1:]:\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))\n",
    "    common_columns = common_columns.intersection(df.columns)\n",
    "\n",
    "# Print the common variables found in all files\n",
    "print(\"\\nCommon Variables:\")\n",
    "print(common_columns)\n",
    "\n",
    "# Initialize an empty set to store all unique columns\n",
    "all_columns = set()\n",
    "\n",
    "# Loop through each CSV file in the folder to find all unique columns\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))\n",
    "    all_columns.update(df.columns)\n",
    "\n",
    "# Find the uncommon variables\n",
    "uncommon_variables = all_columns - common_columns\n",
    "\n",
    "# Print the uncommon variables\n",
    "print(\"\\nUncommon Variables:\")\n",
    "print(uncommon_variables)\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = SGDRegressor(learning_rate='constant', eta0=0.01, random_state=42)\n",
    "\n",
    "# Loop through each CSV file in the folder\n",
    "for idx, csv_file in enumerate(csv_files):\n",
    "    # Load the data from CSV file, keeping only the common columns\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))[list(common_columns) + ['precipitationCal']]\n",
    "    \n",
    "    # Concatenate the data to the DataFrame\n",
    "    if idx == 0:\n",
    "        concatenated_data = df\n",
    "    else:\n",
    "        concatenated_data = pd.concat([concatenated_data, df], ignore_index=True)\n",
    "\n",
    "    # Extract features (X) and target (y) from the data\n",
    "    X = df.drop(columns=['precipitationCal'])\n",
    "    y = df['precipitationCal']\n",
    "\n",
    "    # Ensure y contains only one column\n",
    "    if isinstance(y, pd.DataFrame):\n",
    "        y = y.values.reshape(-1)\n",
    "        y = y[:len(X)]\n",
    "        #y = y.iloc[:, 0]\n",
    "\n",
    "    # Fit the model on the new data\n",
    "    model.partial_fit(X, y)\n",
    "    \n",
    "\n",
    "\n",
    "# Extract features (X) and target (y) from concatenated data for test set\n",
    "X_test = concatenated_data.drop(columns=['precipitationCal'])\n",
    "y_test = concatenated_data['precipitationCal']\n",
    "\n",
    "# Ensure y contains only one column\n",
    "if isinstance(y_test, pd.DataFrame):\n",
    "    y_test = y_test.values.reshape(-1)\n",
    "    y_test = y_test[:len(X_test)]\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error for the testing set\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error on Test Set: {mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fba0534f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Common Variables:\n",
      "{'0615 IMG_TIR2', '1045 IMG_WV', '1415 IMG_TIR1', '1615 IMG_TIR2', '1315 IMG_TIR1', '0945 IMG_TIR2', '1145 IMG_TIR1', '1645 IMG_TIR2', '0715 IMG_WV', '2245 IMG_TIR2', '1815 IMG_WV', '1715 IMG_WV', '1315 IMG_TIR2', '1815 IMG_TIR1', '2145 IMG_WV', '0345 IMG_WV', '1115 IMG_TIR1', '2315 IMG_TIR1', '1145 IMG_WV', '2145 IMG_TIR1', '2015 IMG_TIR1', '0745 IMG_TIR1', '0045 IMG_WV', '0545 IMG_WV', '1245 IMG_TIR1', '0145 IMG_TIR2', '0715 IMG_TIR1', '1345 IMG_TIR2', 'longitude', '0615 IMG_TIR1', '0445 IMG_TIR2', '1545 IMG_WV', '1515 IMG_TIR2', '0115 IMG_TIR1', '1915 IMG_WV', '0245 IMG_TIR2', '2145 IMG_TIR2', '1045 IMG_TIR2', '1045 IMG_TIR1', '1445 IMG_TIR2', '2215 IMG_TIR2', '0115 IMG_TIR2', '1245 IMG_TIR2', '1845 IMG_WV', '1745 IMG_TIR2', 'Date', '0245 IMG_WV', '0345 IMG_TIR2', '1445 IMG_WV', '0645 IMG_TIR1', '1415 IMG_TIR2', '1115 IMG_TIR2', '1445 IMG_TIR1', '1245 IMG_WV', 'precipitationCal', '1545 IMG_TIR1', '2215 IMG_WV', '0345 IMG_TIR1', 'latitude', '1945 IMG_TIR1', '0545 IMG_TIR2', '0215 IMG_WV', '1345 IMG_TIR1', '2115 IMG_TIR2', '1415 IMG_WV', '0245 IMG_TIR1', '0445 IMG_TIR1', '2045 IMG_TIR2', '2015 IMG_TIR2', '2245 IMG_TIR1', '0745 IMG_WV', '2345 IMG_TIR2', '0115 IMG_WV', '1115 IMG_WV', '1715 IMG_TIR2', '1715 IMG_TIR1', '2245 IMG_WV', '1315 IMG_WV', '1915 IMG_TIR2', '1945 IMG_TIR2', '1015 IMG_TIR2', '2315 IMG_TIR2', '2115 IMG_WV', '0145 IMG_TIR1', '0615 IMG_WV', '1745 IMG_WV', '1815 IMG_TIR2', '1945 IMG_WV', '0745 IMG_TIR2', '2015 IMG_WV', '1645 IMG_WV', '1845 IMG_TIR2', '2215 IMG_TIR1', '0445 IMG_WV', '0315 IMG_TIR1', '0545 IMG_TIR1', '0715 IMG_TIR2', '1615 IMG_TIR1', '1145 IMG_TIR2', '0215 IMG_TIR2', '2045 IMG_TIR1', '0015 IMG_TIR1', '1845 IMG_TIR1', '1645 IMG_TIR1', '2045 IMG_WV', '2315 IMG_WV', '2345 IMG_TIR1', '1015 IMG_WV', '0045 IMG_TIR1', '0015 IMG_TIR2', '1515 IMG_WV', '2115 IMG_TIR1', '0215 IMG_TIR1', '0645 IMG_TIR2', '0015 IMG_WV', '1915 IMG_TIR1', '1545 IMG_TIR2', '1515 IMG_TIR1', '0145 IMG_WV', '1345 IMG_WV', '2345 IMG_WV', '0945 IMG_WV', '1745 IMG_TIR1', '0315 IMG_TIR2', '0645 IMG_WV', '0045 IMG_TIR2', '1015 IMG_TIR1', '1615 IMG_WV', '0315 IMG_WV', '0945 IMG_TIR1'}\n",
      "\n",
      "Uncommon Variables:\n",
      "{'0915 IMG_WV', '0415 IMG_WV', '0415 IMG_TIR2', '0815 IMG_WV', '0915 IMG_TIR1', '1215 IMG_TIR1', '0845 IMG_TIR1', '0845 IMG_WV', '0515 IMG_WV', '0815 IMG_TIR1', '0845 IMG_TIR2', '0915 IMG_TIR2', '0415 IMG_TIR1', '0515 IMG_TIR1', '0515 IMG_TIR2', '1215 IMG_WV', '1215 IMG_TIR2', '0815 IMG_TIR2'}\n",
      "Old files: []\n",
      "New file: interpolated_insat_on_imerg_20180101.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180101.csv']\n",
      "New file: interpolated_insat_on_imerg_20180103.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20180103.csv']\n",
      "New file: interpolated_insat_on_imerg_20180109.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180109.csv']\n",
      "New file: interpolated_insat_on_imerg_20180102.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180109.csv', 'interpolated_insat_on_imerg_20180102.csv']\n",
      "New file: interpolated_insat_on_imerg_20180108.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180109.csv', 'interpolated_insat_on_imerg_20180102.csv', 'interpolated_insat_on_imerg_20180108.csv']\n",
      "New file: interpolated_insat_on_imerg_20180106.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180109.csv', 'interpolated_insat_on_imerg_20180102.csv', 'interpolated_insat_on_imerg_20180108.csv', 'interpolated_insat_on_imerg_20180106.csv']\n",
      "New file: interpolated_insat_on_imerg_20180104.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180109.csv', 'interpolated_insat_on_imerg_20180102.csv', 'interpolated_insat_on_imerg_20180108.csv', 'interpolated_insat_on_imerg_20180106.csv', 'interpolated_insat_on_imerg_20180104.csv']\n",
      "New file: interpolated_insat_on_imerg_20180105.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180109.csv', 'interpolated_insat_on_imerg_20180102.csv', 'interpolated_insat_on_imerg_20180108.csv', 'interpolated_insat_on_imerg_20180106.csv', 'interpolated_insat_on_imerg_20180104.csv', 'interpolated_insat_on_imerg_20180105.csv']\n",
      "New file: interpolated_insat_on_imerg_20180107.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180109.csv', 'interpolated_insat_on_imerg_20180102.csv', 'interpolated_insat_on_imerg_20180108.csv', 'interpolated_insat_on_imerg_20180106.csv', 'interpolated_insat_on_imerg_20180104.csv', 'interpolated_insat_on_imerg_20180105.csv', 'interpolated_insat_on_imerg_20180107.csv']\n",
      "New file: interpolated_insat_on_imerg_20180110.csv\n",
      "All files in memory: ['interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180109.csv', 'interpolated_insat_on_imerg_20180102.csv', 'interpolated_insat_on_imerg_20180108.csv', 'interpolated_insat_on_imerg_20180106.csv', 'interpolated_insat_on_imerg_20180104.csv', 'interpolated_insat_on_imerg_20180105.csv', 'interpolated_insat_on_imerg_20180107.csv', 'interpolated_insat_on_imerg_20180110.csv']\n",
      "Mean Squared Error on Test Set: 416703916244625797095829122690734623054276042979025092608.0000\n",
      "Model saved as incremental_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from joblib import dump\n",
    "\n",
    "# Directory containing your CSV files\n",
    "folder_path = '/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2018/'\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Shuffle the list of CSV files\n",
    "import random\n",
    "random.shuffle(csv_files)\n",
    "\n",
    "# Calculate the index to split files into training and testing sets\n",
    "split_index = int(len(csv_files) * 0.7)\n",
    "\n",
    "# Training set and Testing set\n",
    "train_files = csv_files[:split_index]\n",
    "test_files = csv_files[split_index:]\n",
    "\n",
    "# Initialize common columns with columns from the first CSV file\n",
    "common_columns = set(pd.read_csv(os.path.join(folder_path, csv_files[0])).columns)\n",
    "\n",
    "# Loop through each CSV file in the folder to find common columns\n",
    "for csv_file in csv_files[1:]:\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))\n",
    "    common_columns = common_columns.intersection(df.columns)\n",
    "\n",
    "# Print the common variables found in all files\n",
    "print(\"\\nCommon Variables:\")\n",
    "print(common_columns)\n",
    "\n",
    "# Initialize an empty set to store all unique columns\n",
    "all_columns = set()\n",
    "\n",
    "# Loop through each CSV file in the folder to find all unique columns\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))\n",
    "    all_columns.update(df.columns)\n",
    "\n",
    "# Find the uncommon variables\n",
    "uncommon_variables = all_columns - common_columns\n",
    "\n",
    "# Print the uncommon variables\n",
    "print(\"\\nUncommon Variables:\")\n",
    "print(uncommon_variables)\n",
    "\n",
    "# Initialize an empty list to store the files in memory\n",
    "files_in_memory = []\n",
    "\n",
    "# Initialize the model\n",
    "model = SGDRegressor(learning_rate='constant', eta0=0.01, random_state=42)\n",
    "\n",
    "# Loop through each CSV file in the folder\n",
    "for idx, csv_file in enumerate(csv_files):\n",
    "    # Load the data from CSV file, keeping only the common columns\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))[list(common_columns) + ['precipitationCal']]\n",
    "    \n",
    "    # Concatenate the data to the DataFrame\n",
    "    if idx == 0:\n",
    "        concatenated_data = df\n",
    "    else:\n",
    "        concatenated_data = pd.concat([concatenated_data, df], ignore_index=True)\n",
    "\n",
    "    # Extract features (X) and target (y) from the data\n",
    "    X = df.drop(columns=['precipitationCal'])\n",
    "    y = df['precipitationCal']\n",
    "\n",
    "    # Ensure y contains only one column\n",
    "    if isinstance(y, pd.DataFrame):\n",
    "        y = y.values.reshape(-1)\n",
    "        y = y[:len(X)]\n",
    "        #y = y.iloc[:, 0]\n",
    "\n",
    "    # Fit the model on the new data\n",
    "    model.partial_fit(X, y)\n",
    "    \n",
    "    # Print old and new files\n",
    "    print(f\"Old files: {files_in_memory}\")\n",
    "    print(f\"New file: {csv_file}\")\n",
    "    \n",
    "    # Update files in memory\n",
    "    files_in_memory.append(csv_file)\n",
    "\n",
    "# Print all files in memory\n",
    "print(f\"All files in memory: {files_in_memory}\")\n",
    "\n",
    "# Extract features (X) and target (y) from concatenated data for test set\n",
    "X_test = concatenated_data.drop(columns=['precipitationCal'])\n",
    "y_test = concatenated_data['precipitationCal']\n",
    "\n",
    "# Ensure y contains only one column\n",
    "if isinstance(y_test, pd.DataFrame):\n",
    "    y_test = y_test.values.reshape(-1)\n",
    "    y_test = y_test[:len(X_test)]\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error for the testing set\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error on Test Set: {mse:.4f}\")\n",
    "\n",
    "# Save the trained model as a .pkl file\n",
    "model_filename = 'incremental_model.pkl'\n",
    "dump(model, model_filename)\n",
    "print(f\"Model saved as {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1384769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Common Variables:\n",
      "{'1715 IMG_TIR2', '1645 IMG_TIR1', '0715 IMG_TIR2', '2015 IMG_TIR1', '0215 IMG_TIR1', '2215 IMG_TIR1', '0015 IMG_TIR2', '0015 IMG_WV', '1015 IMG_TIR2', '2145 IMG_WV', '0615 IMG_WV', '0115 IMG_TIR1', '1045 IMG_TIR1', '1145 IMG_WV', '1345 IMG_TIR1', '1715 IMG_TIR1', '1115 IMG_TIR2', '0115 IMG_WV', '0645 IMG_TIR1', '1145 IMG_TIR1', '0615 IMG_TIR2', '1315 IMG_TIR1', '1415 IMG_TIR2', '1845 IMG_WV', '1545 IMG_TIR2', '1915 IMG_WV', '2115 IMG_TIR2', '0145 IMG_TIR2', '0315 IMG_WV', '2315 IMG_TIR2', '1815 IMG_TIR2', '1245 IMG_TIR2', '0145 IMG_TIR1', '0615 IMG_TIR1', '1515 IMG_TIR1', '0745 IMG_TIR1', '2045 IMG_TIR1', '1745 IMG_WV', '0445 IMG_WV', 'longitude', '0245 IMG_TIR2', 'precipitationCal', '1315 IMG_WV', '2215 IMG_TIR2', '0545 IMG_TIR1', '1915 IMG_TIR1', '1345 IMG_WV', '2245 IMG_TIR2', '0715 IMG_TIR1', '1815 IMG_TIR1', '1045 IMG_TIR2', '0145 IMG_WV', '1115 IMG_TIR1', '1615 IMG_WV', '2215 IMG_WV', '1245 IMG_WV', '2245 IMG_TIR1', '0245 IMG_WV', '1445 IMG_TIR1', '0315 IMG_TIR1', '1615 IMG_TIR2', '2315 IMG_WV', '0345 IMG_TIR2', '2145 IMG_TIR2', '2045 IMG_WV', '1315 IMG_TIR2', '2045 IMG_TIR2', '1945 IMG_TIR2', '1015 IMG_WV', '0945 IMG_TIR2', '0245 IMG_TIR1', '0745 IMG_WV', '1715 IMG_WV', '0015 IMG_TIR1', '2115 IMG_WV', '1645 IMG_WV', '2145 IMG_TIR1', '0445 IMG_TIR2', '0715 IMG_WV', '1345 IMG_TIR2', 'Date', '1415 IMG_TIR1', '2115 IMG_TIR1', '1445 IMG_TIR2', '0645 IMG_WV', '0545 IMG_TIR2', '0745 IMG_TIR2', '2015 IMG_WV', '1245 IMG_TIR1', '1615 IMG_TIR1', '0345 IMG_WV', '0545 IMG_WV', '1945 IMG_TIR1', '0045 IMG_WV', '0215 IMG_WV', '1115 IMG_WV', '2345 IMG_TIR2', '1015 IMG_TIR1', '0445 IMG_TIR1', '1815 IMG_WV', '0645 IMG_TIR2', '1145 IMG_TIR2', '2245 IMG_WV', '0045 IMG_TIR1', '0045 IMG_TIR2', '1745 IMG_TIR2', '1645 IMG_TIR2', '0115 IMG_TIR2', '0215 IMG_TIR2', '1515 IMG_WV', '1515 IMG_TIR2', '1915 IMG_TIR2', '2345 IMG_TIR1', '1045 IMG_WV', '0945 IMG_WV', '1545 IMG_WV', '1745 IMG_TIR1', 'latitude', '0945 IMG_TIR1', '2345 IMG_WV', '0315 IMG_TIR2', '1545 IMG_TIR1', '1415 IMG_WV', '1945 IMG_WV', '1445 IMG_WV', '1845 IMG_TIR1', '1845 IMG_TIR2', '2315 IMG_TIR1', '2015 IMG_TIR2', '0345 IMG_TIR1'}\n",
      "Old files: []\n",
      "New file: interpolated_insat_on_imerg_20180103.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180103.csv']\n",
      "New file: interpolated_insat_on_imerg_20180110.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180110.csv']\n",
      "New file: interpolated_insat_on_imerg_20180107.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180110.csv', 'interpolated_insat_on_imerg_20180107.csv']\n",
      "New file: interpolated_insat_on_imerg_20180105.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180110.csv', 'interpolated_insat_on_imerg_20180107.csv', 'interpolated_insat_on_imerg_20180105.csv']\n",
      "New file: interpolated_insat_on_imerg_20180106.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180110.csv', 'interpolated_insat_on_imerg_20180107.csv', 'interpolated_insat_on_imerg_20180105.csv', 'interpolated_insat_on_imerg_20180106.csv']\n",
      "New file: interpolated_insat_on_imerg_20180102.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180110.csv', 'interpolated_insat_on_imerg_20180107.csv', 'interpolated_insat_on_imerg_20180105.csv', 'interpolated_insat_on_imerg_20180106.csv', 'interpolated_insat_on_imerg_20180102.csv']\n",
      "New file: interpolated_insat_on_imerg_20180108.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180110.csv', 'interpolated_insat_on_imerg_20180107.csv', 'interpolated_insat_on_imerg_20180105.csv', 'interpolated_insat_on_imerg_20180106.csv', 'interpolated_insat_on_imerg_20180102.csv', 'interpolated_insat_on_imerg_20180108.csv']\n",
      "New file: interpolated_insat_on_imerg_20180109.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180110.csv', 'interpolated_insat_on_imerg_20180107.csv', 'interpolated_insat_on_imerg_20180105.csv', 'interpolated_insat_on_imerg_20180106.csv', 'interpolated_insat_on_imerg_20180102.csv', 'interpolated_insat_on_imerg_20180108.csv', 'interpolated_insat_on_imerg_20180109.csv']\n",
      "New file: interpolated_insat_on_imerg_20180101.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180110.csv', 'interpolated_insat_on_imerg_20180107.csv', 'interpolated_insat_on_imerg_20180105.csv', 'interpolated_insat_on_imerg_20180106.csv', 'interpolated_insat_on_imerg_20180102.csv', 'interpolated_insat_on_imerg_20180108.csv', 'interpolated_insat_on_imerg_20180109.csv', 'interpolated_insat_on_imerg_20180101.csv']\n",
      "New file: interpolated_insat_on_imerg_20180104.csv\n",
      "All files in memory: ['interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180110.csv', 'interpolated_insat_on_imerg_20180107.csv', 'interpolated_insat_on_imerg_20180105.csv', 'interpolated_insat_on_imerg_20180106.csv', 'interpolated_insat_on_imerg_20180102.csv', 'interpolated_insat_on_imerg_20180108.csv', 'interpolated_insat_on_imerg_20180109.csv', 'interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20180104.csv']\n",
      "Model saved as incremental_model.pkl\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['0215 IMG_TIR1', '0615 IMG_WV', '0615 IMG_TIR2', '0615 IMG_TIR1', '1615 IMG_WV', '1615 IMG_TIR2', '1615 IMG_TIR1', '0215 IMG_WV', '0215 IMG_TIR2'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 90\u001b[0m\n\u001b[1;32m     87\u001b[0m new_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Preprocess the new data (keep only common columns)\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m new_data_common \u001b[38;5;241m=\u001b[39m new_data[\u001b[38;5;28mlist\u001b[39m(common_columns)]\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Extract features (X) and target (y) from the new data\u001b[39;00m\n\u001b[1;32m     93\u001b[0m X_new \u001b[38;5;241m=\u001b[39m new_data_common\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecipitationCal\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3813\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3812\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3813\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3815\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6070\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6067\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6068\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6070\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6072\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6074\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6133\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6132\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['0215 IMG_TIR1', '0615 IMG_WV', '0615 IMG_TIR2', '0615 IMG_TIR1', '1615 IMG_WV', '1615 IMG_TIR2', '1615 IMG_TIR1', '0215 IMG_WV', '0215 IMG_TIR2'] not in index\""
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from joblib import dump, load\n",
    "\n",
    "# Directory containing your CSV files\n",
    "folder_path = '/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2018/'\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Shuffle the list of CSV files\n",
    "import random\n",
    "random.shuffle(csv_files)\n",
    "\n",
    "# Calculate the index to split files into training and testing sets\n",
    "split_index = int(len(csv_files) * 0.7)\n",
    "\n",
    "# Training set and Testing set\n",
    "train_files = csv_files[:split_index]\n",
    "test_files = csv_files[split_index:]\n",
    "\n",
    "# Initialize common columns with columns from the first CSV file\n",
    "common_columns = set(pd.read_csv(os.path.join(folder_path, csv_files[0])).columns)\n",
    "\n",
    "# Loop through each CSV file in the folder to find common columns\n",
    "for csv_file in csv_files[1:]:\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))\n",
    "    common_columns = common_columns.intersection(df.columns)\n",
    "\n",
    "# Print the common variables found in all files\n",
    "print(\"\\nCommon Variables:\")\n",
    "print(common_columns)\n",
    "\n",
    "# Initialize an empty list to store the files in memory\n",
    "files_in_memory = []\n",
    "\n",
    "# Initialize the model\n",
    "model = SGDRegressor(learning_rate='constant', eta0=0.01, random_state=42)\n",
    "\n",
    "# Loop through each CSV file in the folder\n",
    "for idx, csv_file in enumerate(csv_files):\n",
    "    # Load the data from CSV file, keeping only the common columns\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))[list(common_columns) + ['precipitationCal']]\n",
    "    \n",
    "    # Extract features (X) and target (y) from the data\n",
    "    X = df.drop(columns=['precipitationCal'])\n",
    "    y = df['precipitationCal']\n",
    "\n",
    "    # Ensure y contains only one column\n",
    "    if isinstance(y, pd.DataFrame):\n",
    "        y = y.values.reshape(-1)\n",
    "        y = y[:len(X)]\n",
    "        #y = y.iloc[:, 0]\n",
    "\n",
    "    # Fit the model on the new data\n",
    "    model.partial_fit(X, y)\n",
    "    \n",
    "    # Print old and new files\n",
    "    print(f\"Old files: {files_in_memory}\")\n",
    "    print(f\"New file: {csv_file}\")\n",
    "    \n",
    "    # Update files in memory\n",
    "    files_in_memory.append(csv_file)\n",
    "\n",
    "# Print all files in memory\n",
    "print(f\"All files in memory: {files_in_memory}\")\n",
    "\n",
    "# Save the trained model as a .pkl file\n",
    "model_filename = 'incremental_model.pkl'\n",
    "dump(model, model_filename)\n",
    "print(f\"Model saved as {model_filename}\")\n",
    "\n",
    "# Now, let's update the model with new data from a folder\n",
    "new_data_folder = '/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2017/'\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load(model_filename)\n",
    "\n",
    "# Iterate through each CSV file in the folder\n",
    "for file_name in os.listdir(new_data_folder):\n",
    "    if file_name.endswith('.csv'):\n",
    "        # Load the CSV file\n",
    "        file_path = os.path.join(new_data_folder, file_name)\n",
    "        new_data = pd.read_csv(file_path)\n",
    "        \n",
    "        # Preprocess the new data (keep only common columns)\n",
    "        new_data_common = new_data[list(common_columns)]\n",
    "        \n",
    "        # Extract features (X) and target (y) from the new data\n",
    "        X_new = new_data_common.drop(columns=['precipitationCal'])\n",
    "        y_new = new_data_common['precipitationCal']\n",
    "\n",
    "        # Ensure y contains only one column\n",
    "        if isinstance(y_new, pd.DataFrame):\n",
    "            y_new = y_new.values.reshape(-1)\n",
    "            y_new = y_new[:len(X_new)]\n",
    "\n",
    "        # Update the model with the new data\n",
    "        loaded_model.partial_fit(X_new, y_new)\n",
    "\n",
    "# Save the updated model\n",
    "updated_model_filename = 'updated_incremental_model.pkl'\n",
    "dump(loaded_model, updated_model_filename)\n",
    "print(f\"Updated model saved as {updated_model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c157bdf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Common Variables:\n",
      "{'1915 IMG_TIR2', '0215 IMG_TIR2', '1245 IMG_TIR2', '0215 IMG_WV', '0645 IMG_TIR1', '1015 IMG_TIR1', '1545 IMG_TIR1', '1445 IMG_WV', '1515 IMG_TIR1', '0445 IMG_TIR1', '0245 IMG_TIR1', '1845 IMG_TIR1', '1945 IMG_TIR2', '1915 IMG_WV', '1815 IMG_TIR2', '1915 IMG_TIR1', '0245 IMG_TIR2', '1315 IMG_TIR1', '2115 IMG_TIR2', '0015 IMG_WV', '0445 IMG_TIR2', '0115 IMG_WV', '1415 IMG_TIR1', '0245 IMG_WV', '0715 IMG_WV', '1345 IMG_WV', '0345 IMG_WV', '2115 IMG_WV', '0215 IMG_TIR1', '2045 IMG_TIR1', '2015 IMG_TIR1', '0715 IMG_TIR2', '1315 IMG_TIR2', '0545 IMG_WV', '1415 IMG_WV', '2315 IMG_TIR2', '1745 IMG_TIR2', '1015 IMG_TIR2', '1615 IMG_WV', '0645 IMG_TIR2', '1345 IMG_TIR2', '1815 IMG_TIR1', '0045 IMG_TIR2', '0015 IMG_TIR1', 'latitude', '1145 IMG_TIR2', '1315 IMG_WV', '2245 IMG_TIR1', '2245 IMG_TIR2', '2015 IMG_WV', '2215 IMG_TIR1', '0445 IMG_WV', 'Date', '0145 IMG_TIR2', '1645 IMG_WV', '1415 IMG_TIR2', '1615 IMG_TIR2', '0345 IMG_TIR2', '1245 IMG_TIR1', '1645 IMG_TIR2', '0645 IMG_WV', '1445 IMG_TIR2', '2045 IMG_TIR2', '0145 IMG_TIR1', '1015 IMG_WV', '0545 IMG_TIR2', '0945 IMG_TIR1', '0315 IMG_TIR2', '0615 IMG_WV', '1145 IMG_TIR1', '0145 IMG_WV', '2115 IMG_TIR1', '2245 IMG_WV', '0315 IMG_WV', '1845 IMG_TIR2', '0715 IMG_TIR1', '0045 IMG_TIR1', '2145 IMG_WV', '2015 IMG_TIR2', '0545 IMG_TIR1', '1745 IMG_WV', '1545 IMG_TIR2', '0115 IMG_TIR1', '1515 IMG_WV', '1945 IMG_TIR1', '0745 IMG_TIR2', '1645 IMG_TIR1', '2345 IMG_TIR1', '0615 IMG_TIR2', '0745 IMG_WV', '1615 IMG_TIR1', '0945 IMG_WV', '0615 IMG_TIR1', '1845 IMG_WV', '1945 IMG_WV', '0315 IMG_TIR1', '1815 IMG_WV', '1045 IMG_TIR2', '1115 IMG_TIR1', '1515 IMG_TIR2', '1115 IMG_WV', '2145 IMG_TIR2', '1115 IMG_TIR2', '1445 IMG_TIR1', '1715 IMG_WV', 'longitude', '0115 IMG_TIR2', '0045 IMG_WV', '0745 IMG_TIR1', '0015 IMG_TIR2', '2215 IMG_TIR2', '1715 IMG_TIR2', '2045 IMG_WV', '0345 IMG_TIR1', '2215 IMG_WV', '1545 IMG_WV', '1145 IMG_WV', '1245 IMG_WV', '2345 IMG_WV', '1745 IMG_TIR1', '2315 IMG_TIR1', '1045 IMG_WV', '1715 IMG_TIR1', '1345 IMG_TIR1', '0945 IMG_TIR2', '2315 IMG_WV', '2345 IMG_TIR2', '2145 IMG_TIR1', '1045 IMG_TIR1', 'precipitationCal'}\n",
      "\n",
      "Uncommon Variables:\n",
      "{'1215 IMG_TIR1', '0515 IMG_TIR1', '1215 IMG_TIR2', '0915 IMG_TIR1', '0415 IMG_TIR2', '0815 IMG_TIR1', '0845 IMG_WV', '0815 IMG_TIR2', '0915 IMG_WV', '0515 IMG_WV', '0845 IMG_TIR2', '1215 IMG_WV', '0415 IMG_WV', '0915 IMG_TIR2', '0845 IMG_TIR1', '0515 IMG_TIR2', '0815 IMG_WV', '0415 IMG_TIR1'}\n",
      "Old files: []\n",
      "New file: interpolated_insat_on_imerg_20180102.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180102.csv']\n",
      "New file: interpolated_insat_on_imerg_20180110.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180102.csv', 'interpolated_insat_on_imerg_20180110.csv']\n",
      "New file: interpolated_insat_on_imerg_20180108.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180102.csv', 'interpolated_insat_on_imerg_20180110.csv', 'interpolated_insat_on_imerg_20180108.csv']\n",
      "New file: interpolated_insat_on_imerg_20180107.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180102.csv', 'interpolated_insat_on_imerg_20180110.csv', 'interpolated_insat_on_imerg_20180108.csv', 'interpolated_insat_on_imerg_20180107.csv']\n",
      "New file: interpolated_insat_on_imerg_20180109.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180102.csv', 'interpolated_insat_on_imerg_20180110.csv', 'interpolated_insat_on_imerg_20180108.csv', 'interpolated_insat_on_imerg_20180107.csv', 'interpolated_insat_on_imerg_20180109.csv']\n",
      "New file: interpolated_insat_on_imerg_20180101.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180102.csv', 'interpolated_insat_on_imerg_20180110.csv', 'interpolated_insat_on_imerg_20180108.csv', 'interpolated_insat_on_imerg_20180107.csv', 'interpolated_insat_on_imerg_20180109.csv', 'interpolated_insat_on_imerg_20180101.csv']\n",
      "New file: interpolated_insat_on_imerg_20180104.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180102.csv', 'interpolated_insat_on_imerg_20180110.csv', 'interpolated_insat_on_imerg_20180108.csv', 'interpolated_insat_on_imerg_20180107.csv', 'interpolated_insat_on_imerg_20180109.csv', 'interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20180104.csv']\n",
      "New file: interpolated_insat_on_imerg_20180105.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180102.csv', 'interpolated_insat_on_imerg_20180110.csv', 'interpolated_insat_on_imerg_20180108.csv', 'interpolated_insat_on_imerg_20180107.csv', 'interpolated_insat_on_imerg_20180109.csv', 'interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20180104.csv', 'interpolated_insat_on_imerg_20180105.csv']\n",
      "New file: interpolated_insat_on_imerg_20180106.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180102.csv', 'interpolated_insat_on_imerg_20180110.csv', 'interpolated_insat_on_imerg_20180108.csv', 'interpolated_insat_on_imerg_20180107.csv', 'interpolated_insat_on_imerg_20180109.csv', 'interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20180104.csv', 'interpolated_insat_on_imerg_20180105.csv', 'interpolated_insat_on_imerg_20180106.csv']\n",
      "New file: interpolated_insat_on_imerg_20180103.csv\n",
      "All files in memory: ['interpolated_insat_on_imerg_20180102.csv', 'interpolated_insat_on_imerg_20180110.csv', 'interpolated_insat_on_imerg_20180108.csv', 'interpolated_insat_on_imerg_20180107.csv', 'interpolated_insat_on_imerg_20180109.csv', 'interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20180104.csv', 'interpolated_insat_on_imerg_20180105.csv', 'interpolated_insat_on_imerg_20180106.csv', 'interpolated_insat_on_imerg_20180103.csv']\n",
      "Mean Squared Error on Test Set: 3352.7911\n",
      "Model saved as random_forest_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from joblib import dump\n",
    "\n",
    "# Directory containing your CSV files\n",
    "folder_path = '/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2018/'\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Shuffle the list of CSV files\n",
    "import random\n",
    "random.shuffle(csv_files)\n",
    "\n",
    "# Calculate the index to split files into training and testing sets\n",
    "split_index = int(len(csv_files) * 0.7)\n",
    "\n",
    "# Training set and Testing set\n",
    "train_files = csv_files[:split_index]\n",
    "test_files = csv_files[split_index:]\n",
    "\n",
    "# Initialize common columns with columns from the first CSV file\n",
    "common_columns = set(pd.read_csv(os.path.join(folder_path, csv_files[0])).columns)\n",
    "\n",
    "# Loop through each CSV file in the folder to find common columns\n",
    "for csv_file in csv_files[1:]:\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))\n",
    "    common_columns = common_columns.intersection(df.columns)\n",
    "\n",
    "# Print the common variables found in all files\n",
    "print(\"\\nCommon Variables:\")\n",
    "print(common_columns)\n",
    "\n",
    "# Initialize an empty set to store all unique columns\n",
    "all_columns = set()\n",
    "\n",
    "# Loop through each CSV file in the folder to find all unique columns\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))\n",
    "    all_columns.update(df.columns)\n",
    "\n",
    "# Find the uncommon variables\n",
    "uncommon_variables = all_columns - common_columns\n",
    "\n",
    "# Print the uncommon variables\n",
    "print(\"\\nUncommon Variables:\")\n",
    "print(uncommon_variables)\n",
    "\n",
    "# Initialize an empty list to store the files in memory\n",
    "files_in_memory = []\n",
    "\n",
    "# Initialize the model (Random Forest Regressor)\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Loop through each CSV file in the folder\n",
    "for idx, csv_file in enumerate(csv_files):\n",
    "    # Load the data from CSV file, keeping only the common columns\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))[list(common_columns) + ['precipitationCal']]\n",
    "    \n",
    "    # Concatenate the data to the DataFrame\n",
    "    if idx == 0:\n",
    "        concatenated_data = df\n",
    "    else:\n",
    "        concatenated_data = pd.concat([concatenated_data, df], ignore_index=True)\n",
    "\n",
    "    # Extract features (X) and target (y) from the data\n",
    "    X = df.drop(columns=['precipitationCal'])\n",
    "    y = df['precipitationCal']\n",
    "\n",
    "    # Ensure y contains only one column\n",
    "    if isinstance(y, pd.DataFrame):\n",
    "        y = y.values.reshape(-1)\n",
    "        y = y[:len(X)]\n",
    "        #y = y.iloc[:, 0]\n",
    "\n",
    "    # Fit the model on the new data\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Print old and new files\n",
    "    print(f\"Old files: {files_in_memory}\")\n",
    "    print(f\"New file: {csv_file}\")\n",
    "    \n",
    "    # Update files in memory\n",
    "    files_in_memory.append(csv_file)\n",
    "\n",
    "# Print all files in memory\n",
    "print(f\"All files in memory: {files_in_memory}\")\n",
    "\n",
    "# Extract features (X) and target (y) from concatenated data for test set\n",
    "X_test = concatenated_data.drop(columns=['precipitationCal'])\n",
    "y_test = concatenated_data['precipitationCal']\n",
    "\n",
    "# Ensure y contains only one column\n",
    "if isinstance(y_test, pd.DataFrame):\n",
    "    y_test = y_test.values.reshape(-1)\n",
    "    y_test = y_test[:len(X_test)]\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error for the testing set\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error on Test Set: {mse:.4f}\")\n",
    "\n",
    "# Save the trained model as a .pkl file\n",
    "model_filename = 'random_forest_model.pkl'\n",
    "dump(model, model_filename)\n",
    "print(f\"Model saved as {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0d06e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Common Variables:\n",
      "{'1145 IMG_TIR1', '0715 IMG_WV', '1745 IMG_TIR1', '1115 IMG_TIR2', '1815 IMG_TIR2', '2315 IMG_WV', '1445 IMG_TIR1', '1915 IMG_TIR1', '1045 IMG_TIR2', '2015 IMG_TIR2', '1515 IMG_TIR1', '0545 IMG_WV', '2315 IMG_TIR2', '1815 IMG_TIR1', '0715 IMG_TIR1', 'precipitationCal', '1015 IMG_TIR1', '1845 IMG_WV', '1745 IMG_TIR2', '0745 IMG_TIR1', '1945 IMG_WV', '1515 IMG_WV', 'latitude', '1445 IMG_TIR2', '1845 IMG_TIR2', '0945 IMG_TIR1', '1015 IMG_TIR2', '1645 IMG_TIR1', '1715 IMG_TIR1', '1445 IMG_WV', '1145 IMG_TIR2', '2145 IMG_TIR2', '1245 IMG_TIR1', '1415 IMG_TIR2', '0015 IMG_WV', '1415 IMG_TIR1', '2345 IMG_TIR1', '0015 IMG_TIR1', '1345 IMG_WV', '0245 IMG_WV', '2115 IMG_WV', 'longitude', '0315 IMG_TIR1', '1345 IMG_TIR2', '0245 IMG_TIR2', '0045 IMG_WV', 'Date', '2145 IMG_TIR1', '0445 IMG_TIR1', '0645 IMG_TIR2', '2245 IMG_TIR2', '1915 IMG_WV', '0545 IMG_TIR2', '2045 IMG_TIR2', '1615 IMG_TIR1', '1015 IMG_WV', '0215 IMG_WV', '0115 IMG_TIR1', '0545 IMG_TIR1', '2215 IMG_TIR1', '1645 IMG_WV', '1315 IMG_WV', '1145 IMG_WV', '1245 IMG_WV', '1515 IMG_TIR2', '1115 IMG_TIR1', '1645 IMG_TIR2', '0115 IMG_WV', '0945 IMG_TIR2', '2115 IMG_TIR2', '2015 IMG_WV', '1915 IMG_TIR2', '0645 IMG_WV', '1615 IMG_WV', '0015 IMG_TIR2', '1115 IMG_WV', '2215 IMG_WV', '1545 IMG_WV', '2045 IMG_TIR1', '0315 IMG_WV', '1715 IMG_TIR2', '2115 IMG_TIR1', '0645 IMG_TIR1', '0045 IMG_TIR1', '2215 IMG_TIR2', '0245 IMG_TIR1', '1045 IMG_WV', '1415 IMG_WV', '1945 IMG_TIR1', '0215 IMG_TIR2', '0715 IMG_TIR2', '0215 IMG_TIR1', '1345 IMG_TIR1', '0345 IMG_TIR1', '0345 IMG_TIR2', '1315 IMG_TIR1', '0745 IMG_TIR2', '1545 IMG_TIR1', '1815 IMG_WV', '2245 IMG_TIR1', '1845 IMG_TIR1', '2245 IMG_WV', '0145 IMG_TIR1', '0445 IMG_WV', '0115 IMG_TIR2', '0345 IMG_WV', '0315 IMG_TIR2', '1745 IMG_WV', '1315 IMG_TIR2', '0615 IMG_WV', '2315 IMG_TIR1', '0445 IMG_TIR2', '0045 IMG_TIR2', '0945 IMG_WV', '2145 IMG_WV', '0145 IMG_TIR2', '0615 IMG_TIR1', '2015 IMG_TIR1', '0745 IMG_WV', '2345 IMG_WV', '0145 IMG_WV', '0615 IMG_TIR2', '1715 IMG_WV', '1945 IMG_TIR2', '1615 IMG_TIR2', '2045 IMG_WV', '1545 IMG_TIR2', '2345 IMG_TIR2', '1245 IMG_TIR2', '1045 IMG_TIR1'}\n",
      "\n",
      "Uncommon Variables:\n",
      "{'0845 IMG_TIR2', '0815 IMG_WV', '0515 IMG_TIR2', '0915 IMG_TIR2', '0845 IMG_WV', '1215 IMG_TIR1', '0415 IMG_WV', '0515 IMG_WV', '0815 IMG_TIR1', '0915 IMG_TIR1', '0415 IMG_TIR2', '1215 IMG_TIR2', '0415 IMG_TIR1', '0845 IMG_TIR1', '0815 IMG_TIR2', '0915 IMG_WV', '1215 IMG_WV', '0515 IMG_TIR1'}\n",
      "Old files: []\n",
      "New file: interpolated_insat_on_imerg_20180106.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180106.csv']\n",
      "New file: interpolated_insat_on_imerg_20180103.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180106.csv', 'interpolated_insat_on_imerg_20180103.csv']\n",
      "New file: interpolated_insat_on_imerg_20180107.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180106.csv', 'interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180107.csv']\n",
      "New file: interpolated_insat_on_imerg_20180101.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180106.csv', 'interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180107.csv', 'interpolated_insat_on_imerg_20180101.csv']\n",
      "New file: interpolated_insat_on_imerg_20180110.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180106.csv', 'interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180107.csv', 'interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20180110.csv']\n",
      "New file: interpolated_insat_on_imerg_20180102.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180106.csv', 'interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180107.csv', 'interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20180110.csv', 'interpolated_insat_on_imerg_20180102.csv']\n",
      "New file: interpolated_insat_on_imerg_20180108.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180106.csv', 'interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180107.csv', 'interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20180110.csv', 'interpolated_insat_on_imerg_20180102.csv', 'interpolated_insat_on_imerg_20180108.csv']\n",
      "New file: interpolated_insat_on_imerg_20180104.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180106.csv', 'interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180107.csv', 'interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20180110.csv', 'interpolated_insat_on_imerg_20180102.csv', 'interpolated_insat_on_imerg_20180108.csv', 'interpolated_insat_on_imerg_20180104.csv']\n",
      "New file: interpolated_insat_on_imerg_20180105.csv\n",
      "Old files: ['interpolated_insat_on_imerg_20180106.csv', 'interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180107.csv', 'interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20180110.csv', 'interpolated_insat_on_imerg_20180102.csv', 'interpolated_insat_on_imerg_20180108.csv', 'interpolated_insat_on_imerg_20180104.csv', 'interpolated_insat_on_imerg_20180105.csv']\n",
      "New file: interpolated_insat_on_imerg_20180109.csv\n",
      "All files in memory: ['interpolated_insat_on_imerg_20180106.csv', 'interpolated_insat_on_imerg_20180103.csv', 'interpolated_insat_on_imerg_20180107.csv', 'interpolated_insat_on_imerg_20180101.csv', 'interpolated_insat_on_imerg_20180110.csv', 'interpolated_insat_on_imerg_20180102.csv', 'interpolated_insat_on_imerg_20180108.csv', 'interpolated_insat_on_imerg_20180104.csv', 'interpolated_insat_on_imerg_20180105.csv', 'interpolated_insat_on_imerg_20180109.csv']\n",
      "Mean Squared Error on Test Set: 751.4148\n",
      "Model saved as XGBoost_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "from joblib import dump\n",
    "\n",
    "# Directory containing your CSV files\n",
    "folder_path = '/Users/kunalpathak9826/Desktop/ISRO/Data/Interpolated CSV/2018/'\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Shuffle the list of CSV files\n",
    "import random\n",
    "random.shuffle(csv_files)\n",
    "\n",
    "# Calculate the index to split files into training and testing sets\n",
    "split_index = int(len(csv_files) * 0.7)\n",
    "\n",
    "# Training set and Testing set\n",
    "train_files = csv_files[:split_index]\n",
    "test_files = csv_files[split_index:]\n",
    "\n",
    "# Initialize common columns with columns from the first CSV file\n",
    "common_columns = set(pd.read_csv(os.path.join(folder_path, csv_files[0])).columns)\n",
    "\n",
    "# Loop through each CSV file in the folder to find common columns\n",
    "for csv_file in csv_files[1:]:\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))\n",
    "    common_columns = common_columns.intersection(df.columns)\n",
    "\n",
    "# Print the common variables found in all files\n",
    "print(\"\\nCommon Variables:\")\n",
    "print(common_columns)\n",
    "\n",
    "# Initialize an empty set to store all unique columns\n",
    "all_columns = set()\n",
    "\n",
    "# Loop through each CSV file in the folder to find all unique columns\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))\n",
    "    all_columns.update(df.columns)\n",
    "\n",
    "# Find the uncommon variables\n",
    "uncommon_variables = all_columns - common_columns\n",
    "\n",
    "# Print the uncommon variables\n",
    "print(\"\\nUncommon Variables:\")\n",
    "print(uncommon_variables)\n",
    "\n",
    "# Initialize an empty list to store the files in memory\n",
    "files_in_memory = []\n",
    "\n",
    "# Initialize the model\n",
    "model = XGBRegressor(random_state=42)\n",
    "\n",
    "# Loop through each CSV file in the folder\n",
    "for idx, csv_file in enumerate(csv_files):\n",
    "    # Load the data from CSV file, keeping only the common columns\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file))[list(common_columns) + ['precipitationCal']]\n",
    "    \n",
    "    # Concatenate the data to the DataFrame\n",
    "    if idx == 0:\n",
    "        concatenated_data = df\n",
    "    else:\n",
    "        concatenated_data = pd.concat([concatenated_data, df], ignore_index=True)\n",
    "\n",
    "    # Extract features (X) and target (y) from the data\n",
    "    X = df.drop(columns=['precipitationCal'])\n",
    "    y = df['precipitationCal']\n",
    "\n",
    "    # Ensure y contains only one column\n",
    "    if isinstance(y, pd.DataFrame):\n",
    "        y = y.values.reshape(-1)\n",
    "        y = y[:len(X)]\n",
    "        #y = y.iloc[:, 0]\n",
    "\n",
    "    # Fit the model on the new data\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Print old and new files\n",
    "    print(f\"Old files: {files_in_memory}\")\n",
    "    print(f\"New file: {csv_file}\")\n",
    "    \n",
    "    # Update files in memory\n",
    "    files_in_memory.append(csv_file)\n",
    "\n",
    "# Print all files in memory\n",
    "print(f\"All files in memory: {files_in_memory}\")\n",
    "\n",
    "# Extract features (X) and target (y) from concatenated data for test set\n",
    "X_test = concatenated_data.drop(columns=['precipitationCal'])\n",
    "y_test = concatenated_data['precipitationCal']\n",
    "\n",
    "# Ensure y contains only one column\n",
    "if isinstance(y_test, pd.DataFrame):\n",
    "    y_test = y_test.values.reshape(-1)\n",
    "    y_test = y_test[:len(X_test)]\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error for the testing set\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error on Test Set: {mse:.4f}\")\n",
    "\n",
    "# Save the trained model as a .pkl file\n",
    "model_filename = 'XGBoost_model.pkl'\n",
    "dump(model, model_filename)\n",
    "print(f\"Model saved as {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f85498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
